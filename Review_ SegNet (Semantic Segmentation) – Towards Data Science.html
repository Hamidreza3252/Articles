<!DOCTYPE html>
<!-- saved from url=(0079)https://towardsdatascience.com/review-segnet-semantic-segmentation-e66f2e30fb96 -->
<html xmlns:cc="http://creativecommons.org/ns#"><head prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# medium-com: http://ogp.me/ns/fb/medium-com#"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=contain"><title>Review: SegNet (Semantic Segmentation) – Towards Data Science</title><link rel="canonical" href="https://towardsdatascience.com/review-segnet-semantic-segmentation-e66f2e30fb96"><meta name="title" content="Review: SegNet (Semantic Segmentation) – Towards Data Science"><meta name="referrer" content="always"><meta name="description" content="In this story, SegNet, by University of Cambridge, is briefly reviewed. Originally, it was submitted to 2015 CVPR, but at last it is not being published in CVPR (But it’s 2015 arXiv tech report…"><meta name="theme-color" content="#000000"><meta property="og:title" content="Review: SegNet (Semantic Segmentation) – Towards Data Science"><meta property="twitter:title" content="Review: SegNet (Semantic Segmentation)"><meta property="og:url" content="https://towardsdatascience.com/review-segnet-semantic-segmentation-e66f2e30fb96"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1200/0*k8ejti9_6CHwxzFQ.gif"><meta property="fb:app_id" content="542599432471018"><meta property="og:description" content="Encoder Decoder Architecture, Using Max Pooling Indices to Upsample, Outperforms FCN, DeepLabv1, DeconvNet"><meta name="twitter:description" content="Encoder Decoder Architecture, Using Max Pooling Indices to Upsample, Outperforms FCN, DeepLabv1, DeconvNet"><meta name="twitter:image:src" content="https://cdn-images-1.medium.com/max/1200/0*k8ejti9_6CHwxzFQ.gif"><link rel="author" href="https://towardsdatascience.com/@sh.tsang"><meta name="author" content="Sik-Ho Tsang"><meta property="og:type" content="article"><meta name="twitter:card" content="summary_large_image"><meta property="article:publisher" content="https://www.facebook.com/towardsdatascience"><meta property="article:author" content="Sik-Ho Tsang"><meta name="robots" content="index, follow"><meta property="article:published_time" content="2019-02-10T14:35:25.230Z"><meta name="twitter:site" content="@TDataScience"><meta property="og:site_name" content="Towards Data Science"><meta name="twitter:label1" value="Reading time"><meta name="twitter:data1" value="4 min read"><meta name="twitter:app:name:iphone" content="Medium"><meta name="twitter:app:id:iphone" content="828256236"><meta name="twitter:app:url:iphone" content="medium://p/e66f2e30fb96"><meta property="al:ios:app_name" content="Medium"><meta property="al:ios:app_store_id" content="828256236"><meta property="al:android:package" content="com.medium.reader"><meta property="al:android:app_name" content="Medium"><meta property="al:ios:url" content="medium://p/e66f2e30fb96"><meta property="al:android:url" content="medium://p/e66f2e30fb96"><meta property="al:web:url" content="https://towardsdatascience.com/review-segnet-semantic-segmentation-e66f2e30fb96"><link rel="search" type="application/opensearchdescription+xml" title="Medium" href="https://towardsdatascience.com/osd.xml"><link rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/e66f2e30fb96"><script async="" src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/branch-latest.min.js.download"></script><script type="application/ld+json">{"@context":"http://schema.org","@type":"NewsArticle","image":{"@type":"ImageObject","width":600,"height":338,"url":"https://cdn-images-1.medium.com/max/600/0*k8ejti9_6CHwxzFQ.gif"},"url":"https://towardsdatascience.com/review-segnet-semantic-segmentation-e66f2e30fb96","dateCreated":"2019-02-10T14:35:25.230Z","datePublished":"2019-02-10T14:35:25.230Z","dateModified":"2019-04-09T03:18:44.640Z","headline":"Review: SegNet (Semantic Segmentation)","name":"Review: SegNet (Semantic Segmentation)","articleId":"e66f2e30fb96","thumbnailUrl":"https://cdn-images-1.medium.com/max/600/0*k8ejti9_6CHwxzFQ.gif","keywords":["Tag:Machine Learning","Tag:Deep Learning","Tag:Artificial Intelligence","Tag:Data Science","Tag:Semantic Segmentation","Topic:Machine Learning","Topic:Data Science","Publication:towards-data-science","LockedPostSource:0","Elevated:false","LayerCake:3"],"author":{"@type":"Person","name":"Sik-Ho Tsang","url":"https://towardsdatascience.com/@sh.tsang"},"creator":["Sik-Ho Tsang"],"publisher":{"@type":"Organization","name":"Towards Data Science","url":"https://towardsdatascience.com","logo":{"@type":"ImageObject","width":161,"height":60,"url":"https://cdn-images-1.medium.com/max/161/1*5EUO1kUYBthpOCPzRj_l2g.png"}},"mainEntityOfPage":"https://towardsdatascience.com/review-segnet-semantic-segmentation-e66f2e30fb96"}</script><meta name="parsely-link" content="https://towardsdatascience.com/review-segnet-semantic-segmentation-e66f2e30fb96"><link rel="stylesheet" type="text/css" class="js-glyph-" id="glyph-8" href="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/m2.css"><link rel="stylesheet" href="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/main-branding-base.DUpq82k2YI6OvEW6173IfA.css"><script>!function(n,e){var t,o,i,c=[],f={passive:!0,capture:!0},r=new Date,a="pointerup",u="pointercancel";function p(n,c){t||(t=c,o=n,i=new Date,w(e),s())}function s(){o>=0&&o<i-r&&(c.forEach(function(n){n(o,t)}),c=[])}function l(t){if(t.cancelable){var o=(t.timeStamp>1e12?new Date:performance.now())-t.timeStamp;"pointerdown"==t.type?function(t,o){function i(){p(t,o),r()}function c(){r()}function r(){e(a,i,f),e(u,c,f)}n(a,i,f),n(u,c,f)}(o,t):p(o,t)}}function w(n){["click","mousedown","keydown","touchstart","pointerdown"].forEach(function(e){n(e,l,f)})}w(n),self.perfMetrics=self.perfMetrics||{},self.perfMetrics.onFirstInputDelay=function(n){c.push(n),s()}}(addEventListener,removeEventListener);</script><script>if (window.top !== window.self) window.top.location = window.self.location.href;var OB_startTime = new Date().getTime(); var OB_loadErrors = []; function _onerror(e) { OB_loadErrors.push(e) }; if (document.addEventListener) document.addEventListener("error", _onerror, true); else if (document.attachEvent) document.attachEvent("onerror", _onerror); function _asyncScript(u) {var d = document, f = d.getElementsByTagName("script")[0], s = d.createElement("script"); s.type = "text/javascript"; s.async = true; s.src = u; f.parentNode.insertBefore(s, f);}function _asyncStyles(u) {var d = document, f = d.getElementsByTagName("script")[0], s = d.createElement("link"); s.rel = "stylesheet"; s.href = u; f.parentNode.insertBefore(s, f); return s}(new Image()).src = "/_/stat?event=pixel.load&origin=" + encodeURIComponent(location.origin);</script><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date; ga("create", "UA-24232453-2", "auto", {"allowLinker": true, "legacyCookieDomain": window.location.hostname}); ga("send", "pageview");ga("create", "UA-19707169-24", "auto", 'tracker0'); ga("tracker0.send", "pageview");</script><script async="" src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/analytics.js.download"></script><!--[if lt IE 9]><script charset="UTF-8" src="https://cdn-static-1.medium.com/_/fp/js/shiv.RI2ePTZ5gFmMgLzG5bEVAA.js"></script><![endif]--><link rel="icon" href="https://cdn-images-1.medium.com/fit/c/128/128/1*F0LADxTtsKOgmPa-_7iUEQ.jpeg" class="js-favicon"><link rel="apple-touch-icon" sizes="152x152" href="https://cdn-images-1.medium.com/fit/c/152/152/1*F0LADxTtsKOgmPa-_7iUEQ.jpeg"><link rel="apple-touch-icon" sizes="120x120" href="https://cdn-images-1.medium.com/fit/c/120/120/1*F0LADxTtsKOgmPa-_7iUEQ.jpeg"><link rel="apple-touch-icon" sizes="76x76" href="https://cdn-images-1.medium.com/fit/c/76/76/1*F0LADxTtsKOgmPa-_7iUEQ.jpeg"><link rel="apple-touch-icon" sizes="60x60" href="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/1_F0LADxTtsKOgmPa-_7iUEQ.jpeg"><link rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg" color="#171717"></head><body itemscope="" class="postShowScreen browser-chrome os-windows is-withMagicUnderlines v-glyph v-glyph--m2 is-js" data-action-scope="_actionscope_0"><script>document.body.className = document.body.className.replace(/(^|\s)is-noJs(\s|$)/, "$1is-js$2")</script><div class="site-main surface-container" id="container"><div class="butterBar butterBar--error" data-action-scope="_actionscope_1"></div><div class="surface" id="_obv.shell._surface_1562948643990" style="display: block; visibility: visible;"><div class="screenContent surface-content is-supplementalPostContentLoaded" data-used="true" data-action-scope="_actionscope_2"><canvas class="canvas-renderer" width="1583" height="740"></canvas><div class="container u-maxWidth740 u-xs-margin0 notesPositionContainer js-notesPositionContainer"><div class="notesMarkers" data-action-scope="_actionscope_4"></div></div><div class="metabar u-clearfix u-boxShadow4px12pxBlackLighter u-textColorTransparentWhiteDarker js-metabar is-withBottomSection is-hiddenWhenMinimized metabar--affixed is-minimized"><div class="branch-journeys-top"></div><div class="js-metabarMiddle metabar-inner u-marginAuto u-maxWidth1032 u-flexCenter u-justifyContentSpaceBetween u-height65 u-xs-height56 u-paddingHorizontal20"><div class="metabar-block u-flex1 u-flexCenter"><div class="js-metabarLogoLeft"><a href="https://medium.com/" data-log-event="home" class="siteNav-logo u-fillTransparentBlackDarker u-flex0 u-flexCenter u-paddingTop0"><span class="svgIcon svgIcon--logoMonogram svgIcon--45px"><svg class="svgIcon-use" width="45" height="45"><path d="M5 40V5h35v35H5zm8.56-12.627c0 .555-.027.687-.318 1.03l-2.457 2.985v.396h6.974v-.396l-2.456-2.985c-.291-.343-.344-.502-.344-1.03V18.42l6.127 13.364h.714l5.256-13.364v10.644c0 .29 0 .342-.185.528l-1.848 1.796v.396h9.19v-.396l-1.822-1.796c-.184-.186-.21-.238-.21-.528V15.937c0-.291.026-.344.21-.528l1.823-1.797v-.396h-6.471l-4.622 11.542-5.203-11.542h-6.79v.396l2.14 2.64c.239.292.291.37.291.768v10.353z"></path></svg></span><span class="u-textScreenReader">Homepage</span></a></div></div><div class="metabar-block u-flex0 u-flexCenter"><div class="u-flexCenter u-height65 u-xs-height56"><div class="buttonSet buttonSet--wide u-lineHeightInherit"><label class="button button--small button--chromeless button--withIcon button--withSvgIcon inputGroup u-sm-hide metabar-predictiveSearch u-baseColor--buttonNormal u-baseColor--placeholderNormal" title="Search"><span class="svgIcon svgIcon--search svgIcon--25px u-baseColor--iconLight"><svg class="svgIcon-use" width="25" height="25"><path d="M20.067 18.933l-4.157-4.157a6 6 0 1 0-.884.884l4.157 4.157a.624.624 0 1 0 .884-.884zM6.5 11c0-2.62 2.13-4.75 4.75-4.75S16 8.38 16 11s-2.13 4.75-4.75 4.75S6.5 13.62 6.5 11z"></path></svg></span><input class="js-predictiveSearchInput textInput textInput--rounded textInput--darkText u-baseColor--textNormal textInput--transparent" type="search" placeholder="Search" required="true" data-collection-id="7f60cf5620c9"></label><a class="button button--small button--chromeless u-sm-show is-inSiteNavBar u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--chromeless u-xs-top1" href="https://towardsdatascience.com/search" title="Search" aria-label="Search"><span class="button-defaultState"><span class="svgIcon svgIcon--search svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M20.067 18.933l-4.157-4.157a6 6 0 1 0-.884.884l4.157 4.157a.624.624 0 1 0 .884-.884zM6.5 11c0-2.62 2.13-4.75 4.75-4.75S16 8.38 16 11s-2.13 4.75-4.75 4.75S6.5 13.62 6.5 11z"></path></svg></span></span></a><button class="button button--small button--chromeless is-inSiteNavBar u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--activity js-notificationsButton u-marginRight16 u-xs-marginRight10 u-lineHeight0 u-size25x25" title="Notifications" aria-label="Notifications" data-action="open-notifications"><span class="svgIcon svgIcon--bell svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="-293 409 25 25"><path d="M-273.327 423.67l-1.673-1.52v-3.646a5.5 5.5 0 0 0-6.04-5.474c-2.86.273-4.96 2.838-4.96 5.71v3.41l-1.68 1.553c-.204.19-.32.456-.32.734V427a1 1 0 0 0 1 1h3.49a3.079 3.079 0 0 0 3.01 2.45 3.08 3.08 0 0 0 3.01-2.45h3.49a1 1 0 0 0 1-1v-2.59c0-.28-.12-.55-.327-.74zm-7.173 5.63c-.842 0-1.55-.546-1.812-1.3h3.624a1.92 1.92 0 0 1-1.812 1.3zm6.35-2.45h-12.7v-2.347l1.63-1.51c.236-.216.37-.522.37-.843v-3.41c0-2.35 1.72-4.356 3.92-4.565a4.353 4.353 0 0 1 4.78 4.33v3.645c0 .324.137.633.376.85l1.624 1.477v2.373z"></path></svg></span></button><button class="button button--chromeless u-baseColor--buttonNormal is-inSiteNavBar js-userActions" aria-haspopup="true" data-action="open-userActions"><div class="avatar"><img src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/0__vE9uUIWyOu7cYeP_" class="avatar-image avatar-image--icon" alt="HamidReza Mirkhani"></div></button></div></div></div></div><div class="u-tintBgColor u-tintSpectrum "><div class="metabar-inner u-marginAuto u-maxWidth1032 u-paddingHorizontal20 js-metabarBottom"><nav role="navigation" class="metabar-block metabar-block--below u-flexCenter u-overflowHidden u-height54"><div class="u-flexCenter u-overflowHidden"><div class="u-marginRight40"><a href="https://towardsdatascience.com/?source=logo-5f069cdb2c1b---7f60cf5620c9" class="u-flexCenter js-collectionLogoOrName"><img height="36" width="97" src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/1_5EUO1kUYBthpOCPzRj_l2g.png" alt="Towards Data Science"></a></div><div class="u-overflowHidden u-xs-hide"><ul class="u-textAlignLeft u-noWrap u-overflowX u-height80 u-marginTop40 js-collectionNavItems"><li class="metabar-navItem js-collectionNavItem u-inlineBlock u-fontSize13 u-textUppercase u-letterSpacing1px u-textColorNormal u-xs-paddingRight12 u-xs-marginRight0 u-xs-paddingTop10"><a class="link link--darken u-accentColor--textDarken link--noUnderline u-baseColor--link js-navItemLink" href="https://towardsdatascience.com/data-science/home">Data Science</a></li><li class="metabar-navItem js-collectionNavItem u-inlineBlock u-fontSize13 u-textUppercase u-letterSpacing1px u-textColorNormal u-xs-paddingRight12 u-xs-marginRight0 u-xs-paddingTop10"><a class="link link--darken u-accentColor--textDarken link--noUnderline u-baseColor--link js-navItemLink" href="https://towardsdatascience.com/machine-learning/home">Machine Learning</a></li><li class="metabar-navItem js-collectionNavItem u-inlineBlock u-fontSize13 u-textUppercase u-letterSpacing1px u-textColorNormal u-xs-paddingRight12 u-xs-marginRight0 u-xs-paddingTop10"><a class="link link--darken u-accentColor--textDarken link--noUnderline u-baseColor--link js-navItemLink" href="https://towardsdatascience.com/programming/home">Programming</a></li><li class="metabar-navItem js-collectionNavItem u-inlineBlock u-fontSize13 u-textUppercase u-letterSpacing1px u-textColorNormal u-xs-paddingRight12 u-xs-marginRight0 u-xs-paddingTop10"><a class="link link--darken u-accentColor--textDarken link--noUnderline u-baseColor--link js-navItemLink" href="https://towardsdatascience.com/data-visualization/home">Visualization</a></li><li class="metabar-navItem js-collectionNavItem u-inlineBlock u-fontSize13 u-textUppercase u-letterSpacing1px u-textColorNormal u-xs-paddingRight12 u-xs-marginRight0 u-xs-paddingTop10"><a class="link link--darken u-accentColor--textDarken link--noUnderline u-baseColor--link js-navItemLink" href="https://towardsdatascience.com/artificial-intelligence/home">AI</a></li><li class="metabar-navItem js-collectionNavItem u-inlineBlock u-fontSize13 u-textUppercase u-letterSpacing1px u-textColorNormal u-xs-paddingRight12 u-xs-marginRight0 u-xs-paddingTop10"><a class="link link--darken u-accentColor--textDarken link--noUnderline u-baseColor--link js-navItemLink" href="https://towardsdatascience.com/data-journalism/home">Journalism</a></li><li class="metabar-navItem js-collectionNavItem u-inlineBlock u-fontSize13 u-textUppercase u-letterSpacing1px u-textColorNormal u-xs-paddingRight12 u-xs-marginRight0 u-xs-paddingTop10"><a class="link link--darken u-accentColor--textDarken link--noUnderline u-baseColor--link js-navItemLink" href="https://towardsdatascience.com/toronto-machine-learning-summit-8bae371d4bb1">Events</a></li><span class="u-borderLeft1 u-baseColor--borderLight"></span><li class="metabar-navItem js-collectionNavItem is-external u-inlineBlock u-fontSize13 u-textUppercase u-letterSpacing1px u-textColorNormal u-xs-paddingRight12 u-xs-marginRight0 u-xs-paddingTop10"><a class="link link--darkenOnHover u-accentColor--textDarken link--noUnderline u-baseColor--link js-navItemLink" href="https://towardsdatascience.com/contribute/home" rel="nofollow noopener" target="_blank">Submit</a></li></ul></div></div></nav></div></div></div><div class="metabar metabar--spacer js-metabarSpacer u-tintBgColor  u-height119 u-xs-height110"></div><main role="main"><article class=" u-minHeight100vhOffset65 u-overflowHidden postArticle postArticle--full is-withAccentColors" lang="en"><div class="postArticle-content js-postField js-notesSource js-trackPostScrolls" data-post-id="e66f2e30fb96" data-source="post_page" data-collection-id="7f60cf5620c9" data-tracking-context="postPage" data-scroll="native"><section name="17dc" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h1 name="ac92" id="ac92" class="graf graf--h3 graf--leading graf--title">Review: SegNet (Semantic Segmentation)</h1><h2 name="7054" id="7054" class="graf graf--h4 graf-after--h3 graf--subtitle">Encoder Decoder Architecture, Using Max Pooling Indices to Upsample, Outperforms FCN, DeepLabv1, DeconvNet</h2><div class="uiScale uiScale-ui--regular uiScale-caption--regular u-flexCenter u-marginVertical24 u-fontSize15 js-postMetaLockup"><div class="u-flex0"><a class="link u-baseColor--link avatar" href="https://towardsdatascience.com/@sh.tsang?source=post_header_lockup" data-action="show-user-card" data-action-source="post_header_lockup" data-action-value="aff72a0c1243" data-action-type="hover" data-user-id="aff72a0c1243" data-collection-slug="towards-data-science" dir="auto"><img src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/1_OxjNUHcLFU8-pp-j8su6pg.jpeg" class="avatar-image u-size50x50" alt="Go to the profile of Sik-Ho Tsang"></a></div><div class="u-flex1 u-paddingLeft15 u-overflowHidden"><div class="u-paddingBottom3"><a class="ds-link ds-link--styleSubtle ui-captionStrong u-inlineBlock link link--darken link--darker" href="https://towardsdatascience.com/@sh.tsang" data-action="show-user-card" data-action-value="aff72a0c1243" data-action-type="hover" data-user-id="aff72a0c1243" data-collection-slug="towards-data-science" dir="auto">Sik-Ho Tsang</a><span class="followState js-followState" data-user-id="aff72a0c1243"><button class="button button--smallest u-noUserSelect button--withChrome u-baseColor--buttonNormal button--withHover button--unblock js-unblockButton u-marginLeft10 u-xs-hide" data-action="toggle-block-user" data-action-value="aff72a0c1243" data-action-source="post_header_lockup"><span class="button-label  button-defaultState">Blocked</span><span class="button-label button-hoverState">Unblock</span></button><button class="button button--primary button--smallest button--dark u-noUserSelect button--withChrome u-accentColor--buttonDark button--follow js-followButton u-marginLeft10 u-xs-hide" data-action="toggle-subscribe-user" data-action-value="aff72a0c1243" data-action-source="post_header_lockup-aff72a0c1243-------------------------follow_byline" data-subscribe-source="post_header_lockup" data-follow-context-entity-id="e66f2e30fb96"><span class="button-label  button-defaultState js-buttonLabel">Follow</span><span class="button-label button-activeState">Following</span></button></span></div><div class="ui-caption u-noWrapWithEllipsis js-testPostMetaInlineSupplemental"><time datetime="2019-02-10T14:35:25.230Z">Feb 10</time><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="4 min read"></span></div></div></div><figure name="4c06" id="4c06" class="graf graf--figure graf-after--h4"><div class="aspectRatioPlaceholder is-locked" style="max-width: 600px; max-height: 338px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.3%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="0*k8ejti9_6CHwxzFQ.gif" data-width="600" data-height="338" data-is-featured="true" data-scroll="native"><img src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/0_k8ejti9_6CHwxzFQ.gif" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="40"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/0*k8ejti9_6CHwxzFQ.gif" src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/0_k8ejti9_6CHwxzFQ(1).gif"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/0*k8ejti9_6CHwxzFQ.gif"></noscript></div></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">SegNet by Authors (</strong><a href="https://www.youtube.com/watch?v=CxanE_W46ts" data-href="https://www.youtube.com/watch?v=CxanE_W46ts" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener" target="_blank">https://www.youtube.com/watch?v=CxanE_W46ts</a>)</figcaption></figure><p name="fed9" id="fed9" class="graf graf--p graf--hasDropCapModel graf--hasDropCap graf-after--figure"><span class="graf-dropCap">In</span> this story, <strong class="markup--strong markup--p-strong">SegNet</strong>, by <strong class="markup--strong markup--p-strong">University of Cambridge</strong>, is briefly reviewed. Originally, it was submitted to 2015 CVPR, but at last it is not being published in CVPR (But it’s <strong class="markup--strong markup--p-strong">2015 arXiv </strong>tech report version and still got over <strong class="markup--strong markup--p-strong">100 citations</strong>). Instead, it is published in <strong class="markup--strong markup--p-strong">2017 TPAMI</strong> with more than <strong class="markup--strong markup--p-strong">1800 citations</strong>. And right now the first author has become the Director of Deep Learning and AI in Magic Leap Inc. (<a href="https://medium.com/@sh.tsang" data-href="https://medium.com/@sh.tsang" data-anchor-type="2" data-user-id="aff72a0c1243" data-action-value="aff72a0c1243" data-action="show-user-card" data-action-type="hover" class="markup--user markup--p-user" target="_blank">Sik-Ho Tsang</a> @ Medium)</p><p name="74bc" id="74bc" class="graf graf--p graf-after--p">Below is the demo from authors:</p><figure name="fe91" id="fe91" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.2%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="56"></canvas><div class="iframeContainer"><iframe data-width="854" data-height="480" width="700" height="393" data-src="/media/87dea9894b49a6c02eb41704f9e3b6b8?postId=e66f2e30fb96" data-media-id="87dea9894b49a6c02eb41704f9e3b6b8" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2FCxanE_W46ts%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/87dea9894b49a6c02eb41704f9e3b6b8.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME data-width="854" data-height="480" width="700" height="393" src="/media/87dea9894b49a6c02eb41704f9e3b6b8?postId=e66f2e30fb96" data-media-id="87dea9894b49a6c02eb41704f9e3b6b8" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2FCxanE_W46ts%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">SegNet by Authors (</strong><a href="https://www.youtube.com/watch?v=CxanE_W46ts" data-href="https://www.youtube.com/watch?v=CxanE_W46ts" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener noopener" target="_blank">https://www.youtube.com/watch?v=CxanE_W46ts</a>)</figcaption></figure><p name="50b8" id="50b8" class="graf graf--p graf-after--figure">There is also an interesting demo that we can choose a random image or even upload our own image to try the SegNet. I have tried as below:</p><ul class="postList"><li name="a508" id="a508" class="graf graf--li graf-after--p"><a href="http://mi.eng.cam.ac.uk/projects/segnet/demo.php" data-href="http://mi.eng.cam.ac.uk/projects/segnet/demo.php" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">http://mi.eng.cam.ac.uk/projects/segnet/demo.php</a></li></ul><figure name="0623" id="0623" class="graf graf--figure graf-after--li graf--trailing"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 296px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 42.199999999999996%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*TX4rkqNaqX_aA-CAId8WPw.png" data-width="1217" data-height="514" data-action="zoom" data-action-value="1*TX4rkqNaqX_aA-CAId8WPw.png" data-scroll="native"><img src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/1_TX4rkqNaqX_aA-CAId8WPw.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="30"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*TX4rkqNaqX_aA-CAId8WPw.png" src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/1_TX4rkqNaqX_aA-CAId8WPw(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*TX4rkqNaqX_aA-CAId8WPw.png"></noscript></div></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">The segmentation result for a road scene image that I found from&nbsp;internet</strong></figcaption></figure></div></div></section><section name="b0ca" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="fb6c" id="fb6c" class="graf graf--h3 graf--leading">Outline</h3><ol class="postList"><li name="819f" id="819f" class="graf graf--li graf-after--h3"><strong class="markup--strong markup--li-strong">Encoder Decoder Architecture</strong></li><li name="62f2" id="62f2" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Differences from DeconvNet and U-Net</strong></li><li name="bca1" id="bca1" class="graf graf--li graf-after--li graf--trailing"><strong class="markup--strong markup--li-strong">Results</strong></li></ol></div></div></section><section name="bfb5" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="2e18" id="2e18" class="graf graf--h3 graf--leading"><strong class="markup--strong markup--h3-strong">1. Encoder Decoder Architecture</strong></h3></div><div class="section-inner sectionLayout--outsetColumn"><figure name="81ab" id="81ab" class="graf graf--figure graf--layoutOutsetCenter graf-after--h3" data-scroll="native"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1032px; max-height: 300px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 29.099999999999998%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*8qIwQ7drLTf08gami25QDw.png" data-width="1128" data-height="328" data-action="zoom" data-action-value="1*8qIwQ7drLTf08gami25QDw.png" data-scroll="native"><img src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/1_8qIwQ7drLTf08gami25QDw.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="20"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1200/1*8qIwQ7drLTf08gami25QDw.png" src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/1_8qIwQ7drLTf08gami25QDw(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1200/1*8qIwQ7drLTf08gami25QDw.png"></noscript></div></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">SegNet: Encoder Decoder Architecture</strong></figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><ul class="postList"><li name="8ea0" id="8ea0" class="graf graf--li graf-after--figure">SegNet has an <strong class="markup--strong markup--li-strong">encoder </strong>network and a corresponding <strong class="markup--strong markup--li-strong">decoder </strong>network, followed by a final pixelwise classification layer.</li></ul><h4 name="3999" id="3999" class="graf graf--h4 graf-after--li">1.1. Encoder</h4><ul class="postList"><li name="79cd" id="79cd" class="graf graf--li graf-after--h4">At the encoder, convolutions and max pooling are performed.</li><li name="e81f" id="e81f" class="graf graf--li graf-after--li">There are 13 convolutional layers from VGG-16. (The original fully connected layers are discarded.)</li><li name="f2a5" id="f2a5" class="graf graf--li graf-after--li">While doing 2×2 max pooling, the corresponding max pooling indices (locations) are stored.</li></ul><h4 name="36a8" id="36a8" class="graf graf--h4 graf-after--li">1.2. Decoder</h4><figure name="57ce" id="57ce" class="graf graf--figure graf-after--h4"><div class="aspectRatioPlaceholder is-locked" style="max-width: 234px; max-height: 221px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 94.39999999999999%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*wdNu8Wd2HOLsOfRUInwwaA.png" data-width="234" data-height="221" data-scroll="native"><img src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/1_wdNu8Wd2HOLsOfRUInwwaA.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="70"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*wdNu8Wd2HOLsOfRUInwwaA.png" src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/1_wdNu8Wd2HOLsOfRUInwwaA(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*wdNu8Wd2HOLsOfRUInwwaA.png"></noscript></div></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">Upsampling Using Max-Pooling Indices</strong></figcaption></figure><ul class="postList"><li name="0795" id="0795" class="graf graf--li graf-after--figure">At the decoder, upsampling and convolutions are performed. At the end, there is softmax classifier for each pixel.</li><li name="2199" id="2199" class="graf graf--li graf-after--li">During upsampling, the max pooling indices at the corresponding encoder layer are recalled to upsample as shown above.</li><li name="89fb" id="89fb" class="graf graf--li graf-after--li graf--trailing">Finally, a K-class softmax classifier is used to predict the class for each pixel.</li></ul></div></div></section><section name="eb87" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="8c52" id="8c52" class="graf graf--h3 graf--leading"><strong class="markup--strong markup--h3-strong">2. Differences from DeconvNet and&nbsp;U-Net</strong></h3><p name="8a4d" id="8a4d" class="graf graf--p graf-after--h3"><a href="https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e" data-href="https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener noopener" target="_blank">DeconvNet</a> and <a href="http://u-net/" data-href="http://U-Net" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">U-Net</a> have similar structures as SegNet.</p><h4 name="ac87" id="ac87" class="graf graf--h4 graf-after--p">2.1. Differences from <a href="https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e" data-href="https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e" class="markup--anchor markup--h4-anchor" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener noopener" target="_blank">DeconvNet</a></h4><ul class="postList"><li name="c597" id="c597" class="graf graf--li graf-after--h4">Similar upsampling approach called unpooling is used.</li><li name="a98e" id="a98e" class="graf graf--li graf-after--li">However, there are fully-connected layers which make the model larger.</li></ul><h4 name="a741" id="a741" class="graf graf--h4 graf-after--li">2.2. Differences from&nbsp;<a href="http://u-net/" data-href="http://U-Net" class="markup--anchor markup--h4-anchor" rel="noopener" target="_blank">U-Net</a></h4><ul class="postList"><li name="9ffc" id="9ffc" class="graf graf--li graf-after--h4">It is used for biomedical image segmentation.</li><li name="78db" id="78db" class="graf graf--li graf-after--li">Instead of using pooling indices, the entire feature maps are transfer from encoder to decoder, then with concatenation to perform convolution.</li><li name="1228" id="1228" class="graf graf--li graf-after--li graf--trailing">This makes the model larger and need more memory.</li></ul></div></div></section><section name="760b" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="286c" id="286c" class="graf graf--h3 graf--leading">3. Results</h3><ul class="postList"><li name="4391" id="4391" class="graf graf--li graf-after--h3">Two datasets are tried. One is CamVid dataset for Road Scene Segmentation. One is SUN RGB-D dataset for Indoor Scene Segmentation.</li></ul><h4 name="c8c7" id="c8c7" class="graf graf--h4 graf-after--li">3.1. CamVid dataset for Road Scene Segmentation</h4></div><div class="section-inner sectionLayout--outsetColumn"><figure name="5eac" id="5eac" class="graf graf--figure graf--layoutOutsetCenter graf-after--h4" data-scroll="native"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1032px; max-height: 327px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 31.7%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*GHGbOKojBBvtxyBlG8XAlw.png" data-width="1186" data-height="376" data-action="zoom" data-action-value="1*GHGbOKojBBvtxyBlG8XAlw.png" data-scroll="native"><img src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/1_GHGbOKojBBvtxyBlG8XAlw.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="22"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1200/1*GHGbOKojBBvtxyBlG8XAlw.png" src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/1_GHGbOKojBBvtxyBlG8XAlw(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1200/1*GHGbOKojBBvtxyBlG8XAlw.png"></noscript></div></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">Compared With Conventional Approaches on CamVid dataset for Road Scene Segmentation</strong></figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><ul class="postList"><li name="cc90" id="cc90" class="graf graf--li graf-after--figure">As shown above, SegNet obtains very good results for many classes. It also got the highest class average and global average.</li></ul></div><div class="section-inner sectionLayout--outsetColumn"><figure name="daea" id="daea" class="graf graf--figure graf--layoutOutsetCenter graf-after--li" data-scroll="native"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1032px; max-height: 204px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 19.8%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*aSanrhujo09-PYNneyPBvw.png" data-width="1116" data-height="221" data-action="zoom" data-action-value="1*aSanrhujo09-PYNneyPBvw.png" data-scroll="native"><img src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/1_aSanrhujo09-PYNneyPBvw.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="12"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1200/1*aSanrhujo09-PYNneyPBvw.png" src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/1_aSanrhujo09-PYNneyPBvw(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1200/1*aSanrhujo09-PYNneyPBvw.png"></noscript></div></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">Compared With Deep Learning Approaches on CamVid dataset for Road Scene Segmentation</strong></figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><ul class="postList"><li name="4cdd" id="4cdd" class="graf graf--li graf-after--figure">SegNet obtains highest global average accuracy (G), class average accuracy (C), mIOU and Boundary F1-measure (BF). It outperforms <a href="https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1" data-href="https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1" class="markup--anchor markup--li-anchor" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener noopener" target="_blank">FCN</a>, <a href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d" data-href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d" class="markup--anchor markup--li-anchor" target="_blank">DeepLabv1 </a>and <a href="https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e" data-href="https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e" class="markup--anchor markup--li-anchor" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener noopener" target="_blank">DeconvNet</a>.</li></ul></div><div class="section-inner sectionLayout--outsetColumn"><figure name="833b" id="833b" class="graf graf--figure graf--layoutOutsetCenter graf-after--li" data-scroll="native"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1032px; max-height: 1063px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 103%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*1YEl-0zr77hfUetR-HmnJw.png" data-width="1083" data-height="1116" data-action="zoom" data-action-value="1*1YEl-0zr77hfUetR-HmnJw.png" data-scroll="native"><img src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/1_1YEl-0zr77hfUetR-HmnJw.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1200/1*1YEl-0zr77hfUetR-HmnJw.png" src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/1_1YEl-0zr77hfUetR-HmnJw(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1200/1*1YEl-0zr77hfUetR-HmnJw.png"></noscript></div></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">Qualitative Results</strong></figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><h4 name="5087" id="5087" class="graf graf--h4 graf-after--figure">3.2. SUN RGB-D Dataset for Indoor Scene Segmentation</h4><ul class="postList"><li name="7f65" id="7f65" class="graf graf--li graf-after--h4">Only RGB is used, depth (D) information are not used.</li></ul></div><div class="section-inner sectionLayout--outsetColumn"><figure name="bee0" id="bee0" class="graf graf--figure graf--layoutOutsetCenter graf-after--li" data-scroll="native"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1032px; max-height: 177px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 17.1%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*tB_mufCuOLK8imyUHceSVQ.png" data-width="1099" data-height="188" data-action="zoom" data-action-value="1*tB_mufCuOLK8imyUHceSVQ.png" data-scroll="native"><img src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/1_tB_mufCuOLK8imyUHceSVQ.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="12"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1200/1*tB_mufCuOLK8imyUHceSVQ.png" src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/1_tB_mufCuOLK8imyUHceSVQ(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1200/1*tB_mufCuOLK8imyUHceSVQ.png"></noscript></div></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">Compared With Deep Learning Approaches on SUN RGB-D Dataset for Indoor Scene Segmentation</strong></figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><ul class="postList"><li name="21ab" id="21ab" class="graf graf--li graf-after--figure">Again, SegNet outperforms <a href="https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1" data-href="https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1" class="markup--anchor markup--li-anchor" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener noopener" target="_blank">FCN</a>, <a href="https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e" data-href="https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e" class="markup--anchor markup--li-anchor" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener noopener" target="_blank">DeconvNet</a>, and <a href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d" data-href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">DeepLabv1</a>.</li><li name="d63b" id="d63b" class="graf graf--li graf-after--li">SegNet only got a bit inferior to <a href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d" data-href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">DeepLabv1</a> for mIOU.</li></ul></div><div class="section-inner sectionLayout--outsetColumn"><figure name="4090" id="4090" class="graf graf--figure graf--layoutOutsetCenter graf-after--li" data-scroll="native"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1032px; max-height: 138px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 13.3%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*_7h5yLmGj4gbh-aD7_-Ydw.png" data-width="1200" data-height="160" data-action="zoom" data-action-value="1*_7h5yLmGj4gbh-aD7_-Ydw.png" data-scroll="native"><img src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/1__7h5yLmGj4gbh-aD7_-Ydw.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="10"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1200/1*_7h5yLmGj4gbh-aD7_-Ydw.png" src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/1__7h5yLmGj4gbh-aD7_-Ydw(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1200/1*_7h5yLmGj4gbh-aD7_-Ydw.png"></noscript></div></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">Class Average Accuracy for Different Classes</strong></figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><ul class="postList"><li name="cf47" id="cf47" class="graf graf--li graf-after--figure">Higher accuracy for large-size classes.</li><li name="19c9" id="19c9" class="graf graf--li graf-after--li">Lower accuracy for small-size classes.</li></ul></div><div class="section-inner sectionLayout--outsetColumn"><figure name="41e8" id="41e8" class="graf graf--figure graf--layoutOutsetCenter graf-after--li" data-scroll="native"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1032px; max-height: 939px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 91%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*F0WEadFwdQ1JWyHaH7UoPw.png" data-width="1204" data-height="1096" data-action="zoom" data-action-value="1*F0WEadFwdQ1JWyHaH7UoPw.png" data-scroll="native"><img src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/1_F0WEadFwdQ1JWyHaH7UoPw.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="67"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1200/1*F0WEadFwdQ1JWyHaH7UoPw.png" src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/1_F0WEadFwdQ1JWyHaH7UoPw(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1200/1*F0WEadFwdQ1JWyHaH7UoPw.png"></noscript></div></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">Qualitative Results</strong></figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><h4 name="fb52" id="fb52" class="graf graf--h4 graf-after--figure">3.3. Memory and Inference Time</h4></div><div class="section-inner sectionLayout--outsetColumn"><figure name="576d" id="576d" class="graf graf--figure graf--layoutOutsetCenter graf-after--h4" data-scroll="native"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1032px; max-height: 116px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 11.200000000000001%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*q9-COyyJ2d8oK6XhuKzb-g.png" data-width="1233" data-height="138" data-action="zoom" data-action-value="1*q9-COyyJ2d8oK6XhuKzb-g.png" data-scroll="native"><img src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/1_q9-COyyJ2d8oK6XhuKzb-g.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="7"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1200/1*q9-COyyJ2d8oK6XhuKzb-g.png" src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/1_q9-COyyJ2d8oK6XhuKzb-g(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1200/1*q9-COyyJ2d8oK6XhuKzb-g.png"></noscript></div></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">Memory and Inference Time</strong></figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><ul class="postList"><li name="25d8" id="25d8" class="graf graf--li graf-after--figure">SegNet is slower than <a href="https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1" data-href="https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1" class="markup--anchor markup--li-anchor" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener noopener" target="_blank">FCN</a> and <a href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d" data-href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">DeepLabv1</a> because SegNet contains the decoder architecture. And it is faster than <a href="https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e" data-href="https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e" class="markup--anchor markup--li-anchor" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener noopener" target="_blank">DeconvNet</a> because it does not have fully connected layers.</li><li name="cfd6" id="cfd6" class="graf graf--li graf-after--li graf--trailing">And SegNet has low memory requirement during both training and testing. And the model size is much smaller than <a href="https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1" data-href="https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1" class="markup--anchor markup--li-anchor" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener noopener" target="_blank">FCN</a> and <a href="https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e" data-href="https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e" class="markup--anchor markup--li-anchor" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener noopener" target="_blank">DeconvNet</a>.</li></ul></div></div></section><section name="433c" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h4 name="de3b" id="de3b" class="graf graf--h4 graf--leading">References</h4><p name="9eaa" id="9eaa" class="graf graf--p graf-after--h4">[2015 arXiv] [SegNet]<br><a href="https://arxiv.org/abs/1505.07293" data-href="https://arxiv.org/abs/1505.07293" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling</a></p><p name="3317" id="3317" class="graf graf--p graf-after--p">[2017 TPAMI] [SegNet]<br><a href="https://arxiv.org/abs/1511.00561" data-href="https://arxiv.org/abs/1511.00561" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</a></p><h4 name="93e9" id="93e9" class="graf graf--h4 graf-after--p">My Previous&nbsp;Reviews</h4><p name="4da9" id="4da9" class="graf graf--p graf-after--h4"><strong class="markup--strong markup--p-strong">Image Classification<br></strong>[<a href="https://medium.com/@sh.tsang/paper-brief-review-of-lenet-1-lenet-4-lenet-5-boosted-lenet-4-image-classification-1f5f809dbf17" data-href="https://medium.com/@sh.tsang/paper-brief-review-of-lenet-1-lenet-4-lenet-5-boosted-lenet-4-image-classification-1f5f809dbf17" class="markup--anchor markup--p-anchor" target="_blank">LeNet</a>] [<a href="https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160" data-href="https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160" class="markup--anchor markup--p-anchor" target="_blank">AlexNet</a>] [<a href="https://medium.com/coinmonks/paper-review-of-zfnet-the-winner-of-ilsvlc-2013-image-classification-d1a5a0c45103" data-href="https://medium.com/coinmonks/paper-review-of-zfnet-the-winner-of-ilsvlc-2013-image-classification-d1a5a0c45103" class="markup--anchor markup--p-anchor" target="_blank">ZFNet</a>] [<a href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" data-href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" class="markup--anchor markup--p-anchor" target="_blank">VGGNet</a>] [<a href="https://medium.com/coinmonks/review-sppnet-1st-runner-up-object-detection-2nd-runner-up-image-classification-in-ilsvrc-906da3753679" data-href="https://medium.com/coinmonks/review-sppnet-1st-runner-up-object-detection-2nd-runner-up-image-classification-in-ilsvrc-906da3753679" class="markup--anchor markup--p-anchor" target="_blank">SPPNet</a>] [<a href="https://medium.com/coinmonks/review-prelu-net-the-first-to-surpass-human-level-performance-in-ilsvrc-2015-image-f619dddd5617" data-href="https://medium.com/coinmonks/review-prelu-net-the-first-to-surpass-human-level-performance-in-ilsvrc-2015-image-f619dddd5617" class="markup--anchor markup--p-anchor" target="_blank">PReLU-Net</a>] [<a href="https://towardsdatascience.com/review-stn-spatial-transformer-network-image-classification-d3cbd98a70aa" data-href="https://towardsdatascience.com/review-stn-spatial-transformer-network-image-classification-d3cbd98a70aa" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener" target="_blank">STN</a>] [<a href="https://medium.com/@sh.tsang/review-deep-image-a-big-data-solution-for-image-recognition-99e5f7b1c802" data-href="https://medium.com/@sh.tsang/review-deep-image-a-big-data-solution-for-image-recognition-99e5f7b1c802" class="markup--anchor markup--p-anchor" target="_blank">DeepImage</a>] [<a href="https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7" data-href="https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7" class="markup--anchor markup--p-anchor" target="_blank">GoogLeNet / Inception-v1</a>] [<a href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" data-href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" class="markup--anchor markup--p-anchor" target="_blank">BN-Inception / Inception-v2</a>] [<a href="https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c" data-href="https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c" class="markup--anchor markup--p-anchor" target="_blank">Inception-v3</a>] [<a href="https://towardsdatascience.com/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc" data-href="https://towardsdatascience.com/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc" class="markup--anchor markup--p-anchor" rel="noopener nofollow noopener noopener nofollow noopener noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">Inception-v4</a>] [<a href="https://towardsdatascience.com/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568" data-href="https://towardsdatascience.com/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568" class="markup--anchor markup--p-anchor" rel="noopener nofollow noopener noopener nofollow noopener noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">Xception</a>] [<a href="https://towardsdatascience.com/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69" data-href="https://towardsdatascience.com/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69" class="markup--anchor markup--p-anchor" rel="noopener nofollow noopener noopener nofollow noopener noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">MobileNetV1</a>] [<a href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8" data-href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8" class="markup--anchor markup--p-anchor" rel="noopener nofollow noopener noopener nofollow noopener noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">ResNet</a>] [<a href="https://towardsdatascience.com/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e" data-href="https://towardsdatascience.com/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e" class="markup--anchor markup--p-anchor" rel="noopener nofollow noopener noopener nofollow noopener noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">Pre-Activation ResNet</a>] [<a href="https://medium.com/@sh.tsang/review-rir-resnet-in-resnet-image-classification-be4c79fde8ba" data-href="https://medium.com/@sh.tsang/review-rir-resnet-in-resnet-image-classification-be4c79fde8ba" class="markup--anchor markup--p-anchor" target="_blank">RiR</a>] [<a href="https://towardsdatascience.com/review-ror-resnet-of-resnet-multilevel-resnet-image-classification-cd3b0fcc19bb" data-href="https://towardsdatascience.com/review-ror-resnet-of-resnet-multilevel-resnet-image-classification-cd3b0fcc19bb" class="markup--anchor markup--p-anchor" rel="noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">RoR</a>] [<a href="https://towardsdatascience.com/review-stochastic-depth-image-classification-a4e225807f4a" data-href="https://towardsdatascience.com/review-stochastic-depth-image-classification-a4e225807f4a" class="markup--anchor markup--p-anchor" rel="noopener nofollow noopener noopener nofollow noopener noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">Stochastic Depth</a>] [<a href="https://towardsdatascience.com/review-wrns-wide-residual-networks-image-classification-d3feb3fb2004" data-href="https://towardsdatascience.com/review-wrns-wide-residual-networks-image-classification-d3feb3fb2004" class="markup--anchor markup--p-anchor" rel="noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">WRN</a>] [<a href="https://medium.com/datadriveninvestor/review-fractalnet-image-classification-c5bdd855a090" data-href="https://medium.com/datadriveninvestor/review-fractalnet-image-classification-c5bdd855a090" class="markup--anchor markup--p-anchor" target="_blank">FractalNet</a>] [<a href="https://towardsdatascience.com/review-trimps-soushen-winner-in-ilsvrc-2016-image-classification-dfbc423111dd" data-href="https://towardsdatascience.com/review-trimps-soushen-winner-in-ilsvrc-2016-image-classification-dfbc423111dd" class="markup--anchor markup--p-anchor" rel="noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">Trimps-Soushen</a>] [<a href="https://towardsdatascience.com/review-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea" data-href="https://towardsdatascience.com/review-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea" class="markup--anchor markup--p-anchor" rel="noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">PolyNet</a>] [<a href="https://towardsdatascience.com/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac" data-href="https://towardsdatascience.com/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac" class="markup--anchor markup--p-anchor" rel="noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">ResNeXt</a>] [<a href="https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803" data-href="https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener noopener nofollow noopener noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">DenseNet</a>] [<a href="https://medium.com/@sh.tsang/review-pyramidnet-deep-pyramidal-residual-networks-image-classification-85a87b60ae78" data-href="https://medium.com/@sh.tsang/review-pyramidnet-deep-pyramidal-residual-networks-image-classification-85a87b60ae78" class="markup--anchor markup--p-anchor" target="_blank">PyramidNet</a>]</p><p name="8b77" id="8b77" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Object Detection<br></strong>[<a href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" data-href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" class="markup--anchor markup--p-anchor" target="_blank">OverFeat</a>] [<a href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" data-href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" class="markup--anchor markup--p-anchor" target="_blank">R-CNN</a>] [<a href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" data-href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" class="markup--anchor markup--p-anchor" target="_blank">Fast R-CNN</a>] [<a href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202" data-href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">Faster R-CNN</a>] [<a href="https://towardsdatascience.com/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6" data-href="https://towardsdatascience.com/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">DeepID-Net</a>] [<a href="https://towardsdatascience.com/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c" data-href="https://towardsdatascience.com/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c" class="markup--anchor markup--p-anchor" rel="nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">R-FCN</a>] [<a href="https://towardsdatascience.com/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766" data-href="https://towardsdatascience.com/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766" class="markup--anchor markup--p-anchor" rel="nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">ION</a>] [<a href="https://towardsdatascience.com/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413" data-href="https://towardsdatascience.com/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413" class="markup--anchor markup--p-anchor" rel="nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">MultiPathNet</a>] [<a href="https://medium.com/datadriveninvestor/review-noc-winner-in-2015-coco-ilsvrc-detection-object-detection-d5cc84e372a" data-href="https://medium.com/datadriveninvestor/review-noc-winner-in-2015-coco-ilsvrc-detection-object-detection-d5cc84e372a" class="markup--anchor markup--p-anchor" target="_blank">NoC</a>] [<a href="https://towardsdatascience.com/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4" data-href="https://towardsdatascience.com/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4" class="markup--anchor markup--p-anchor" rel="nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">G-RMI</a>] [<a href="https://medium.com/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151" data-href="https://medium.com/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151" class="markup--anchor markup--p-anchor" target="_blank">TDM</a>] [<a href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11" data-href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">SSD</a>] [<a href="https://towardsdatascience.com/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5" data-href="https://towardsdatascience.com/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">DSSD</a>] [<a href="https://towardsdatascience.com/yolov1-you-only-look-once-object-detection-e1f3ffec8a89" data-href="https://towardsdatascience.com/yolov1-you-only-look-once-object-detection-e1f3ffec8a89" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">YOLOv1</a>] [<a href="https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65" data-href="https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">YOLOv2 / YOLO9000</a>] [<a href="https://towardsdatascience.com/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6" data-href="https://towardsdatascience.com/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6" class="markup--anchor markup--p-anchor" target="_blank">YOLOv3</a>] [<a href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610" data-href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610" class="markup--anchor markup--p-anchor" rel="nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">FPN</a>] [<a href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4" data-href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4" class="markup--anchor markup--p-anchor" rel="nofollow noopener nofollow noopener noopener" target="_blank">RetinaNet</a>] [<a href="https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44" data-href="https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener" target="_blank">DCN</a>]</p><p name="6582" id="6582" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Semantic Segmentation<br></strong>[<a href="https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1" data-href="https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">FCN</a>] [<a href="https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e" data-href="https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">DeconvNet</a>] [<a href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d" data-href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">DeepLabv1 &amp; DeepLabv2</a>] [<a href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" data-href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" class="markup--anchor markup--p-anchor" target="_blank">ParseNet</a>] [<a href="https://towardsdatascience.com/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5" data-href="https://towardsdatascience.com/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">DilatedNet</a>] [<a href="https://towardsdatascience.com/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d" data-href="https://towardsdatascience.com/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">PSPNet</a>] [<a href="https://towardsdatascience.com/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74" data-href="https://towardsdatascience.com/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74" class="markup--anchor markup--p-anchor" rel="nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">DeepLabv3</a>]</p><p name="fc65" id="fc65" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Biomedical Image Segmentation<br></strong>[<a href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" data-href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" class="markup--anchor markup--p-anchor" target="_blank">CUMedVision1</a>] [<a href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" data-href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" class="markup--anchor markup--p-anchor" target="_blank">CUMedVision2 / DCAN</a>] [<a href="https://towardsdatascience.com/review-u-net-biomedical-image-segmentation-d02bf06ca760" data-href="https://towardsdatascience.com/review-u-net-biomedical-image-segmentation-d02bf06ca760" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">U-Net</a>] [<a href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" data-href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" class="markup--anchor markup--p-anchor" target="_blank">CFS-FCN</a>] [<a href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" data-href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" class="markup--anchor markup--p-anchor" target="_blank">U-Net+ResNet</a>]</p><p name="3134" id="3134" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Instance Segmentation<br></strong>[<a href="https://towardsdatascience.com/review-deepmask-instance-segmentation-30327a072339" data-href="https://towardsdatascience.com/review-deepmask-instance-segmentation-30327a072339" class="markup--anchor markup--p-anchor" rel="nofollow noopener nofollow noopener noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">DeepMask</a>] [<a href="https://towardsdatascience.com/review-sharpmask-instance-segmentation-6509f7401a61" data-href="https://towardsdatascience.com/review-sharpmask-instance-segmentation-6509f7401a61" class="markup--anchor markup--p-anchor" rel="nofollow noopener nofollow noopener noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">SharpMask</a>] [<a href="https://towardsdatascience.com/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413" data-href="https://towardsdatascience.com/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">MultiPathNet</a>] [<a href="https://towardsdatascience.com/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34" data-href="https://towardsdatascience.com/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34" class="markup--anchor markup--p-anchor" rel="nofollow noopener nofollow noopener noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">MNC</a>] [<a href="https://towardsdatascience.com/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92" data-href="https://towardsdatascience.com/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener nofollow noopener noopener" target="_blank">InstanceFCN</a>] [<a href="https://towardsdatascience.com/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2" data-href="https://towardsdatascience.com/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2" class="markup--anchor markup--p-anchor" rel="nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">FCIS</a>]</p><p name="58de" id="58de" class="graf graf--p graf-after--p graf--trailing"><strong class="markup--strong markup--p-strong">Super Resolution<br></strong>[<a href="https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c" data-href="https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c" class="markup--anchor markup--p-anchor" target="_blank">SRCNN</a>] [<a href="https://towardsdatascience.com/review-fsrcnn-super-resolution-80ca2ee14da4" data-href="https://towardsdatascience.com/review-fsrcnn-super-resolution-80ca2ee14da4" class="markup--anchor markup--p-anchor" rel="nofollow noopener nofollow noopener noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">FSRCNN</a>] [<a href="https://towardsdatascience.com/review-vdsr-super-resolution-f8050d49362f" data-href="https://towardsdatascience.com/review-vdsr-super-resolution-f8050d49362f" class="markup--anchor markup--p-anchor" rel="nofollow noopener nofollow noopener noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener" target="_blank">VDSR</a>] [<a href="https://medium.com/datadriveninvestor/review-espcn-real-time-sr-super-resolution-8dceca249350" data-href="https://medium.com/datadriveninvestor/review-espcn-real-time-sr-super-resolution-8dceca249350" class="markup--anchor markup--p-anchor" target="_blank">ESPCN</a>] [<a href="https://medium.com/datadriveninvestor/review-red-net-residual-encoder-decoder-network-denoising-super-resolution-cb6364ae161e" data-href="https://medium.com/datadriveninvestor/review-red-net-residual-encoder-decoder-network-denoising-super-resolution-cb6364ae161e" class="markup--anchor markup--p-anchor" target="_blank">RED-Net</a>] [<a href="https://medium.com/datadriveninvestor/review-drcn-deeply-recursive-convolutional-network-super-resolution-f0a380f79b20" data-href="https://medium.com/datadriveninvestor/review-drcn-deeply-recursive-convolutional-network-super-resolution-f0a380f79b20" class="markup--anchor markup--p-anchor" target="_blank">DRCN</a>] [<a href="https://towardsdatascience.com/review-drrn-deep-recursive-residual-network-super-resolution-dca4a35ce994" data-href="https://towardsdatascience.com/review-drrn-deep-recursive-residual-network-super-resolution-dca4a35ce994" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">DRRN</a>] [<a href="https://towardsdatascience.com/review-lapsrn-ms-lapsrn-laplacian-pyramid-super-resolution-network-super-resolution-c5fe2b65f5e8" data-href="https://towardsdatascience.com/review-lapsrn-ms-lapsrn-laplacian-pyramid-super-resolution-network-super-resolution-c5fe2b65f5e8" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LapSRN &amp; MS-LapSRN</a>]</p></div></div></section></div><footer class="u-paddingTop10"><div class="container u-maxWidth740"><div class="row"><div class="col u-size12of12"></div></div><div class="row"><div class="col u-size12of12 js-postTags"><div class="u-paddingBottom10"><ul class="tags tags--postTags tags--borderless"><li><a class="link u-baseColor--link" href="https://towardsdatascience.com/tagged/machine-learning?source=post" data-action-source="post" data-collection-slug="towards-data-science">Machine Learning</a></li><li><a class="link u-baseColor--link" href="https://towardsdatascience.com/tagged/deep-learning?source=post" data-action-source="post" data-collection-slug="towards-data-science">Deep Learning</a></li><li><a class="link u-baseColor--link" href="https://towardsdatascience.com/tagged/artificial-intelligence?source=post" data-action-source="post" data-collection-slug="towards-data-science">Artificial Intelligence</a></li><li><a class="link u-baseColor--link" href="https://towardsdatascience.com/tagged/data-science?source=post" data-action-source="post" data-collection-slug="towards-data-science">Data Science</a></li><li><a class="link u-baseColor--link" href="https://towardsdatascience.com/tagged/semantic-segmentation?source=post" data-action-source="post" data-collection-slug="towards-data-science">Semantic Segmentation</a></li></ul></div></div></div><div class="postActions js-postActionsFooter "><div class="u-flexCenter"><div class="u-flex1"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="e66f2e30fb96" data-is-icon-29px="true" data-is-circle="true" data-has-recommend-list="true" data-source="post_actions_footer-----e66f2e30fb96---------------------clap_footer" data-clap-string-singular="clap" data-clap-string-plural="claps"><div class="u-relative u-foreground"><button class="button button--large button--circle button--withChrome u-baseColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker clapButton--largePill u-relative u-foreground u-xs-paddingLeft13 u-width60 u-height60 u-accentColor--textNormal u-accentColor--buttonNormal clap-onboardingcollection" data-action="multivote" data-action-value="e66f2e30fb96" data-action-type="long-press" data-action-source="post_actions_footer-----e66f2e30fb96---------------------clap_footer" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--33px u-relative u-topNegative2 u-xs-top0"><svg class="svgIcon-use" width="33" height="33"><path d="M28.86 17.342l-3.64-6.402c-.292-.433-.712-.729-1.163-.8a1.124 1.124 0 0 0-.889.213c-.63.488-.742 1.181-.33 2.061l1.222 2.587 1.4 2.46c2.234 4.085 1.511 8.007-2.145 11.663-.26.26-.526.49-.797.707 1.42-.084 2.881-.683 4.292-2.094 3.822-3.823 3.565-7.876 2.05-10.395zm-6.252 11.075c3.352-3.35 3.998-6.775 1.978-10.469l-3.378-5.945c-.292-.432-.712-.728-1.163-.8a1.122 1.122 0 0 0-.89.213c-.63.49-.742 1.182-.33 2.061l1.72 3.638a.502.502 0 0 1-.806.568l-8.91-8.91a1.335 1.335 0 0 0-1.887 1.886l5.292 5.292a.5.5 0 0 1-.707.707l-5.292-5.292-1.492-1.492c-.503-.503-1.382-.505-1.887 0a1.337 1.337 0 0 0 0 1.886l1.493 1.492 5.292 5.292a.499.499 0 0 1-.353.854.5.5 0 0 1-.354-.147L5.642 13.96a1.338 1.338 0 0 0-1.887 0 1.338 1.338 0 0 0 0 1.887l2.23 2.228 3.322 3.324a.499.499 0 0 1-.353.853.502.502 0 0 1-.354-.146l-3.323-3.324a1.333 1.333 0 0 0-1.886 0 1.325 1.325 0 0 0-.39.943c0 .356.138.691.39.943l6.396 6.397c3.528 3.53 8.86 5.313 12.821 1.353zM12.73 9.26l5.68 5.68-.49-1.037c-.518-1.107-.426-2.13.224-2.89l-3.303-3.304a1.337 1.337 0 0 0-1.886 0 1.326 1.326 0 0 0-.39.944c0 .217.067.42.165.607zm14.787 19.184c-1.599 1.6-3.417 2.392-5.353 2.392-.349 0-.7-.03-1.058-.082a7.922 7.922 0 0 1-3.667.887c-3.049 0-6.115-1.626-8.359-3.87l-6.396-6.397A2.315 2.315 0 0 1 2 19.724a2.327 2.327 0 0 1 1.923-2.296l-.875-.875a2.339 2.339 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.647l-.139-.139c-.91-.91-.91-2.39 0-3.3.884-.884 2.421-.882 3.301 0l.138.14a2.335 2.335 0 0 1 3.948-1.24l.093.092c.091-.423.291-.828.62-1.157a2.336 2.336 0 0 1 3.3 0l3.384 3.386a2.167 2.167 0 0 1 1.271-.173c.534.086 1.03.354 1.441.765.11-.549.415-1.034.911-1.418a2.12 2.12 0 0 1 1.661-.41c.727.117 1.385.565 1.853 1.262l3.652 6.423c1.704 2.832 2.025 7.377-2.205 11.607zM13.217.484l-1.917.882 2.37 2.837-.454-3.719zm8.487.877l-1.928-.86-.44 3.697 2.368-2.837zM16.5 3.293L15.478-.005h2.044L16.5 3.293z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--33px u-relative u-topNegative2 u-xs-top0"><svg class="svgIcon-use" width="33" height="33"><g fill-rule="evenodd"><path d="M29.58 17.1l-3.854-6.78c-.365-.543-.876-.899-1.431-.989a1.491 1.491 0 0 0-1.16.281c-.42.327-.65.736-.7 1.207v.001l3.623 6.367c2.46 4.498 1.67 8.802-2.333 12.807-.265.265-.536.505-.81.728 1.973-.222 3.474-1.286 4.45-2.263 4.166-4.165 3.875-8.6 2.215-11.36zm-4.831.82l-3.581-6.3c-.296-.439-.725-.742-1.183-.815a1.105 1.105 0 0 0-.89.213c-.647.502-.755 1.188-.33 2.098l1.825 3.858a.601.601 0 0 1-.197.747.596.596 0 0 1-.77-.067L10.178 8.21c-.508-.506-1.393-.506-1.901 0a1.335 1.335 0 0 0-.393.95c0 .36.139.698.393.95v.001l5.61 5.61a.599.599 0 1 1-.848.847l-5.606-5.606c-.001 0-.002 0-.003-.002L5.848 9.375a1.349 1.349 0 0 0-1.902 0 1.348 1.348 0 0 0 0 1.901l1.582 1.582 5.61 5.61a.6.6 0 0 1-.848.848l-5.61-5.61c-.51-.508-1.393-.508-1.9 0a1.332 1.332 0 0 0-.394.95c0 .36.139.697.393.952l2.363 2.362c.002.001.002.002.002.003l3.52 3.52a.6.6 0 0 1-.848.847l-3.522-3.523h-.001a1.336 1.336 0 0 0-.95-.393 1.345 1.345 0 0 0-.949 2.295l6.779 6.78c3.715 3.713 9.327 5.598 13.49 1.434 3.527-3.528 4.21-7.13 2.086-11.015zM11.817 7.727c.06-.328.213-.64.466-.893.64-.64 1.755-.64 2.396 0l3.232 3.232c-.82.783-1.09 1.833-.764 2.992l-5.33-5.33z"></path><path d="M13.285.48l-1.916.881 2.37 2.837z"></path><path d="M21.719 1.361L19.79.501l-.44 3.697z"></path><path d="M16.502 3.298L15.481 0h2.043z"></path></g></svg></span></span></button><div class="clapUndo u-width60 u-round u-height32 u-absolute u-borderBox u-paddingRight5 u-transition--transform200Springu-backgroundGrayLighter js-clapUndo" style="top: 14px; padding: 2px;"><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon u-floatRight" data-action="multivote-undo" data-action-value="e66f2e30fb96"><span class="svgIcon svgIcon--removeThin svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M20.13 8.11l-5.61 5.61-5.609-5.61-.801.801 5.61 5.61-5.61 5.61.801.8 5.61-5.609 5.61 5.61.8-.801-5.609-5.61 5.61-5.61" fill-rule="evenodd"></path></svg></span></button></div></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft16"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-textColorDarker" data-action="show-recommends" data-action-value="e66f2e30fb96">191 claps</button><span class="u-xs-hide"></span></span></div></div><div class="buttonSet u-flex0"><a class="button button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon button--dark button--chromeless u-xs-hide u-marginRight12" href="https://medium.com/p/e66f2e30fb96/share/twitter" title="Share on Twitter" aria-label="Share on Twitter" target="_blank" data-action-source="post_actions_footer"><span class="button-defaultState"><span class="svgIcon svgIcon--twitterFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M22.053 7.54a4.474 4.474 0 0 0-3.31-1.455 4.526 4.526 0 0 0-4.526 4.524c0 .35.04.7.082 1.05a12.9 12.9 0 0 1-9.3-4.77c-.39.69-.61 1.46-.65 2.26.03 1.6.83 2.99 2.02 3.79-.72-.02-1.41-.22-2.02-.57-.01.02-.01.04 0 .08-.01 2.17 1.55 4 3.63 4.44-.39.08-.79.13-1.21.16-.28-.03-.57-.05-.81-.08.54 1.77 2.21 3.08 4.2 3.15a9.564 9.564 0 0 1-5.66 1.94c-.34-.03-.7-.06-1.05-.08 2 1.27 4.38 2.02 6.94 2.02 8.31 0 12.86-6.9 12.84-12.85.02-.24.01-.43 0-.65.89-.62 1.65-1.42 2.26-2.34-.82.38-1.69.62-2.59.72a4.37 4.37 0 0 0 1.94-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></span></span></a><a class="button button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon button--dark button--chromeless u-xs-hide u-marginRight12" href="https://medium.com/p/e66f2e30fb96/share/facebook" title="Share on Facebook" aria-label="Share on Facebook" target="_blank" data-action-source="post_actions_footer"><span class="button-defaultState"><span class="svgIcon svgIcon--facebookSquare svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M23.209 5H5.792A.792.792 0 0 0 5 5.791V23.21c0 .437.354.791.792.791h9.303v-7.125H12.72v-2.968h2.375v-2.375c0-2.455 1.553-3.662 3.741-3.662 1.049 0 1.95.078 2.213.112v2.565h-1.517c-1.192 0-1.469.567-1.469 1.397v1.963h2.969l-.594 2.968h-2.375L18.11 24h5.099a.791.791 0 0 0 .791-.791V5.79a.791.791 0 0 0-.791-.79"></path></svg></span></span></a><button class="button button--large button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon u-xs-show u-marginRight10" title="Share this story on Twitter or Facebook" aria-label="Share this story on Twitter or Facebook" data-action="show-share-popover" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--share svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M20.385 8H19a.5.5 0 1 0 .011 1h1.39c.43 0 .84.168 1.14.473.31.305.48.71.48 1.142v10.77c0 .43-.17.837-.47 1.142-.3.305-.71.473-1.14.473H8.62c-.43 0-.84-.168-1.144-.473a1.603 1.603 0 0 1-.473-1.142v-10.77c0-.43.17-.837.48-1.142A1.599 1.599 0 0 1 8.62 9H10a.502.502 0 0 0 0-1H8.615c-.67 0-1.338.255-1.85.766-.51.51-.765 1.18-.765 1.85v10.77c0 .668.255 1.337.766 1.848.51.51 1.18.766 1.85.766h11.77c.668 0 1.337-.255 1.848-.766.51-.51.766-1.18.766-1.85v-10.77c0-.668-.255-1.337-.766-1.848A2.61 2.61 0 0 0 20.384 8zm-8.67-2.508L14 3.207v8.362c0 .27.224.5.5.5s.5-.23.5-.5V3.2l2.285 2.285a.49.49 0 0 0 .704-.001.511.511 0 0 0 0-.708l-3.14-3.14a.504.504 0 0 0-.71 0L11 4.776a.501.501 0 0 0 .71.706" fill-rule="evenodd"></path></svg></span></button><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon" data-action="respond" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--response svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M21.27 20.058c1.89-1.826 2.754-4.17 2.754-6.674C24.024 8.21 19.67 4 14.1 4 8.53 4 4 8.21 4 13.384c0 5.175 4.53 9.385 10.1 9.385 1.007 0 2-.14 2.95-.41.285.25.592.49.918.7 1.306.87 2.716 1.31 4.19 1.31.276-.01.494-.14.6-.36a.625.625 0 0 0-.052-.65c-.61-.84-1.042-1.71-1.282-2.58a5.417 5.417 0 0 1-.154-.75zm-3.85 1.324l-.083-.28-.388.12a9.72 9.72 0 0 1-2.85.424c-4.96 0-8.99-3.706-8.99-8.262 0-4.556 4.03-8.263 8.99-8.263 4.95 0 8.77 3.71 8.77 8.27 0 2.25-.75 4.35-2.5 5.92l-.24.21v.32c0 .07 0 .19.02.37.03.29.1.6.19.92.19.7.49 1.4.89 2.08-.93-.14-1.83-.49-2.67-1.06-.34-.22-.88-.48-1.16-.74z"></path></svg></span></button><button class="button button--chromeless u-baseColor--buttonNormal u-marginRight12" data-action="scroll-to-responses">1</button><button class="button button--large button--dark button--chromeless is-touchIconFadeInPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="e66f2e30fb96" data-action-source="post_actions_footer"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--29px u-marginRight4"><svg class="svgIcon-use" width="29" height="29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4zM21 23l-5.91-3.955-.148-.107a.751.751 0 0 0-.884 0l-.147.107L8 23V6.615C8 5.725 8.725 5 9.615 5h9.77C20.275 5 21 5.725 21 6.615V23z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--29px u-marginRight4"><svg class="svgIcon-use" width="29" height="29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4z" fill-rule="evenodd"></path></svg></span></span></button><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon js-moreActionsButton" title="More actions" aria-label="More actions" data-action="more-actions"><span class="svgIcon svgIcon--more svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="-480.5 272.5 21 21"><path d="M-463 284.6c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5z"></path></svg></span></button></div></div></div></div><div class="u-maxWidth740 u-paddingTop20 u-marginTop20 u-borderTopLightest container u-paddingBottom20 u-xs-paddingBottom10 js-postAttributionFooterContainer"><div class="row js-postFooterInfo"><div class="col u-size6of12 u-xs-size12of12"><li class="uiScale uiScale-ui--small uiScale-caption--regular u-block u-paddingBottom18 js-cardUser"><div class="u-marginLeft20 u-floatRight"><span class="followState js-followState" data-user-id="aff72a0c1243"><button class="button button--small u-noUserSelect button--withChrome u-baseColor--buttonNormal button--withHover button--unblock js-unblockButton" data-action="toggle-block-user" data-action-value="aff72a0c1243" data-action-source="footer_card"><span class="button-label  button-defaultState">Blocked</span><span class="button-label button-hoverState">Unblock</span></button><button class="button button--primary button--small u-noUserSelect button--withChrome u-accentColor--buttonNormal button--follow js-followButton" data-action="toggle-subscribe-user" data-action-value="aff72a0c1243" data-action-source="footer_card-aff72a0c1243-------------------------follow_footer" data-subscribe-source="footer_card" data-follow-context-entity-id="e66f2e30fb96"><span class="button-label  button-defaultState js-buttonLabel">Follow</span><span class="button-label button-activeState">Following</span></button></span></div><div class="u-tableCell"><a class="link u-baseColor--link avatar" href="https://towardsdatascience.com/@sh.tsang?source=footer_card" title="Go to the profile of Sik-Ho Tsang" aria-label="Go to the profile of Sik-Ho Tsang" data-action-source="footer_card" data-user-id="aff72a0c1243" data-collection-slug="towards-data-science" dir="auto"><img src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/1_OxjNUHcLFU8-pp-j8su6pg(1).jpeg" class="avatar-image avatar-image--small" alt="Go to the profile of Sik-Ho Tsang"></a></div><div class="u-tableCell u-verticalAlignMiddle u-breakWord u-paddingLeft15"><h3 class="ui-h3 u-fontSize18 u-lineHeightTighter u-marginBottom4"><a class="link link--primary u-accentColor--hoverTextNormal" href="https://towardsdatascience.com/@sh.tsang" property="cc:attributionName" title="Go to the profile of Sik-Ho Tsang" aria-label="Go to the profile of Sik-Ho Tsang" rel="author cc:attributionUrl" data-user-id="aff72a0c1243" data-collection-slug="towards-data-science" dir="auto">Sik-Ho Tsang</a></h3><p class="ui-body u-fontSize14 u-lineHeightBaseSans u-textColorDark u-marginBottom4">PhD, Researcher. I share what I've learnt and done. :) My LinkedIn: <a rel="nofollow" href="https://www.linkedin.com/in/sh-tsang/">https://www.linkedin.com/in/sh-tsang/</a></p></div></li></div><div class="col u-size6of12 u-xs-size12of12 u-xs-marginTop30"><li class="uiScale uiScale-ui--small uiScale-caption--regular u-block u-paddingBottom18 js-cardCollection"><div class="u-marginLeft20 u-floatRight"><button class="button button--primary button--small u-noUserSelect button--withChrome u-accentColor--buttonNormal js-relationshipButton" data-action="toggle-follow-collection" data-action-source="footer_card----7f60cf5620c9----------------------follow_footer" data-collection-id="7f60cf5620c9"><span class="button-label  js-buttonLabel">Follow</span></button></div><div class="u-tableCell "><a class="link u-baseColor--link avatar avatar--roundedRectangle" href="https://towardsdatascience.com/?source=footer_card" title="Go to Towards Data Science" aria-label="Go to Towards Data Science" data-action-source="footer_card" data-collection-slug="towards-data-science"><img src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/1_F0LADxTtsKOgmPa-_7iUEQ.jpeg" class="avatar-image u-size60x60" alt="Towards Data Science"></a></div><div class="u-tableCell u-verticalAlignMiddle u-breakWord u-paddingLeft15"><h3 class="ui-h3 u-fontSize18 u-lineHeightTighter u-marginBottom4"><a class="link link--primary u-accentColor--hoverTextNormal" href="https://towardsdatascience.com/?source=footer_card" rel="collection" data-action-source="footer_card" data-collection-slug="towards-data-science">Towards Data Science</a></h3><p class="ui-body u-fontSize14 u-lineHeightBaseSans u-textColorDark u-marginBottom4">Sharing concepts, ideas, and codes.</p><div class="buttonSet"></div></div></li></div></div></div><div class="js-postFooterPlacements" data-post-id="e66f2e30fb96" data-collection-id="7f60cf5620c9" data-scroll="native"><div class="streamItem streamItem--placementCardGrid js-streamItem"><div class="u-clearfix u-backgroundGrayLightest"><div class="row u-marginAuto u-maxWidth1032 u-paddingTop30 u-paddingBottom40"><div class="col u-padding8 u-xs-size12of12 u-size4of12"><div class="uiScale uiScale-ui--small uiScale-caption--regular u-height280 u-width100pct u-backgroundWhite u-borderCardBorder u-boxShadow u-borderBox u-borderRadius4 js-trackPostPresentation" data-post-id="2991573a9195" data-source="placement_card_footer_grid---------0-41" data-tracking-context="placement" data-scroll="native"><a class="link link--noUnderline u-baseColor--link" href="https://towardsdatascience.com/12-things-i-learned-during-my-first-year-as-a-machine-learning-engineer-2991573a9195?source=placement_card_footer_grid---------0-41" data-action-source="placement_card_footer_grid---------0-41"><div class="u-backgroundCover u-backgroundColorGrayLight u-height100 u-width100pct u-borderBottomLight u-borderRadiusTop4" style="background-image: url(&quot;https://cdn-images-1.medium.com/fit/c/400/120/1*Objn6iYe6g4-DLDV67JKWA.jpeg&quot;); background-position: 50% 50% !important;"></div></a><div class="u-padding15 u-borderBox u-flexColumn u-height180"><a class="link link--noUnderline u-baseColor--link u-flex1" href="https://towardsdatascience.com/12-things-i-learned-during-my-first-year-as-a-machine-learning-engineer-2991573a9195?source=placement_card_footer_grid---------0-41" data-action-source="placement_card_footer_grid---------0-41"><div class="uiScale uiScale-ui--regular uiScale-caption--small u-textColorNormal u-marginBottom7"><div class="u-floatRight u-textColorNormal"><span class="svgIcon svgIcon--star svgIcon--15px"><svg class="svgIcon-use" width="15" height="15"><path d="M7.438 2.324c.034-.099.09-.099.123 0l1.2 3.53a.29.29 0 0 0 .26.19h3.884c.11 0 .127.049.038.111L9.8 8.327a.271.271 0 0 0-.099.291l1.2 3.53c.034.1-.011.131-.098.069l-3.142-2.18a.303.303 0 0 0-.32 0l-3.145 2.182c-.087.06-.132.03-.099-.068l1.2-3.53a.271.271 0 0 0-.098-.292L2.056 6.146c-.087-.06-.071-.112.038-.112h3.884a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div><div class="u-noWrapWithEllipsis u-marginRight40">More from Towards Data Science</div></div><div class="ui-h3 ui-clamp2 u-textColorDarkest u-contentSansBold u-fontSize24 u-maxHeight2LineHeightTighter u-lineClamp2 u-textOverflowEllipsis u-letterSpacingTight u-paddingBottom2">12 Things I Learned During My First Year as a Machine Learning Engineer</div></a><div class="u-paddingBottom10 u-flex0 u-flexCenter"><div class="u-flex1 u-minWidth0 u-marginRight10"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://towardsdatascience.com/@mrdbourke" data-action="show-user-card" data-action-value="dbc019e228f5" data-action-type="hover" data-user-id="dbc019e228f5" data-collection-slug="towards-data-science" dir="auto"><div class="u-relative u-inlineBlock u-flex0"><img src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/1_Rmq5dGAMlFDSSm6ZuZE9sg.png" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Daniel Bourke"><div class="avatar-halo u-absolute u-textColorGreenNormal svgIcon" style="width: calc(100% + 10px); height: calc(100% + 10px); top:-5px; left:-5px"><svg viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M3.44615311,11.6601601 C6.57294867,5.47967718 12.9131553,1.5 19.9642857,1.5 C27.0154162,1.5 33.3556228,5.47967718 36.4824183,11.6601601 L37.3747245,11.2087295 C34.0793076,4.69494641 27.3961457,0.5 19.9642857,0.5 C12.5324257,0.5 5.84926381,4.69494641 2.55384689,11.2087295 L3.44615311,11.6601601 Z"></path><path d="M36.4824183,28.2564276 C33.3556228,34.4369105 27.0154162,38.4165876 19.9642857,38.4165876 C12.9131553,38.4165876 6.57294867,34.4369105 3.44615311,28.2564276 L2.55384689,28.7078582 C5.84926381,35.2216412 12.5324257,39.4165876 19.9642857,39.4165876 C27.3961457,39.4165876 34.0793076,35.2216412 37.3747245,28.7078582 L36.4824183,28.2564276 Z"></path></svg></div></div></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--darker" href="https://towardsdatascience.com/@mrdbourke?source=placement_card_footer_grid---------0-41" data-action="show-user-card" data-action-source="placement_card_footer_grid---------0-41" data-action-value="dbc019e228f5" data-action-type="hover" data-user-id="dbc019e228f5" data-collection-slug="towards-data-science" dir="auto">Daniel Bourke</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://towardsdatascience.com/12-things-i-learned-during-my-first-year-as-a-machine-learning-engineer-2991573a9195?source=placement_card_footer_grid---------0-41" data-action="open-post" data-action-value="https://towardsdatascience.com/12-things-i-learned-during-my-first-year-as-a-machine-learning-engineer-2991573a9195?source=placement_card_footer_grid---------0-41" data-action-source="preview-listing"><time datetime="2019-07-06T14:11:54.400Z">Jul 6</time></a><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="11 min read"></span><span class="u-paddingLeft4"><span class="svgIcon svgIcon--star svgIcon--15px"><svg class="svgIcon-use" width="15" height="15"><path d="M7.438 2.324c.034-.099.09-.099.123 0l1.2 3.53a.29.29 0 0 0 .26.19h3.884c.11 0 .127.049.038.111L9.8 8.327a.271.271 0 0 0-.099.291l1.2 3.53c.034.1-.011.131-.098.069l-3.142-2.18a.303.303 0 0 0-.32 0l-3.145 2.182c-.087.06-.132.03-.099-.068l1.2-3.53a.271.271 0 0 0-.098-.292L2.056 6.146c-.087-.06-.071-.112.038-.112h3.884a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></span></div></div></div></div><div class="u-flex0 u-flexCenter"><div class="buttonSet"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="2991573a9195" data-is-label-padded="true" data-source="placement_card_footer_grid-----2991573a9195----0-41----------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="multivote" data-action-value="2991573a9195" data-action-type="long-press" data-action-source="placement_card_footer_grid-----2991573a9195----0-41----------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents u-marginLeft4" data-action="show-recommends" data-action-value="2991573a9195">3.6K</button></span></div></div><div class="u-height20 u-borderRightLighter u-inlineBlock u-relative u-marginRight10 u-marginLeft12"></div><div class="buttonSet"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="2991573a9195" data-action-source="placement_card_footer_grid-----2991573a9195----0-41----------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button></div></div></div></div></div></div><div class="col u-padding8 u-xs-size12of12 u-size4of12"><div class="uiScale uiScale-ui--small uiScale-caption--regular u-height280 u-width100pct u-backgroundWhite u-borderCardBorder u-boxShadow u-borderBox u-borderRadius4 js-trackPostPresentation" data-post-id="19893bd969bd" data-source="placement_card_footer_grid---------1-41" data-tracking-context="placement" data-scroll="native"><a class="link link--noUnderline u-baseColor--link" href="https://towardsdatascience.com/how-a-simple-mix-of-object-oriented-programming-can-sharpen-your-deep-learning-prototype-19893bd969bd?source=placement_card_footer_grid---------1-41" data-action-source="placement_card_footer_grid---------1-41"><div class="u-backgroundCover u-backgroundColorGrayLight u-height100 u-width100pct u-borderBottomLight u-borderRadiusTop4" style="background-image: url(&quot;https://cdn-images-1.medium.com/fit/c/400/120/1*uyQ61XHkTY2qHhovclaLIQ.png&quot;); background-position: 50% 50% !important;"></div></a><div class="u-padding15 u-borderBox u-flexColumn u-height180"><a class="link link--noUnderline u-baseColor--link u-flex1" href="https://towardsdatascience.com/how-a-simple-mix-of-object-oriented-programming-can-sharpen-your-deep-learning-prototype-19893bd969bd?source=placement_card_footer_grid---------1-41" data-action-source="placement_card_footer_grid---------1-41"><div class="uiScale uiScale-ui--regular uiScale-caption--small u-textColorNormal u-marginBottom7"><div class="u-floatRight u-textColorNormal"><span class="svgIcon svgIcon--star svgIcon--15px"><svg class="svgIcon-use" width="15" height="15"><path d="M7.438 2.324c.034-.099.09-.099.123 0l1.2 3.53a.29.29 0 0 0 .26.19h3.884c.11 0 .127.049.038.111L9.8 8.327a.271.271 0 0 0-.099.291l1.2 3.53c.034.1-.011.131-.098.069l-3.142-2.18a.303.303 0 0 0-.32 0l-3.145 2.182c-.087.06-.132.03-.099-.068l1.2-3.53a.271.271 0 0 0-.098-.292L2.056 6.146c-.087-.06-.071-.112.038-.112h3.884a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div><div class="u-noWrapWithEllipsis u-marginRight40">More from Towards Data Science</div></div><div class="ui-h3 ui-clamp2 u-textColorDarkest u-contentSansBold u-fontSize24 u-maxHeight2LineHeightTighter u-lineClamp2 u-textOverflowEllipsis u-letterSpacingTight u-paddingBottom2">How a simple mix of object-oriented programming can sharpen your deep learning prototype</div></a><div class="u-paddingBottom10 u-flex0 u-flexCenter"><div class="u-flex1 u-minWidth0 u-marginRight10"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://towardsdatascience.com/@tirthajyoti" data-action="show-user-card" data-action-value="cb9d97d4b61a" data-action-type="hover" data-user-id="cb9d97d4b61a" data-collection-slug="towards-data-science" dir="auto"><div class="u-relative u-inlineBlock u-flex0"><img src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/1_dROuRoTytntKE6LLBKKzKA.jpeg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Tirthajyoti Sarkar"><div class="avatar-halo u-absolute u-textColorGreenNormal svgIcon" style="width: calc(100% + 10px); height: calc(100% + 10px); top:-5px; left:-5px"><svg viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M3.44615311,11.6601601 C6.57294867,5.47967718 12.9131553,1.5 19.9642857,1.5 C27.0154162,1.5 33.3556228,5.47967718 36.4824183,11.6601601 L37.3747245,11.2087295 C34.0793076,4.69494641 27.3961457,0.5 19.9642857,0.5 C12.5324257,0.5 5.84926381,4.69494641 2.55384689,11.2087295 L3.44615311,11.6601601 Z"></path><path d="M36.4824183,28.2564276 C33.3556228,34.4369105 27.0154162,38.4165876 19.9642857,38.4165876 C12.9131553,38.4165876 6.57294867,34.4369105 3.44615311,28.2564276 L2.55384689,28.7078582 C5.84926381,35.2216412 12.5324257,39.4165876 19.9642857,39.4165876 C27.3961457,39.4165876 34.0793076,35.2216412 37.3747245,28.7078582 L36.4824183,28.2564276 Z"></path></svg></div></div></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--darker" href="https://towardsdatascience.com/@tirthajyoti?source=placement_card_footer_grid---------1-41" data-action="show-user-card" data-action-source="placement_card_footer_grid---------1-41" data-action-value="cb9d97d4b61a" data-action-type="hover" data-user-id="cb9d97d4b61a" data-collection-slug="towards-data-science" dir="auto">Tirthajyoti Sarkar</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://towardsdatascience.com/how-a-simple-mix-of-object-oriented-programming-can-sharpen-your-deep-learning-prototype-19893bd969bd?source=placement_card_footer_grid---------1-41" data-action="open-post" data-action-value="https://towardsdatascience.com/how-a-simple-mix-of-object-oriented-programming-can-sharpen-your-deep-learning-prototype-19893bd969bd?source=placement_card_footer_grid---------1-41" data-action-source="preview-listing"><time datetime="2019-07-05T23:22:18.671Z">Jul 5</time></a><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="11 min read"></span><span class="u-paddingLeft4"><span class="svgIcon svgIcon--star svgIcon--15px"><svg class="svgIcon-use" width="15" height="15"><path d="M7.438 2.324c.034-.099.09-.099.123 0l1.2 3.53a.29.29 0 0 0 .26.19h3.884c.11 0 .127.049.038.111L9.8 8.327a.271.271 0 0 0-.099.291l1.2 3.53c.034.1-.011.131-.098.069l-3.142-2.18a.303.303 0 0 0-.32 0l-3.145 2.182c-.087.06-.132.03-.099-.068l1.2-3.53a.271.271 0 0 0-.098-.292L2.056 6.146c-.087-.06-.071-.112.038-.112h3.884a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></span></div></div></div></div><div class="u-flex0 u-flexCenter"><div class="buttonSet"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="19893bd969bd" data-is-label-padded="true" data-source="placement_card_footer_grid-----19893bd969bd----1-41----------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="multivote" data-action-value="19893bd969bd" data-action-type="long-press" data-action-source="placement_card_footer_grid-----19893bd969bd----1-41----------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents u-marginLeft4" data-action="show-recommends" data-action-value="19893bd969bd">1.7K</button></span></div></div><div class="u-height20 u-borderRightLighter u-inlineBlock u-relative u-marginRight10 u-marginLeft12"></div><div class="buttonSet"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="19893bd969bd" data-action-source="placement_card_footer_grid-----19893bd969bd----1-41----------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button></div></div></div></div></div></div><div class="col u-padding8 u-xs-size12of12 u-size4of12"><div class="uiScale uiScale-ui--small uiScale-caption--regular u-height280 u-width100pct u-backgroundWhite u-borderCardBorder u-boxShadow u-borderBox u-borderRadius4 js-trackPostPresentation" data-post-id="7da416751f64" data-source="placement_card_footer_grid---------2-41" data-tracking-context="placement" data-scroll="native"><a class="link link--noUnderline u-baseColor--link" href="https://towardsdatascience.com/object-oriented-programming-for-data-scientists-build-your-ml-estimator-7da416751f64?source=placement_card_footer_grid---------2-41" data-action-source="placement_card_footer_grid---------2-41"><div class="u-backgroundCover u-backgroundColorGrayLight u-height100 u-width100pct u-borderBottomLight u-borderRadiusTop4" style="background-image: url(&quot;https://cdn-images-1.medium.com/fit/c/400/120/1*M_XOnlAT7CEkkhedW7kZEg.jpeg&quot;); background-position: 50% 50% !important;"></div></a><div class="u-padding15 u-borderBox u-flexColumn u-height180"><a class="link link--noUnderline u-baseColor--link u-flex1" href="https://towardsdatascience.com/object-oriented-programming-for-data-scientists-build-your-ml-estimator-7da416751f64?source=placement_card_footer_grid---------2-41" data-action-source="placement_card_footer_grid---------2-41"><div class="uiScale uiScale-ui--regular uiScale-caption--small u-textColorNormal u-marginBottom7"><div class="u-floatRight u-textColorNormal"><span class="svgIcon svgIcon--star svgIcon--15px"><svg class="svgIcon-use" width="15" height="15"><path d="M7.438 2.324c.034-.099.09-.099.123 0l1.2 3.53a.29.29 0 0 0 .26.19h3.884c.11 0 .127.049.038.111L9.8 8.327a.271.271 0 0 0-.099.291l1.2 3.53c.034.1-.011.131-.098.069l-3.142-2.18a.303.303 0 0 0-.32 0l-3.145 2.182c-.087.06-.132.03-.099-.068l1.2-3.53a.271.271 0 0 0-.098-.292L2.056 6.146c-.087-.06-.071-.112.038-.112h3.884a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div><div class="u-noWrapWithEllipsis u-marginRight40">More from Towards Data Science</div></div><div class="ui-h3 ui-clamp2 u-textColorDarkest u-contentSansBold u-fontSize24 u-maxHeight2LineHeightTighter u-lineClamp2 u-textOverflowEllipsis u-letterSpacingTight u-paddingBottom2">Object-oriented programming for data scientists: Build your ML estimator</div></a><div class="u-paddingBottom10 u-flex0 u-flexCenter"><div class="u-flex1 u-minWidth0 u-marginRight10"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://towardsdatascience.com/@tirthajyoti" data-action="show-user-card" data-action-value="cb9d97d4b61a" data-action-type="hover" data-user-id="cb9d97d4b61a" data-collection-slug="towards-data-science" dir="auto"><div class="u-relative u-inlineBlock u-flex0"><img src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/1_dROuRoTytntKE6LLBKKzKA.jpeg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Tirthajyoti Sarkar"><div class="avatar-halo u-absolute u-textColorGreenNormal svgIcon" style="width: calc(100% + 10px); height: calc(100% + 10px); top:-5px; left:-5px"><svg viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M3.44615311,11.6601601 C6.57294867,5.47967718 12.9131553,1.5 19.9642857,1.5 C27.0154162,1.5 33.3556228,5.47967718 36.4824183,11.6601601 L37.3747245,11.2087295 C34.0793076,4.69494641 27.3961457,0.5 19.9642857,0.5 C12.5324257,0.5 5.84926381,4.69494641 2.55384689,11.2087295 L3.44615311,11.6601601 Z"></path><path d="M36.4824183,28.2564276 C33.3556228,34.4369105 27.0154162,38.4165876 19.9642857,38.4165876 C12.9131553,38.4165876 6.57294867,34.4369105 3.44615311,28.2564276 L2.55384689,28.7078582 C5.84926381,35.2216412 12.5324257,39.4165876 19.9642857,39.4165876 C27.3961457,39.4165876 34.0793076,35.2216412 37.3747245,28.7078582 L36.4824183,28.2564276 Z"></path></svg></div></div></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--darker" href="https://towardsdatascience.com/@tirthajyoti?source=placement_card_footer_grid---------2-41" data-action="show-user-card" data-action-source="placement_card_footer_grid---------2-41" data-action-value="cb9d97d4b61a" data-action-type="hover" data-user-id="cb9d97d4b61a" data-collection-slug="towards-data-science" dir="auto">Tirthajyoti Sarkar</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://towardsdatascience.com/object-oriented-programming-for-data-scientists-build-your-ml-estimator-7da416751f64?source=placement_card_footer_grid---------2-41" data-action="open-post" data-action-value="https://towardsdatascience.com/object-oriented-programming-for-data-scientists-build-your-ml-estimator-7da416751f64?source=placement_card_footer_grid---------2-41" data-action-source="preview-listing"><time datetime="2019-07-10T02:20:58.915Z">Jul 9</time></a><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="13 min read"></span><span class="u-paddingLeft4"><span class="svgIcon svgIcon--star svgIcon--15px"><svg class="svgIcon-use" width="15" height="15"><path d="M7.438 2.324c.034-.099.09-.099.123 0l1.2 3.53a.29.29 0 0 0 .26.19h3.884c.11 0 .127.049.038.111L9.8 8.327a.271.271 0 0 0-.099.291l1.2 3.53c.034.1-.011.131-.098.069l-3.142-2.18a.303.303 0 0 0-.32 0l-3.145 2.182c-.087.06-.132.03-.099-.068l1.2-3.53a.271.271 0 0 0-.098-.292L2.056 6.146c-.087-.06-.071-.112.038-.112h3.884a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></span></div></div></div></div><div class="u-flex0 u-flexCenter"><div class="buttonSet"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="7da416751f64" data-is-label-padded="true" data-source="placement_card_footer_grid-----7da416751f64----2-41----------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="multivote" data-action-value="7da416751f64" data-action-type="long-press" data-action-source="placement_card_footer_grid-----7da416751f64----2-41----------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents u-marginLeft4" data-action="show-recommends" data-action-value="7da416751f64">381</button></span></div></div><div class="u-height20 u-borderRightLighter u-inlineBlock u-relative u-marginRight10 u-marginLeft12"></div><div class="buttonSet"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="7da416751f64" data-action-source="placement_card_footer_grid-----7da416751f64----2-41----------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button></div></div></div></div></div></div></div></div></div></div><div class="u-padding0 u-clearfix u-backgroundGrayLightest u-print-hide supplementalPostContent js-responsesWrapper" data-action-scope="_actionscope_5"><div class="container u-maxWidth740"><div class="responsesStreamWrapper u-maxWidth640 js-responsesStreamWrapper"><div class="container responsesStream-title u-paddingTop15"><div class="row"><header class="heading"><div class="u-clearfix"><div class="heading-content u-floatLeft"><span class="heading-title heading-title--semibold">Responses</span></div></div></header></div></div><div class="responsesStream-editor cardChromeless u-marginBottom20 u-paddingLeft20 u-paddingRight20 js-responsesStreamEditor"><div class="inlineNewPostControl js-inlineNewPostControl" data-action-scope="_actionscope_6"><div class="inlineEditor is-collapsed is-postEditMode js-inlineEditor" data-action="focus-editor"><div class="u-paddingTop20 js-block js-inlineEditorContent"><div class="inlineEditor-header"><div class="inlineEditor-avatar u-paddingRight20"><div class="avatar u-inline"><img src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/0__vE9uUIWyOu7cYeP_(1)" class="avatar-image u-size36x36 u-xs-size32x32" alt="HamidReza Mirkhani"></div></div><div class="inlineEditor-headerContent"><div class="inlineEditor-placeholder js-inlineEditorPrompt">Write a response…</div><div class="inlineEditor-author u-accentColor--textNormal">HamidReza Mirkhani</div></div></div></div></div></div></div><div class="responsesStream js-responsesStream"></div><div class="container js-showOtherResponses"><div class="row"><button class="button button--primary button--withChrome u-accentColor--buttonNormal responsesStream-showOtherResponses cardChromeless u-width100pct u-marginVertical20 u-heightAuto" data-action="show-other-responses">Show all responses</button></div></div><div class="responsesStream js-responsesStreamOther"></div></div></div></div><div class="supplementalPostContent js-heroPromo"></div></footer></article></main><aside class="u-marginAuto u-maxWidth1032 js-postLeftSidebar"><div class="u-foreground u-top0 u-fixed u-sm-hide js-postShareWidget u-transition--fadeIn300" data-scroll="fixed" style="transform: translateY(150px);"><div class="u-breakWord u-md-hide u-width131"><div class="u-width131 collection-title u-fontWeightBold u-fontSize18 u-lineHeightTight"><a href="https://towardsdatascience.com/?source=logo-5f069cdb2c1b">Towards Data Science</a></div><div class="u-width131 u-multiline-clamp u-textColorNormal u-fontSize14 u-lineHeightTight u-paddingTop3">Sharing concepts, ideas, and codes.</div><div class="u-paddingTop15 u-paddingBottom30 u-borderBottomLight u-marginBottom30"><button class="button button--primary button--small u-noUserSelect button--withChrome u-accentColor--buttonNormal js-relationshipButton" data-action="toggle-follow-collection" data-action-source="post_sidebar----7f60cf5620c9----------------------post_sidebar" data-collection-id="7f60cf5620c9"><span class="button-label  js-buttonLabel">Follow</span></button></div></div><ul><li class="u-marginVertical10"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="e66f2e30fb96" data-is-icon-29px="true" data-has-recommend-list="true" data-source="post_share_widget-----e66f2e30fb96---------------------clap_sidebar"><div class="u-relative u-foreground"><button class="button button--primary button--large button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="multivote" data-action-value="e66f2e30fb96" data-action-type="long-press" data-action-source="post_share_widget-----e66f2e30fb96---------------------clap_sidebar" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><g fill-rule="evenodd"><path d="M13.739 1l.761 2.966L15.261 1z"></path><path d="M16.815 4.776l1.84-2.551-1.43-.471z"></path><path d="M10.378 2.224l1.84 2.551-.408-3.022z"></path><path d="M22.382 22.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L6.11 15.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L8.43 9.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L20.628 15c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM12.99 6.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><g fill-rule="evenodd"><path d="M13.738 1l.762 2.966L15.262 1z"></path><path d="M18.634 2.224l-1.432-.47-.408 3.022z"></path><path d="M11.79 1.754l-1.431.47 1.84 2.552z"></path><path d="M24.472 14.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M14.58 10.887c-.156-.83.096-1.569.692-2.142L12.78 6.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M17.812 10.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L9.2 7.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L7.046 9.54 5.802 8.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394l1.241 1.241 4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L4.89 11.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C21.74 20.8 22.271 18 20.62 14.982l-2.809-4.942z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton" data-action="show-recommends" data-action-value="e66f2e30fb96">191</button></span></div></li><li class="u-marginVertical10 u-marginLeft3"><button class="button button--large button--dark button--chromeless is-touchIconFadeInPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="e66f2e30fb96" data-action-source="post_share_widget-----e66f2e30fb96---------------------bookmark_sidebar"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4zM21 23l-5.91-3.955-.148-.107a.751.751 0 0 0-.884 0l-.147.107L8 23V6.615C8 5.725 8.725 5 9.615 5h9.77C20.275 5 21 5.725 21 6.615V23z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4z" fill-rule="evenodd"></path></svg></span></span></button></li><li class="u-marginVertical10 u-marginLeft3"><a class="button button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon button--dark button--chromeless" href="https://medium.com/p/e66f2e30fb96/share/twitter" title="Share on Twitter" aria-label="Share on Twitter" target="_blank" data-action-source="post_share_widget"><span class="button-defaultState"><span class="svgIcon svgIcon--twitterFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M22.053 7.54a4.474 4.474 0 0 0-3.31-1.455 4.526 4.526 0 0 0-4.526 4.524c0 .35.04.7.082 1.05a12.9 12.9 0 0 1-9.3-4.77c-.39.69-.61 1.46-.65 2.26.03 1.6.83 2.99 2.02 3.79-.72-.02-1.41-.22-2.02-.57-.01.02-.01.04 0 .08-.01 2.17 1.55 4 3.63 4.44-.39.08-.79.13-1.21.16-.28-.03-.57-.05-.81-.08.54 1.77 2.21 3.08 4.2 3.15a9.564 9.564 0 0 1-5.66 1.94c-.34-.03-.7-.06-1.05-.08 2 1.27 4.38 2.02 6.94 2.02 8.31 0 12.86-6.9 12.84-12.85.02-.24.01-.43 0-.65.89-.62 1.65-1.42 2.26-2.34-.82.38-1.69.62-2.59.72a4.37 4.37 0 0 0 1.94-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></span></span></a></li><li class="u-marginVertical10 u-marginLeft3"><a class="button button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon button--dark button--chromeless" href="https://medium.com/p/e66f2e30fb96/share/facebook" title="Share on Facebook" aria-label="Share on Facebook" target="_blank" data-action-source="post_share_widget"><span class="button-defaultState"><span class="svgIcon svgIcon--facebookSquare svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M23.209 5H5.792A.792.792 0 0 0 5 5.791V23.21c0 .437.354.791.792.791h9.303v-7.125H12.72v-2.968h2.375v-2.375c0-2.455 1.553-3.662 3.741-3.662 1.049 0 1.95.078 2.213.112v2.565h-1.517c-1.192 0-1.469.567-1.469 1.397v1.963h2.969l-.594 2.968h-2.375L18.11 24h5.099a.791.791 0 0 0 .791-.791V5.79a.791.791 0 0 0-.791-.79"></path></svg></span></span></a></li></ul></div></aside><style class="js-collectionStyle">
.u-accentColor--borderLight {border-color: #668AAA !important;}
.u-accentColor--borderNormal {border-color: #668AAA !important;}
.u-accentColor--borderDark {border-color: #5A7690 !important;}
.u-accentColor--iconLight .svgIcon,.u-accentColor--iconLight.svgIcon {fill: #668AAA !important;}
.u-accentColor--iconNormal .svgIcon,.u-accentColor--iconNormal.svgIcon {fill: #668AAA !important;}
.u-accentColor--iconDark .svgIcon,.u-accentColor--iconDark.svgIcon {fill: #5A7690 !important;}
.u-accentColor--textNormal {color: #5A7690 !important;}
.u-accentColor--hoverTextNormal:hover {color: #5A7690 !important;}
.u-accentColor--textNormal.u-accentColor--textDarken:hover {color: #546C83 !important;}
.u-accentColor--textDark {color: #546C83 !important;}
.u-accentColor--backgroundLight {background-color: #668AAA !important;}
.u-accentColor--backgroundNormal {background-color: #668AAA !important;}
.u-accentColor--backgroundDark {background-color: #5A7690 !important;}
.u-accentColor--buttonDark {border-color: #5A7690 !important; color: #546C83 !important;}
.u-accentColor--buttonDark:hover {border-color: #546C83 !important;}
.u-accentColor--buttonDark .icon:before,.u-accentColor--buttonDark .svgIcon{color: #5A7690 !important; fill: #5A7690 !important;}
.u-accentColor--buttonNormal:not(.clapButton--largePill) {border-color: #668AAA !important; color: #5A7690 !important;}
.u-accentColor--buttonNormal:hover {border-color: #5A7690 !important;}
.u-accentColor--buttonNormal .icon:before,.u-accentColor--buttonNormal .svgIcon{color: #668AAA !important; fill: #668AAA !important;}
.u-accentColor--buttonNormal.button--filled .icon:before,.u-accentColor--buttonNormal.button--filled .svgIcon{color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-accentColor--buttonDark.button--filled,.u-accentColor--buttonDark.button--withChrome.is-active,.u-accentColor--fillWhenActive.is-active {background-color: #5A7690 !important; border-color: #5A7690 !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-accentColor--buttonNormal.button--filled:not(.clapButton--largePill),.u-accentColor--buttonNormal.button--withChrome.is-active:not(.clapButton--largePill) {background-color: #668AAA !important; border-color: #668AAA !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.postArticle.is-withAccentColors .markup--user,.postArticle.is-withAccentColors .markup--query {color: #5A7690 !important;}.u-tintBgColor {background-color: rgba(53, 88, 118, 1) !important;}.u-tintBgColor .u-fadeLeft:before {background-image: linear-gradient(to right, rgba(53, 88, 118, 1) 0%, rgba(53, 88, 118, 0) 100%) !important;}.u-tintBgColor .u-fadeRight:after {background-image: linear-gradient(to right, rgba(53, 88, 118, 0) 0%, rgba(53, 88, 118, 1) 100%) !important;}
.u-tintSpectrum .u-baseColor--borderLight {border-color: #9FB3C6 !important;}
.u-tintSpectrum .u-baseColor--borderNormal {border-color: #C5D2E1 !important;}
.u-tintSpectrum .u-baseColor--borderDark {border-color: #E9F1FA !important;}
.u-tintSpectrum .u-baseColor--iconLight .svgIcon,.u-tintSpectrum .u-baseColor--iconLight.svgIcon {fill: #9FB3C6 !important;}
.u-tintSpectrum .u-baseColor--iconNormal .svgIcon,.u-tintSpectrum .u-baseColor--iconNormal.svgIcon {fill: #C5D2E1 !important;}
.u-tintSpectrum .u-baseColor--iconDark .svgIcon,.u-tintSpectrum .u-baseColor--iconDark.svgIcon {fill: #E9F1FA !important;}
.u-tintSpectrum .u-baseColor--textNormal {color: #C5D2E1 !important;}
.u-tintSpectrum .u-baseColor--textNormal.u-baseColor--textDarken:hover {color: #FBFFFF !important;}
.u-tintSpectrum .u-baseColor--textDark {color: #FBFFFF !important;}
.u-tintSpectrum .u-baseColor--textDarker {color: #FBFFFF !important;}
.u-tintSpectrum .u-baseColor--backgroundLight {background-color: #9FB3C6 !important;}
.u-tintSpectrum .u-baseColor--backgroundNormal {background-color: #C5D2E1 !important;}
.u-tintSpectrum .u-baseColor--backgroundDark {background-color: #E9F1FA !important;}
.u-tintSpectrum .u-baseColor--buttonLight {border-color: #9FB3C6 !important; color: #9FB3C6 !important;}
.u-tintSpectrum .u-baseColor--buttonLight:hover {border-color: #9FB3C6 !important;}
.u-tintSpectrum .u-baseColor--buttonLight .icon:before,.u-tintSpectrum .u-baseColor--buttonLight .svgIcon {color: #9FB3C6 !important; fill: #9FB3C6 !important;}
.u-tintSpectrum .u-baseColor--buttonDark {border-color: #E9F1FA !important; color: #FBFFFF !important;}
.u-tintSpectrum .u-baseColor--buttonDark:hover {border-color: #FBFFFF !important;}
.u-tintSpectrum .u-baseColor--buttonDark .icon:before,.u-tintSpectrum .u-baseColor--buttonDark .svgIcon {color: #E9F1FA !important; fill: #E9F1FA !important;}
.u-tintSpectrum .u-baseColor--buttonNormal {border-color: #C5D2E1 !important; color: #C5D2E1 !important;}
.u-tintSpectrum .u-baseColor--buttonNormal:hover {border-color: #E9F1FA !important;}
.u-tintSpectrum .u-baseColor--buttonNormal .icon:before,.u-tintSpectrum .u-baseColor--buttonNormal .svgIcon {color: #C5D2E1 !important; fill: #C5D2E1 !important;}
.u-tintSpectrum .u-baseColor--buttonDark.button--filled,.u-tintSpectrum .u-baseColor--buttonDark.button--withChrome.is-active {background-color: #E9F1FA !important; border-color: #E9F1FA !important; color: rgba(53, 88, 118, 1) !important; fill: rgba(53, 88, 118, 1) !important;}
.u-tintSpectrum .u-baseColor--buttonNormal.button--filled,.u-tintSpectrum .u-baseColor--buttonNormal.button--withChrome.is-active {background-color: #C5D2E1 !important; border-color: #C5D2E1 !important; color: rgba(53, 88, 118, 1) !important; fill: rgba(53, 88, 118, 1) !important;}
.u-tintSpectrum .u-baseColor--link {color: #C5D2E1 !important;}
.u-tintSpectrum .u-baseColor--link.link--darkenOnHover:hover {color: #FBFFFF !important;}
.u-tintSpectrum .u-baseColor--link.link--darken:hover,.u-tintSpectrum .u-baseColor--link.link--darken:focus,.u-tintSpectrum .u-baseColor--link.link--darken:active {color: #FBFFFF !important;}
.u-tintSpectrum .u-baseColor--link.link--dark {color: #FBFFFF !important;}
.u-tintSpectrum .u-baseColor--link.link--dark.link--darken:hover,.u-tintSpectrum .u-baseColor--link.link--dark.link--darken:focus,.u-tintSpectrum .u-baseColor--link.link--dark.link--darken:active {color: #FBFFFF !important;}
.u-tintSpectrum .u-baseColor--link.link--darker {color: #FBFFFF !important;}
.u-tintSpectrum .u-baseColor--placeholderNormal ::-webkit-input-placeholder {color: #9FB3C6;}
.u-tintSpectrum .u-baseColor--placeholderNormal ::-moz-placeholder {color: #9FB3C6;}
.u-tintSpectrum .u-baseColor--placeholderNormal :-ms-input-placeholder {color: #9FB3C6;}
.u-tintSpectrum .svgIcon--logoWordmark {stroke: none !important; fill: #FBFFFF !important;}
.u-tintSpectrum .svgIcon--logoMonogram {stroke: none !important; fill: #FBFFFF !important;}
.u-tintSpectrum  .ui-h1,.u-tintSpectrum  .ui-h2,.u-tintSpectrum  .ui-h3,.u-tintSpectrum  .ui-h4,.u-tintSpectrum  .ui-brand1,.u-tintSpectrum  .ui-brand2,.u-tintSpectrum  .ui-captionStrong {color: #FBFFFF !important; fill: #FBFFFF !important;}
.u-tintSpectrum  .ui-body,.u-tintSpectrum  .ui-caps {color: #FBFFFF !important; fill: #FBFFFF !important;}
.u-tintSpectrum  .ui-summary,.u-tintSpectrum  .ui-caption {color: #9FB3C6 !important; fill: #9FB3C6 !important;}
.u-tintSpectrum .u-accentColor--borderLight {border-color: #9FB3C6 !important;}
.u-tintSpectrum .u-accentColor--borderNormal {border-color: #C5D2E1 !important;}
.u-tintSpectrum .u-accentColor--borderDark {border-color: #E9F1FA !important;}
.u-tintSpectrum .u-accentColor--iconLight .svgIcon,.u-tintSpectrum .u-accentColor--iconLight.svgIcon {fill: #9FB3C6 !important;}
.u-tintSpectrum .u-accentColor--iconNormal .svgIcon,.u-tintSpectrum .u-accentColor--iconNormal.svgIcon {fill: #C5D2E1 !important;}
.u-tintSpectrum .u-accentColor--iconDark .svgIcon,.u-tintSpectrum .u-accentColor--iconDark.svgIcon {fill: #E9F1FA !important;}
.u-tintSpectrum .u-accentColor--textNormal {color: #C5D2E1 !important;}
.u-tintSpectrum .u-accentColor--hoverTextNormal:hover {color: #C5D2E1 !important;}
.u-tintSpectrum .u-accentColor--textNormal.u-accentColor--textDarken:hover {color: #FBFFFF !important;}
.u-tintSpectrum .u-accentColor--textDark {color: #FBFFFF !important;}
.u-tintSpectrum .u-accentColor--backgroundLight {background-color: #9FB3C6 !important;}
.u-tintSpectrum .u-accentColor--backgroundNormal {background-color: #C5D2E1 !important;}
.u-tintSpectrum .u-accentColor--backgroundDark {background-color: #E9F1FA !important;}
.u-tintSpectrum .u-accentColor--buttonDark {border-color: #E9F1FA !important; color: #FBFFFF !important;}
.u-tintSpectrum .u-accentColor--buttonDark:hover {border-color: #FBFFFF !important;}
.u-tintSpectrum .u-accentColor--buttonDark .icon:before,.u-tintSpectrum .u-accentColor--buttonDark .svgIcon{color: #E9F1FA !important; fill: #E9F1FA !important;}
.u-tintSpectrum .u-accentColor--buttonNormal:not(.clapButton--largePill) {border-color: #C5D2E1 !important; color: #C5D2E1 !important;}
.u-tintSpectrum .u-accentColor--buttonNormal:hover {border-color: #E9F1FA !important;}
.u-tintSpectrum .u-accentColor--buttonNormal .icon:before,.u-tintSpectrum .u-accentColor--buttonNormal .svgIcon{color: #C5D2E1 !important; fill: #C5D2E1 !important;}
.u-tintSpectrum .u-accentColor--buttonNormal.button--filled .icon:before,.u-tintSpectrum .u-accentColor--buttonNormal.button--filled .svgIcon{color: rgba(53, 88, 118, 1) !important; fill: rgba(53, 88, 118, 1) !important;}
.u-tintSpectrum .u-accentColor--buttonDark.button--filled,.u-tintSpectrum .u-accentColor--buttonDark.button--withChrome.is-active,.u-tintSpectrum .u-accentColor--fillWhenActive.is-active {background-color: #E9F1FA !important; border-color: #E9F1FA !important; color: rgba(53, 88, 118, 1) !important; fill: rgba(53, 88, 118, 1) !important;}
.u-tintSpectrum .u-accentColor--buttonNormal.button--filled:not(.clapButton--largePill),.u-tintSpectrum .u-accentColor--buttonNormal.button--withChrome.is-active:not(.clapButton--largePill) {background-color: #C5D2E1 !important; border-color: #C5D2E1 !important; color: rgba(53, 88, 118, 1) !important; fill: rgba(53, 88, 118, 1) !important;}
.u-tintSpectrum .postArticle.is-withAccentColors .markup--user,.u-tintSpectrum .postArticle.is-withAccentColors .markup--query {color: #C5D2E1 !important;}
.u-accentColor--highlightFaint {background-color: rgba(233, 242, 253, 1) !important;}
.u-accentColor--highlightStrong.is-active .svgIcon {fill: rgba(200, 228, 255, 1) !important;}
.postArticle.is-withAccentColors .markup--quote.is-other {background-color: rgba(233, 242, 253, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-other {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(233, 242, 253, 1), rgba(233, 242, 253, 1));}
.postArticle.is-withAccentColors .markup--quote.is-me {background-color: rgba(215, 235, 254, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-me {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(215, 235, 254, 1), rgba(215, 235, 254, 1));}
.postArticle.is-withAccentColors .markup--quote.is-targeted {background-color: rgba(200, 228, 255, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-targeted {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(200, 228, 255, 1), rgba(200, 228, 255, 1));}
.postArticle.is-withAccentColors .markup--quote.is-selected {background-color: rgba(200, 228, 255, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-selected {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(200, 228, 255, 1), rgba(200, 228, 255, 1));}
.postArticle.is-withAccentColors .markup--highlight {background-color: rgba(200, 228, 255, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--highlight {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(200, 228, 255, 1), rgba(200, 228, 255, 1));}.u-baseColor--iconNormal.avatar-halo {fill: rgba(0, 0, 0, 0.4980392156862745) !important;}</style><style class="js-collectionStyleConstant">.u-imageBgColor {background-color: rgba(0, 0, 0, 0.24705882352941178);}
.u-imageSpectrum .u-baseColor--borderLight {border-color: rgba(255, 255, 255, 0.6980392156862745) !important;}
.u-imageSpectrum .u-baseColor--borderNormal {border-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-baseColor--borderDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--iconLight .svgIcon,.u-imageSpectrum .u-baseColor--iconLight.svgIcon {fill: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-baseColor--iconNormal .svgIcon,.u-imageSpectrum .u-baseColor--iconNormal.svgIcon {fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--iconDark .svgIcon,.u-imageSpectrum .u-baseColor--iconDark.svgIcon {fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--textNormal {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--textNormal.u-baseColor--textDarken:hover {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--textDark {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--textDarker {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--backgroundLight {background-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-baseColor--backgroundNormal {background-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--backgroundDark {background-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonLight {border-color: rgba(255, 255, 255, 0.6980392156862745) !important; color: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-baseColor--buttonLight:hover {border-color: rgba(255, 255, 255, 0.6980392156862745) !important;}
.u-imageSpectrum .u-baseColor--buttonLight .icon:before,.u-imageSpectrum .u-baseColor--buttonLight .svgIcon {color: rgba(255, 255, 255, 0.8) !important; fill: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-baseColor--buttonDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonDark:hover {border-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonDark .icon:before,.u-imageSpectrum .u-baseColor--buttonDark .svgIcon {color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal {border-color: rgba(255, 255, 255, 0.8980392156862745) !important; color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal:hover {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal .icon:before,.u-imageSpectrum .u-baseColor--buttonNormal .svgIcon {color: rgba(255, 255, 255, 0.9490196078431372) !important; fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--buttonDark.button--filled,.u-imageSpectrum .u-baseColor--buttonDark.button--withChrome.is-active {background-color: rgba(255, 255, 255, 1) !important; border-color: rgba(255, 255, 255, 1) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal.button--filled,.u-imageSpectrum .u-baseColor--buttonNormal.button--withChrome.is-active {background-color: rgba(255, 255, 255, 0.9490196078431372) !important; border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-baseColor--link {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--link.link--darkenOnHover:hover {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--darken:hover,.u-imageSpectrum .u-baseColor--link.link--darken:focus,.u-imageSpectrum .u-baseColor--link.link--darken:active {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--dark {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--dark.link--darken:hover,.u-imageSpectrum .u-baseColor--link.link--dark.link--darken:focus,.u-imageSpectrum .u-baseColor--link.link--dark.link--darken:active {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--darker {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--placeholderNormal ::-webkit-input-placeholder {color: rgba(255, 255, 255, 0.8);}
.u-imageSpectrum .u-baseColor--placeholderNormal ::-moz-placeholder {color: rgba(255, 255, 255, 0.8);}
.u-imageSpectrum .u-baseColor--placeholderNormal :-ms-input-placeholder {color: rgba(255, 255, 255, 0.8);}
.u-imageSpectrum .svgIcon--logoWordmark {stroke: none !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .svgIcon--logoMonogram {stroke: none !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum  .ui-h1,.u-imageSpectrum  .ui-h2,.u-imageSpectrum  .ui-h3,.u-imageSpectrum  .ui-h4,.u-imageSpectrum  .ui-brand1,.u-imageSpectrum  .ui-brand2,.u-imageSpectrum  .ui-captionStrong {color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum  .ui-body,.u-imageSpectrum  .ui-caps {color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum  .ui-summary,.u-imageSpectrum  .ui-caption {color: rgba(255, 255, 255, 0.8) !important; fill: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-accentColor--borderLight {border-color: rgba(255, 255, 255, 0.6980392156862745) !important;}
.u-imageSpectrum .u-accentColor--borderNormal {border-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-accentColor--borderDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--iconLight .svgIcon,.u-imageSpectrum .u-accentColor--iconLight.svgIcon {fill: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-accentColor--iconNormal .svgIcon,.u-imageSpectrum .u-accentColor--iconNormal.svgIcon {fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--iconDark .svgIcon,.u-imageSpectrum .u-accentColor--iconDark.svgIcon {fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--textNormal {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--hoverTextNormal:hover {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--textNormal.u-accentColor--textDarken:hover {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--textDark {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--backgroundLight {background-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-accentColor--backgroundNormal {background-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--backgroundDark {background-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonDark:hover {border-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonDark .icon:before,.u-imageSpectrum .u-accentColor--buttonDark .svgIcon{color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal:not(.clapButton--largePill) {border-color: rgba(255, 255, 255, 0.8980392156862745) !important; color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal:hover {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal .icon:before,.u-imageSpectrum .u-accentColor--buttonNormal .svgIcon{color: rgba(255, 255, 255, 0.9490196078431372) !important; fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal.button--filled .icon:before,.u-imageSpectrum .u-accentColor--buttonNormal.button--filled .svgIcon{color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-accentColor--buttonDark.button--filled,.u-imageSpectrum .u-accentColor--buttonDark.button--withChrome.is-active,.u-imageSpectrum .u-accentColor--fillWhenActive.is-active {background-color: rgba(255, 255, 255, 1) !important; border-color: rgba(255, 255, 255, 1) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal.button--filled:not(.clapButton--largePill),.u-imageSpectrum .u-accentColor--buttonNormal.button--withChrome.is-active:not(.clapButton--largePill) {background-color: rgba(255, 255, 255, 0.9490196078431372) !important; border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .postArticle.is-withAccentColors .markup--user,.u-imageSpectrum .postArticle.is-withAccentColors .markup--query {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--highlightFaint {background-color: rgba(255, 255, 255, 0.2) !important;}
.u-imageSpectrum .u-accentColor--highlightStrong.is-active .svgIcon {fill: rgba(255, 255, 255, 0.6) !important;}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-other {background-color: rgba(255, 255, 255, 0.2) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-other {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 0.2));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-me {background-color: rgba(255, 255, 255, 0.4) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-me {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.4), rgba(255, 255, 255, 0.4));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-targeted {background-color: rgba(255, 255, 255, 0.6) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-targeted {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.6), rgba(255, 255, 255, 0.6));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-selected {background-color: rgba(255, 255, 255, 0.6) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-selected {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.6), rgba(255, 255, 255, 0.6));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--highlight {background-color: rgba(255, 255, 255, 0.6) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--highlight {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.6), rgba(255, 255, 255, 0.6));}.u-resetSpectrum .u-tintBgColor {background-color: rgba(255, 255, 255, 1) !important;}.u-resetSpectrum .u-tintBgColor .u-fadeLeft:before {background-image: linear-gradient(to right, rgba(255, 255, 255, 1) 0%, rgba(255, 255, 255, 0) 100%) !important;}.u-resetSpectrum .u-tintBgColor .u-fadeRight:after {background-image: linear-gradient(to right, rgba(255, 255, 255, 0) 0%, rgba(255, 255, 255, 1) 100%) !important;}
.u-resetSpectrum .u-baseColor--borderLight {border-color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--borderNormal {border-color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--borderDark {border-color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--iconLight .svgIcon,.u-resetSpectrum .u-baseColor--iconLight.svgIcon {fill: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--iconNormal .svgIcon,.u-resetSpectrum .u-baseColor--iconNormal.svgIcon {fill: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--iconDark .svgIcon,.u-resetSpectrum .u-baseColor--iconDark.svgIcon {fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--textNormal {color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--textNormal.u-baseColor--textDarken:hover {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--textDark {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--textDarker {color: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum .u-baseColor--backgroundLight {background-color: rgba(0, 0, 0, 0.09803921568627451) !important;}
.u-resetSpectrum .u-baseColor--backgroundNormal {background-color: rgba(0, 0, 0, 0.2) !important;}
.u-resetSpectrum .u-baseColor--backgroundDark {background-color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonLight {border-color: rgba(0, 0, 0, 0.2980392156862745) !important; color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonLight:hover {border-color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonLight .icon:before,.u-resetSpectrum .u-baseColor--buttonLight .svgIcon {color: rgba(0, 0, 0, 0.2980392156862745) !important; fill: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonDark {border-color: rgba(0, 0, 0, 0.6) !important; color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--buttonDark:hover {border-color: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum .u-baseColor--buttonDark .icon:before,.u-resetSpectrum .u-baseColor--buttonDark .svgIcon {color: rgba(0, 0, 0, 0.6) !important; fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal {border-color: rgba(0, 0, 0, 0.4980392156862745) !important; color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal:hover {border-color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal .icon:before,.u-resetSpectrum .u-baseColor--buttonNormal .svgIcon {color: rgba(0, 0, 0, 0.4980392156862745) !important; fill: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonDark.button--filled,.u-resetSpectrum .u-baseColor--buttonDark.button--withChrome.is-active {background-color: rgba(0, 0, 0, 0.2980392156862745) !important; border-color: rgba(0, 0, 0, 0.2980392156862745) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal.button--filled,.u-resetSpectrum .u-baseColor--buttonNormal.button--withChrome.is-active {background-color: rgba(0, 0, 0, 0.2) !important; border-color: rgba(0, 0, 0, 0.2) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-baseColor--link {color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--link.link--darkenOnHover:hover {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--link.link--darken:hover,.u-resetSpectrum .u-baseColor--link.link--darken:focus,.u-resetSpectrum .u-baseColor--link.link--darken:active {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--link.link--dark {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--link.link--dark.link--darken:hover,.u-resetSpectrum .u-baseColor--link.link--dark.link--darken:focus,.u-resetSpectrum .u-baseColor--link.link--dark.link--darken:active {color: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum .u-baseColor--link.link--darker {color: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum .u-baseColor--placeholderNormal ::-webkit-input-placeholder {color: rgba(0, 0, 0, 0.2980392156862745);}
.u-resetSpectrum .u-baseColor--placeholderNormal ::-moz-placeholder {color: rgba(0, 0, 0, 0.2980392156862745);}
.u-resetSpectrum .u-baseColor--placeholderNormal :-ms-input-placeholder {color: rgba(0, 0, 0, 0.2980392156862745);}
.u-resetSpectrum .svgIcon--logoWordmark {stroke: none !important; fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .svgIcon--logoMonogram {stroke: none !important; fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum  .ui-h1,.u-resetSpectrum  .ui-h2,.u-resetSpectrum  .ui-h3,.u-resetSpectrum  .ui-h4,.u-resetSpectrum  .ui-brand1,.u-resetSpectrum  .ui-brand2,.u-resetSpectrum  .ui-captionStrong {color: rgba(0, 0, 0, 0.8) !important; fill: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum  .ui-body,.u-resetSpectrum  .ui-caps {color: rgba(0, 0, 0, 0.6) !important; fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum  .ui-summary,.u-resetSpectrum  .ui-caption {color: rgba(0, 0, 0, 0.2980392156862745) !important; fill: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-accentColor--borderLight {border-color: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--borderNormal {border-color: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--borderDark {border-color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--iconLight .svgIcon,.u-resetSpectrum .u-accentColor--iconLight.svgIcon {fill: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--iconNormal .svgIcon,.u-resetSpectrum .u-accentColor--iconNormal.svgIcon {fill: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--iconDark .svgIcon,.u-resetSpectrum .u-accentColor--iconDark.svgIcon {fill: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--textNormal {color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--hoverTextNormal:hover {color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--textNormal.u-accentColor--textDarken:hover {color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--textDark {color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--backgroundLight {background-color: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--backgroundNormal {background-color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--backgroundDark {background-color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark {border-color: rgba(0, 171, 107, 1) !important; color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark:hover {border-color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark .icon:before,.u-resetSpectrum .u-accentColor--buttonDark .svgIcon{color: rgba(28, 153, 99, 1) !important; fill: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal:not(.clapButton--largePill) {border-color: rgba(2, 184, 117, 1) !important; color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal:hover {border-color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal .icon:before,.u-resetSpectrum .u-accentColor--buttonNormal .svgIcon{color: rgba(0, 171, 107, 1) !important; fill: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal.button--filled .icon:before,.u-resetSpectrum .u-accentColor--buttonNormal.button--filled .svgIcon{color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark.button--filled,.u-resetSpectrum .u-accentColor--buttonDark.button--withChrome.is-active,.u-resetSpectrum .u-accentColor--fillWhenActive.is-active {background-color: rgba(28, 153, 99, 1) !important; border-color: rgba(28, 153, 99, 1) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal.button--filled:not(.clapButton--largePill),.u-resetSpectrum .u-accentColor--buttonNormal.button--withChrome.is-active:not(.clapButton--largePill) {background-color: rgba(0, 171, 107, 1) !important; border-color: rgba(0, 171, 107, 1) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .postArticle.is-withAccentColors .markup--user,.u-resetSpectrum .postArticle.is-withAccentColors .markup--query {color: rgba(0, 171, 107, 1) !important;}</style><div class="highlightMenu" data-action-scope="_actionscope_3" style="left: 595px; top: 104px;"><div class="highlightMenu-inner"><div class="buttonSet"><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--highlightMenu u-accentColor--highlightStrong js-highlightMenuQuoteButton" data-action="quote" data-action-source="quote_menu--------------------------highlight_text" data-skip-onboarding="true"><span class="svgIcon svgIcon--highlighter svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M13.7 15.964l5.204-9.387-4.726-2.62-5.204 9.387 4.726 2.62zm-.493.885l-1.313 2.37-1.252.54-.702 1.263-3.796-.865 1.228-2.213-.202-1.35 1.314-2.37 4.722 2.616z" fill-rule="evenodd"></path></svg></span></button><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--highlightMenu" data-action="quote-respond" data-action-source="quote_menu--------------------------respond_text" data-skip-onboarding="true"><span class="svgIcon svgIcon--responseFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19.074 21.117c-1.244 0-2.432-.37-3.532-1.096a7.792 7.792 0 0 1-.703-.52c-.77.21-1.57.32-2.38.32-4.67 0-8.46-3.5-8.46-7.8C4 7.7 7.79 4.2 12.46 4.2c4.662 0 8.457 3.5 8.457 7.803 0 2.058-.85 3.984-2.403 5.448.023.17.06.35.118.55.192.69.537 1.38 1.026 2.04.15.21.172.48.058.7a.686.686 0 0 1-.613.38h-.03z" fill-rule="evenodd"></path></svg></span></button><a class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--chromeless button--highlightMenu js-highlightMenuTwitterShare" href="https://medium.com/p/e66f2e30fb96/share/twitter?type=highlight&amp;text=Review%3A%20SegNet%20(Semantic%20Segmentation)" title="Share on Twitter" aria-label="Share on Twitter" target="_blank" data-action="twitter"><span class="button-defaultState"><span class="svgIcon svgIcon--twitterFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M21.725 5.338c-.744.47-1.605.804-2.513 1.006a3.978 3.978 0 0 0-2.942-1.293c-2.22 0-4.02 1.81-4.02 4.02 0 .32.034.63.07.94-3.31-.18-6.27-1.78-8.255-4.23a4.544 4.544 0 0 0-.574 2.01c.04 1.43.74 2.66 1.8 3.38-.63-.01-1.25-.19-1.79-.5v.08c0 1.93 1.38 3.56 3.23 3.95-.34.07-.7.12-1.07.14-.25-.02-.5-.04-.72-.07.49 1.58 1.97 2.74 3.74 2.8a8.49 8.49 0 0 1-5.02 1.72c-.3-.03-.62-.04-.93-.07A11.447 11.447 0 0 0 8.88 21c7.386 0 11.43-6.13 11.414-11.414.015-.21.01-.38 0-.578a7.604 7.604 0 0 0 2.01-2.08 7.27 7.27 0 0 1-2.297.645 3.856 3.856 0 0 0 1.72-2.23"></path></svg></span></span></a><div class="buttonSet-separator"></div><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--highlightMenu" data-action="highlight" data-action-source="quote_menu--------------------------privatenote_text" data-skip-onboarding="true"><span class="svgIcon svgIcon--privatenoteFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M17.662 4.552H7.346A4.36 4.36 0 0 0 3 8.898v5.685c0 2.168 1.614 3.962 3.697 4.28v2.77c0 .303.35.476.59.29l3.904-2.994h6.48c2.39 0 4.35-1.96 4.35-4.35V8.9c0-2.39-1.95-4.346-4.34-4.346zM16 14.31a.99.99 0 0 1-1.003.99h-4.994C9.45 15.3 9 14.85 9 14.31v-3.02a.99.99 0 0 1 1-.99v-.782a2.5 2.5 0 0 1 2.5-2.51c1.38 0 2.5 1.13 2.5 2.51v.782c.552.002 1 .452 1 .99v3.02z"></path><path d="M14 9.81c0-.832-.674-1.68-1.5-1.68-.833 0-1.5.84-1.5 1.68v.49h3v-.49z"></path></g></svg></span></button></div></div><div class="highlightMenu-arrowClip"><span class="highlightMenu-arrow"></span></div></div></div></div></div><div class="loadingBar"></div><script>// <![CDATA[
window["obvInit"] = function (opt_embedded) {window["obvInit"]["embedded"] = opt_embedded; window["obvInit"]["ready"] = true;}
// ]]></script><script>// <![CDATA[
var GLOBALS = {"audioUrl":"https://d1fcbxp97j4nb2.cloudfront.net","baseUrl":"https://towardsdatascience.com","buildLabel":"38086-f2efc7b","currentUser":{"userId":"5f069cdb2c1b","username":"hamid.mirkhani","name":"HamidReza Mirkhani","email":"hamid.mirkhani@gmail.com","imageId":"0*_vE9uUIWyOu7cYeP.","createdAt":1489992563121,"isVerified":true,"subscriberEmail":"","onboardingStatus":1,"googleAccountId":"111595853085083752588","googleEmail":"hamid.mirkhani@gmail.com","hasPastMemberships":false,"isEnrolledInHightower":false,"isEligibleForHightower":true,"hightowerLastLockedAt":0,"isWriterProgramEnrolled":true,"isWriterProgramInvited":true,"isWriterProgramOptedOut":false,"writerProgramVersion":4,"writerProgramEnrolledAt":1551501618933,"friendLinkOnboarding":0,"hasAdditionalUnlocks":false,"hasApiAccess":false,"isQuarantined":false,"writerProgramDistributionSettingOptedIn":true},"currentUserHasUnverifiedEmail":false,"isAuthenticated":true,"isCurrentUserVerified":true,"language":"en-us","miroUrl":"https://cdn-images-1.medium.com","moduleUrls":{"base":"https://cdn-static-1.medium.com/_/fp/gen-js/main-base.bundle.99f-NRZ4oVdPRdW-ao8DQw.js","common-async":"https://cdn-static-1.medium.com/_/fp/gen-js/main-common-async.bundle.FlwFcvMmAdE8L4RC6Z-SKQ.js","hightower":"https://cdn-static-1.medium.com/_/fp/gen-js/main-hightower.bundle.kLcVGH57ddzik5SzyRVo5Q.js","home-screens":"https://cdn-static-1.medium.com/_/fp/gen-js/main-home-screens.bundle.DKe4H0j5P1E3G565goDtvg.js","misc-screens":"https://cdn-static-1.medium.com/_/fp/gen-js/main-misc-screens.bundle.kstvlHIeeAo7ZfyaTf8Bbg.js","notes":"https://cdn-static-1.medium.com/_/fp/gen-js/main-notes.bundle.jNkicUtbDgE4YuHg2DfzMw.js","payments":"https://cdn-static-1.medium.com/_/fp/gen-js/main-payments.bundle.Skd3NHYv_QyolcNDvircoA.js","posters":"https://cdn-static-1.medium.com/_/fp/gen-js/main-posters.bundle.rpCzRLCS93_kxET1qGGNcQ.js","power-readers":"https://cdn-static-1.medium.com/_/fp/gen-js/main-power-readers.bundle.YB5RO-0JrjOJPRQjMF69Tw.js","pubs":"https://cdn-static-1.medium.com/_/fp/gen-js/main-pubs.bundle.dK7vJpwcR7mYhilM0w4HMw.js","stats":"https://cdn-static-1.medium.com/_/fp/gen-js/main-stats.bundle.8rvs1eSJx02clv7X6LTNiQ.js"},"previewConfig":{"weightThreshold":1,"weightImageParagraph":0.51,"weightIframeParagraph":0.8,"weightTextParagraph":0.08,"weightEmptyParagraph":0,"weightP":0.003,"weightH":0.005,"weightBq":0.003,"minPTextLength":60,"truncateBoundaryChars":20,"detectTitle":true,"detectTitleLevThreshold":0.15},"productName":"Medium","supportsEdit":true,"termsUrl":"//medium.com/policy/9db0094a1e0f","textshotHost":"textshot.medium.com","transactionId":"1562948641551:a2b499061656","useragent":{"browser":"chrome","family":"chrome","os":"windows","version":75,"supportsDesktopEdit":true,"supportsInteract":true,"supportsView":true,"isMobile":false,"isTablet":false,"isNative":false,"supportsFileAPI":true,"isTier1":true,"clientVersion":"","unknownParagraphsBad":false,"clientChannel":"","supportsRealScrollEvents":true,"supportsVhUnits":true,"ruinsViewportSections":false,"supportsHtml5Video":true,"supportsMagicUnderlines":true,"isWebView":false,"isFacebookWebView":false,"supportsProgressiveMedia":true,"supportsPromotedPosts":true,"isBot":false,"isNativeIphone":false,"supportsCssVariables":true,"supportsVideoSections":true,"emojiSupportLevel":1,"isSearchBot":false,"isSyndicationBot":false,"isNativeAndroid":false,"isNativeIos":false,"isSeoBot":false,"supportsScrollableMetabar":true},"variants":{"allow_access":true,"allow_signup":true,"allow_test_auth":"disallow","signin_services":"twitter,facebook,google,email,google-fastidv,google-one-tap","signup_services":"twitter,facebook,google,email,google-fastidv,google-one-tap","google_sign_in_android":true,"reengagement_notification_duration":3,"browsable_stream_config_bucket":"curated-topics","enable_dedicated_series_tab_api_ios":true,"enable_post_import":true,"available_monthly_plan":"60e220181034","available_annual_plan":"2c754bcc2995","disable_ios_resume_reading_toast":true,"is_not_medium_subscriber":true,"glyph_font_set":"m2","enable_branding":true,"enable_branding_fonts":true,"max_premium_content_per_user_under_metering":3,"enable_automated_mission_control_triggers":true,"enable_lite_profile":true,"enable_marketing_emails":true,"enable_parsely":true,"enable_branch_io":true,"enable_ios_post_stats":true,"enable_lite_topics":true,"enable_lite_stories":true,"redis_read_write_splitting":true,"enable_tipalti_onboarding":true,"enable_international_tax_withholding":true,"enable_annual_renewal_reminder_email":true,"enable_janky_spam_rules":"users,posts","enable_new_collaborative_filtering_data":true,"android_rating_prompt_stories_read_threshold":2,"enable_google_one_tap":true,"enable_email_sign_in_captcha":true,"enable_primary_topic_for_mobile":true,"enable_lite_post":true,"enable_logged_out_homepage_signup":true,"use_new_admin_topic_backend":true,"enable_quarantine_rules":true,"enable_patronus_on_kubernetes":true,"pub_sidebar":true,"disable_mobile_featured_chunk":true,"enable_embedding_based_diversification":true,"enable_pub_newsletters":true,"enable_lite_pub_header_menu":true,"enable_lite_claps":true,"enable_lite_post_manager_gear_menu":true,"enable_live_user_post_scoring":true,"enable_lite_post_highlights":true,"enable_lite_post_highlights_view_only":true,"enable_tick_landing_page":true,"enable_lite_private_notes":true,"enable_lite_private_notes_li_100":true,"enable_trumpland_landing_page":true,"enable_lite_email_sign_in_flow":true,"enable_daily_read_digest_promo":true,"enable_lite_paywall_alert":true,"enable_edit_alt_text":true,"enable_serve_recs_from_ml_rank_homepage":true,"enable_serve_recs_from_ml_rank_digest":true,"enable_serve_recs_from_ml_rank_app_highlights":true,"enable_lite_thanks_to":true,"enable_lite_google_captcha":true,"enable_lite_branch_io":true,"enable_lite_notifications":true,"enable_ticks_digest_promo":true,"enable_lite_verify_email_butter_bar":true,"remove_social_proof_on_digest":true,"enable_lite_unread_notification_count":true},"xsrfToken":"NK7xE7lh717v","iosAppId":"828256236","supportEmail":"yourfriends@medium.com","fp":{"/icons/monogram-mask.svg":"https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg","/icons/favicon-dev-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-dev-editor.YKKRxBO8EMvIqhyCwIiJeQ.ico","/icons/favicon-hatch-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-hatch-editor.BuEyHIqlyh2s_XEk4Rl32Q.ico","/icons/favicon-medium-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-medium-editor.PiakrZWB7Yb80quUVQWM6g.ico"},"authBaseUrl":"https://medium.com","imageUploadSizeMb":25,"isAuthDomainRequest":false,"domainCollectionSlug":"towards-data-science","algoliaApiEndpoint":"https://MQ57UUUQZ2-dsn.algolia.net","algoliaAppId":"MQ57UUUQZ2","algoliaSearchOnlyApiKey":"394474ced050e3911ae2249ecc774921","iosAppStoreUrl":"https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8","iosAppLinkBaseUrl":"medium:","algoliaIndexPrefix":"medium_","androidPlayStoreUrl":"https://play.google.com/store/apps/details?id=com.medium.reader","googleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","androidPackage":"com.medium.reader","androidPlayStoreMarketScheme":"market://details?id=com.medium.reader","googleAuthUri":"https://accounts.google.com/o/oauth2/auth","androidScheme":"medium","layoutData":{"useDynamicScripts":false,"googleAnalyticsTrackingCode":"UA-24232453-2","jsShivUrl":"https://cdn-static-1.medium.com/_/fp/js/shiv.RI2ePTZ5gFmMgLzG5bEVAA.js","useDynamicCss":false,"faviconUrl":"https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium.3Y6xpZ-0FSdWDnPM3hSBIA.ico","faviconImageId":"1*8I-HPL0bfoIzGied-dzOvA.png","fontSets":[{"id":8,"url":"https://glyph.medium.com/css/e/sr/latin/e/ssr/latin/e/ssb/latin/m2.css"},{"id":11,"url":"https://glyph.medium.com/css/m2.css"},{"id":9,"url":"https://glyph.medium.com/css/mkt.css"}],"editorFaviconUrl":"https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium-editor.3Y6xpZ-0FSdWDnPM3hSBIA.ico","glyphUrl":"https://glyph.medium.com"},"authBaseUrlRev":"moc.muidem//:sptth","isDnt":false,"stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl","archiveUploadSizeMb":100,"paymentData":{"currencies":{"1":{"label":"US Dollar","external":"usd"}},"countries":{"1":{"label":"United States of America","external":"US"}},"accountTypes":{"1":{"label":"Individual","external":"individual"},"2":{"label":"Company","external":"company"}}},"previewConfig2":{"weightThreshold":1,"weightImageParagraph":0.05,"raiseImage":true,"enforceHeaderHierarchy":true,"isImageInsetRight":true},"isAmp":false,"iosScheme":"medium","isSwBoot":false,"lightstep":{"accessToken":"ce5be895bef60919541332990ac9fef2","carrier":"{\"ot-tracer-spanid\":\"7a92b5ed14552b34\",\"ot-tracer-traceid\":\"32198f2945325f2e\",\"ot-tracer-sampled\":\"true\"}","host":"collector-medium.lightstep.com"},"facebook":{"key":"542599432471018","namespace":"medium-com","scope":{"default":["public_profile","email"],"connect":["public_profile","email"],"login":["public_profile","email"],"share":["public_profile","email"]}},"editorsPicksTopicId":"3985d2a191c5","popularOnMediumTopicId":"9d34e48ecf94","memberContentTopicId":"13d7efd82fb2","audioContentTopicId":"3792abbd134","brandedSequenceId":"7d337ddf1941","isDoNotAuth":false,"buggle":{"url":"https://buggle.medium.com","videoUrl":"https://cdn-videos-1.medium.com","audioUrl":"https://cdn-audio-1.medium.com"},"referrerType":2,"isMeteredOut":false,"meterConfig":{"maxUnlockCount":3,"windowLength":"MONTHLY"},"partnerProgramEmail":"partnerprogram@medium.com","userResearchPrompts":[{"promptId":"li_post_page","type":0,"url":"www.calendly.com"},{"promptId":"li_home_page","type":1,"url":"mediumuserfeedback.typeform.com/to/GcFjEO"},{"promptId":"li_profile_page","type":2,"url":"www.calendly.com"}],"recaptchaKey":"6LdAokEUAAAAAC7seICd4vtC8chDb3jIXDQulyUJ","signinWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"countryCode":"CA","bypassMeter":false,"branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","paypal":{"clientMode":"production","oneYearGift":{"name":"Medium Membership (1 Year, Digital Gift Code)","description":"Unlimited access to the best and brightest stories on Medium. Gift codes can be redeemed at medium.com/redeem.","price":"50.00","currency":"USD","sku":"membership-gift-1-yr"}},"collectionConfig":{"mediumOwnedAndOperatedCollectionIds":["544c7006046e","bcc38c8f6edf","444d13b52878","8d6b8a439e32","92d2092dc598","1285ba81cada","cb8577c9149e","8ccfed20cbb2","ae2a65f35510","3f6ecf56618"]}}
// ]]></script><script charset="UTF-8" src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/main-base.bundle.99f-NRZ4oVdPRdW-ao8DQw.js.download" async=""></script><script>// <![CDATA[
window["obvInit"]({"value":{"id":"e66f2e30fb96","versionId":"e082586ca00a","creatorId":"aff72a0c1243","creator":{"userId":"aff72a0c1243","name":"Sik-Ho Tsang","username":"sh.tsang","createdAt":1524566858911,"imageId":"1*OxjNUHcLFU8-pp-j8su6pg.jpeg","backgroundImageId":"","bio":"PhD, Researcher. I share what I've learnt and done. :) My LinkedIn: https://www.linkedin.com/in/sh-tsang/","twitterScreenName":"","socialStats":{"userId":"aff72a0c1243","usersFollowedCount":31,"usersFollowedByCount":2051,"type":"SocialStats"},"social":{"userId":"5f069cdb2c1b","targetUserId":"aff72a0c1243","type":"Social"},"facebookAccountId":"","allowNotes":1,"mediumMemberAt":0,"isNsfw":false,"isWriterProgramEnrolled":true,"isQuarantined":false,"type":"User"},"homeCollection":{"id":"7f60cf5620c9","name":"Towards Data Science","slug":"towards-data-science","tags":["DATA SCIENCE","MACHINE LEARNING","ARTIFICIAL INTELLIGENCE","ANALYTICS","PROGRAMMING"],"creatorId":"895063a310f4","description":"Sharing concepts, ideas, and codes.","shortDescription":"Sharing concepts, ideas, and codes.","image":{"imageId":"1*F0LADxTtsKOgmPa-_7iUEQ.jpeg","filter":"","backgroundSize":"","originalWidth":1275,"originalHeight":1275,"strategy":"resample","height":0,"width":0},"metadata":{"followerCount":240824,"activeAt":1562947087565},"virtuals":{"permissions":{"canPublish":false,"canPublishAll":false,"canRepublish":false,"canRemove":false,"canManageAll":false,"canSubmit":false,"canEditPosts":false,"canAddWriters":false,"canViewStats":false,"canSendNewsletter":false,"canViewLockedPosts":false,"canViewCloaked":false,"canEditOwnPosts":false,"canBeAssignedAuthor":false,"canEnrollInHightower":false,"canLockPostsForMediumMembers":false,"canLockOwnPostsForMediumMembers":false},"isSubscribed":false,"isNewsletterSubscribed":false,"isEnrolledInHightower":false,"isEligibleForHightower":false,"mediumNewsletterId":"","isSubscribedToMediumNewsletter":false},"logo":{"imageId":"1*5EUO1kUYBthpOCPzRj_l2g.png","filter":"","backgroundSize":"","originalWidth":1010,"originalHeight":376,"strategy":"resample","height":0,"width":0},"twitterUsername":"TDataScience","facebookPageName":"towardsdatascience","collectionMastheadId":"8b6aceffde6","domain":"towardsdatascience.com","sections":[{"type":2,"collectionHeaderMetadata":{"title":"Towards Data Science","description":"Sharing concepts, ideas, and codes","backgroundImage":{},"logoImage":{},"alignment":2,"layout":5}},{"type":1,"postListMetadata":{"source":3,"layout":4,"number":2,"postIds":["39967f07a67f","1b295ea5bbe0"]}},{"type":1,"postListMetadata":{"source":3,"layout":4,"number":3,"postIds":["85e123a448bf","937d1fb582ee","792160ab5495"],"sectionHeader":"Featured "}},{"type":1,"postListMetadata":{"source":1,"layout":4,"number":6,"postIds":[],"sectionHeader":"Latest"}},{"type":1,"postListMetadata":{"source":3,"layout":4,"number":2,"postIds":["272f28835af7","67a3e08ce50d"],"sectionHeader":"Our Letters"}},{"type":3,"promoMetadata":{"sectionHeader":"","promoId":"3245ee5b4331"}},{"type":1,"postListMetadata":{"source":2,"layout":4,"number":6,"postIds":[],"sectionHeader":"Trending"}},{"type":1,"postListMetadata":{"source":3,"layout":4,"number":2,"postIds":["3bf37f75a345","3920888f831c"],"sectionHeader":"Our Readers’ Guide"}},{"type":1,"postListMetadata":{"source":4,"layout":4,"number":9,"postIds":[],"tagSlug":"Towards Data Science","sectionHeader":"Editors' Picks"}},{"type":1,"postListMetadata":{"source":3,"layout":4,"number":2,"postIds":["96667b06af5","d691af11cc2f"],"sectionHeader":"Contribute"}},{"type":1,"postListMetadata":{"source":3,"layout":4,"number":2,"postIds":["8bae371d4bb1","766cdd74d13e"]}},{"type":1,"postListMetadata":{"source":4,"layout":5,"number":3,"postIds":[],"tagSlug":"Towards Data Science","sectionHeader":"Last Chance To Read"}}],"tintColor":"#FF355876","lightText":true,"favicon":{"imageId":"1*F0LADxTtsKOgmPa-_7iUEQ.jpeg","filter":"","backgroundSize":"","originalWidth":1275,"originalHeight":1275,"strategy":"resample","height":0,"width":0},"colorPalette":{"defaultBackgroundSpectrum":{"colorPoints":[{"color":"#FF668AAA","point":0},{"color":"#FF61809D","point":0.1},{"color":"#FF5A7690","point":0.2},{"color":"#FF546C83","point":0.3},{"color":"#FF4D6275","point":0.4},{"color":"#FF455768","point":0.5},{"color":"#FF3D4C5A","point":0.6},{"color":"#FF34414C","point":0.7},{"color":"#FF2B353E","point":0.8},{"color":"#FF21282F","point":0.9},{"color":"#FF161B1F","point":1}],"backgroundColor":"#FFFFFFFF"},"tintBackgroundSpectrum":{"colorPoints":[{"color":"#FF355876","point":0},{"color":"#FF4D6C88","point":0.1},{"color":"#FF637F99","point":0.2},{"color":"#FF7791A8","point":0.3},{"color":"#FF8CA2B7","point":0.4},{"color":"#FF9FB3C6","point":0.5},{"color":"#FFB2C3D4","point":0.6},{"color":"#FFC5D2E1","point":0.7},{"color":"#FFD7E2EE","point":0.8},{"color":"#FFE9F1FA","point":0.9},{"color":"#FFFBFFFF","point":1}],"backgroundColor":"#FF355876"},"highlightSpectrum":{"colorPoints":[{"color":"#FFEDF4FC","point":0},{"color":"#FFE9F2FD","point":0.1},{"color":"#FFE6F1FD","point":0.2},{"color":"#FFE2EFFD","point":0.3},{"color":"#FFDFEEFD","point":0.4},{"color":"#FFDBECFE","point":0.5},{"color":"#FFD7EBFE","point":0.6},{"color":"#FFD4E9FE","point":0.7},{"color":"#FFD0E7FF","point":0.8},{"color":"#FFCCE6FF","point":0.9},{"color":"#FFC8E4FF","point":1}],"backgroundColor":"#FFFFFFFF"}},"navItems":[{"type":4,"title":"Data Science","url":"https://towardsdatascience.com/data-science/home","topicId":"cf416843aadc","source":"topicId"},{"type":4,"title":"Machine Learning","url":"https://towardsdatascience.com/machine-learning/home","topicId":"a5c9b2f1cb6b","source":"topicId"},{"type":4,"title":"Programming","url":"https://towardsdatascience.com/programming/home","topicId":"41533a1dc73c","source":"topicId"},{"type":4,"title":"Visualization","url":"https://towardsdatascience.com/data-visualization/home","topicId":"825e6cb8b9ce","source":"topicId"},{"type":4,"title":"AI","url":"https://towardsdatascience.com/artificial-intelligence/home","topicId":"7f029b17bf96","source":"topicId"},{"type":4,"title":"Journalism","url":"https://towardsdatascience.com/data-journalism/home","topicId":"27a6ac3980c6","source":"topicId"},{"type":2,"title":"Events","postId":"8bae371d4bb1","url":"https://towardsdatascience.com/toronto-machine-learning-summit-8bae371d4bb1","source":"postId"},{"type":3,"title":"Submit","url":"https://towardsdatascience.com/contribute/home"}],"colorBehavior":2,"instantArticlesState":0,"acceleratedMobilePagesState":0,"googleAnalyticsId":"UA-19707169-24","ampLogo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"header":{"title":"Towards Data Science","description":"Sharing concepts, ideas, and codes","backgroundImage":{},"logoImage":{},"alignment":2,"layout":5},"paidForDomainAt":1509037374118,"type":"Collection"},"homeCollectionId":"7f60cf5620c9","title":"Review: SegNet (Semantic Segmentation)","detectedLanguage":"en","latestVersion":"e082586ca00a","latestPublishedVersion":"e082586ca00a","hasUnpublishedEdits":false,"latestRev":690,"createdAt":1549470644413,"updatedAt":1554779924640,"acceptedAt":0,"firstPublishedAt":1549809325230,"latestPublishedAt":1554779924640,"vote":false,"experimentalCss":"","displayAuthor":"","content":{"subtitle":"Encoder Decoder Architecture, Using Max Pooling Indices to Upsample, Outperforms FCN, DeepLabv1, DeconvNet","bodyModel":{"paragraphs":[{"name":"ac92","type":3,"text":"Review: SegNet (Semantic Segmentation)","markups":[]},{"name":"7054","type":13,"text":"Encoder Decoder Architecture, Using Max Pooling Indices to Upsample, Outperforms FCN, DeepLabv1, DeconvNet","markups":[]},{"name":"4c06","type":4,"text":"SegNet by Authors (https://www.youtube.com/watch?v=CxanE_W46ts)","markups":[{"type":3,"start":19,"end":62,"href":"https://www.youtube.com/watch?v=CxanE_W46ts","title":"","rel":"nofollow noopener","anchorType":0},{"type":1,"start":0,"end":19}],"layout":1,"metadata":{"id":"0*k8ejti9_6CHwxzFQ.gif","originalWidth":600,"originalHeight":338,"isFeatured":true}},{"name":"fed9","type":1,"text":"In this story, SegNet, by University of Cambridge, is briefly reviewed. Originally, it was submitted to 2015 CVPR, but at last it is not being published in CVPR (But it’s 2015 arXiv tech report version and still got over 100 citations). Instead, it is published in 2017 TPAMI with more than 1800 citations. And right now the first author has become the Director of Deep Learning and AI in Magic Leap Inc. (Sik-Ho Tsang @ Medium)","markups":[{"type":3,"start":406,"end":418,"anchorType":2,"userId":"aff72a0c1243"},{"type":1,"start":15,"end":21},{"type":1,"start":26,"end":49},{"type":1,"start":171,"end":182},{"type":1,"start":221,"end":234},{"type":1,"start":265,"end":275},{"type":1,"start":291,"end":305}],"hasDropCap":true},{"name":"74bc","type":1,"text":"Below is the demo from authors:","markups":[]},{"name":"fe91","type":11,"text":"SegNet by Authors (https://www.youtube.com/watch?v=CxanE_W46ts)","markups":[{"type":3,"start":19,"end":62,"href":"https://www.youtube.com/watch?v=CxanE_W46ts","title":"","rel":"nofollow noopener noopener","anchorType":0},{"type":1,"start":0,"end":19}],"layout":1,"iframe":{"mediaResourceId":"87dea9894b49a6c02eb41704f9e3b6b8","iframeWidth":854,"iframeHeight":480,"thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2FCxanE_W46ts%2Fhqdefault.jpg&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"50b8","type":1,"text":"There is also an interesting demo that we can choose a random image or even upload our own image to try the SegNet. I have tried as below:","markups":[]},{"name":"a508","type":9,"text":"http://mi.eng.cam.ac.uk/projects/segnet/demo.php","markups":[{"type":3,"start":0,"end":48,"href":"http://mi.eng.cam.ac.uk/projects/segnet/demo.php","title":"","rel":"nofollow","anchorType":0}]},{"name":"0623","type":4,"text":"The segmentation result for a road scene image that I found from internet","markups":[{"type":1,"start":0,"end":73}],"layout":1,"metadata":{"id":"1*TX4rkqNaqX_aA-CAId8WPw.png","originalWidth":1217,"originalHeight":514}},{"name":"fb6c","type":3,"text":"Outline","markups":[]},{"name":"819f","type":10,"text":"Encoder Decoder Architecture","markups":[{"type":1,"start":0,"end":28}]},{"name":"62f2","type":10,"text":"Differences from DeconvNet and U-Net","markups":[{"type":1,"start":0,"end":36}]},{"name":"bca1","type":10,"text":"Results","markups":[{"type":1,"start":0,"end":7}]},{"name":"2e18","type":3,"text":"1. Encoder Decoder Architecture","markups":[{"type":1,"start":0,"end":31}]},{"name":"81ab","type":4,"text":"SegNet: Encoder Decoder Architecture","markups":[{"type":1,"start":0,"end":36}],"layout":3,"metadata":{"id":"1*8qIwQ7drLTf08gami25QDw.png","originalWidth":1128,"originalHeight":328}},{"name":"8ea0","type":9,"text":"SegNet has an encoder network and a corresponding decoder network, followed by a final pixelwise classification layer.","markups":[{"type":1,"start":14,"end":22},{"type":1,"start":50,"end":58}]},{"name":"3999","type":13,"text":"1.1. Encoder","markups":[]},{"name":"79cd","type":9,"text":"At the encoder, convolutions and max pooling are performed.","markups":[]},{"name":"e81f","type":9,"text":"There are 13 convolutional layers from VGG-16. (The original fully connected layers are discarded.)","markups":[]},{"name":"f2a5","type":9,"text":"While doing 2×2 max pooling, the corresponding max pooling indices (locations) are stored.","markups":[]},{"name":"36a8","type":13,"text":"1.2. Decoder","markups":[]},{"name":"57ce","type":4,"text":"Upsampling Using Max-Pooling Indices","markups":[{"type":1,"start":0,"end":36}],"layout":1,"metadata":{"id":"1*wdNu8Wd2HOLsOfRUInwwaA.png","originalWidth":234,"originalHeight":221}},{"name":"0795","type":9,"text":"At the decoder, upsampling and convolutions are performed. At the end, there is softmax classifier for each pixel.","markups":[]},{"name":"2199","type":9,"text":"During upsampling, the max pooling indices at the corresponding encoder layer are recalled to upsample as shown above.","markups":[]},{"name":"89fb","type":9,"text":"Finally, a K-class softmax classifier is used to predict the class for each pixel.","markups":[]},{"name":"8c52","type":3,"text":"2. Differences from DeconvNet and U-Net","markups":[{"type":1,"start":0,"end":39}]},{"name":"8a4d","type":1,"text":"DeconvNet and U-Net have similar structures as SegNet.","markups":[{"type":3,"start":0,"end":9,"href":"https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e","title":"","rel":"noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener noopener","anchorType":0},{"type":3,"start":14,"end":19,"href":"http://U-Net","title":"","rel":"","anchorType":0}]},{"name":"ac87","type":13,"text":"2.1. Differences from DeconvNet","markups":[{"type":3,"start":22,"end":31,"href":"https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e","title":"","rel":"noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener noopener","anchorType":0}]},{"name":"c597","type":9,"text":"Similar upsampling approach called unpooling is used.","markups":[]},{"name":"a98e","type":9,"text":"However, there are fully-connected layers which make the model larger.","markups":[]},{"name":"a741","type":13,"text":"2.2. Differences from U-Net","markups":[{"type":3,"start":22,"end":27,"href":"http://U-Net","title":"","rel":"","anchorType":0}]},{"name":"9ffc","type":9,"text":"It is used for biomedical image segmentation.","markups":[]},{"name":"78db","type":9,"text":"Instead of using pooling indices, the entire feature maps are transfer from encoder to decoder, then with concatenation to perform convolution.","markups":[]},{"name":"1228","type":9,"text":"This makes the model larger and need more memory.","markups":[]},{"name":"286c","type":3,"text":"3. Results","markups":[]},{"name":"4391","type":9,"text":"Two datasets are tried. One is CamVid dataset for Road Scene Segmentation. One is SUN RGB-D dataset for Indoor Scene Segmentation.","markups":[]},{"name":"c8c7","type":13,"text":"3.1. CamVid dataset for Road Scene Segmentation","markups":[]},{"name":"5eac","type":4,"text":"Compared With Conventional Approaches on CamVid dataset for Road Scene Segmentation","markups":[{"type":1,"start":0,"end":83}],"layout":3,"metadata":{"id":"1*GHGbOKojBBvtxyBlG8XAlw.png","originalWidth":1186,"originalHeight":376}},{"name":"cc90","type":9,"text":"As shown above, SegNet obtains very good results for many classes. It also got the highest class average and global average.","markups":[]},{"name":"daea","type":4,"text":"Compared With Deep Learning Approaches on CamVid dataset for Road Scene Segmentation","markups":[{"type":1,"start":0,"end":84}],"layout":3,"metadata":{"id":"1*aSanrhujo09-PYNneyPBvw.png","originalWidth":1116,"originalHeight":221}},{"name":"4cdd","type":9,"text":"SegNet obtains highest global average accuracy (G), class average accuracy (C), mIOU and Boundary F1-measure (BF). It outperforms FCN, DeepLabv1 and DeconvNet.","markups":[{"type":3,"start":130,"end":133,"href":"https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1","title":"","rel":"noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener noopener","anchorType":0},{"type":3,"start":135,"end":145,"href":"https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d","title":"","rel":"","anchorType":0},{"type":3,"start":149,"end":158,"href":"https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e","title":"","rel":"noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener noopener","anchorType":0}]},{"name":"833b","type":4,"text":"Qualitative Results","markups":[{"type":1,"start":0,"end":19}],"layout":3,"metadata":{"id":"1*1YEl-0zr77hfUetR-HmnJw.png","originalWidth":1083,"originalHeight":1116}},{"name":"5087","type":13,"text":"3.2. SUN RGB-D Dataset for Indoor Scene Segmentation","markups":[]},{"name":"7f65","type":9,"text":"Only RGB is used, depth (D) information are not used.","markups":[]},{"name":"bee0","type":4,"text":"Compared With Deep Learning Approaches on SUN RGB-D Dataset for Indoor Scene Segmentation","markups":[{"type":1,"start":0,"end":89}],"layout":3,"metadata":{"id":"1*tB_mufCuOLK8imyUHceSVQ.png","originalWidth":1099,"originalHeight":188}},{"name":"21ab","type":9,"text":"Again, SegNet outperforms FCN, DeconvNet, and DeepLabv1.","markups":[{"type":3,"start":26,"end":29,"href":"https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1","title":"","rel":"noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener noopener","anchorType":0},{"type":3,"start":31,"end":40,"href":"https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e","title":"","rel":"noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener noopener","anchorType":0},{"type":3,"start":46,"end":55,"href":"https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d","title":"","rel":"noopener","anchorType":0}]},{"name":"d63b","type":9,"text":"SegNet only got a bit inferior to DeepLabv1 for mIOU.","markups":[{"type":3,"start":34,"end":43,"href":"https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d","title":"","rel":"noopener","anchorType":0}]},{"name":"4090","type":4,"text":"Class Average Accuracy for Different Classes","markups":[{"type":1,"start":0,"end":44}],"layout":3,"metadata":{"id":"1*_7h5yLmGj4gbh-aD7_-Ydw.png","originalWidth":1200,"originalHeight":160}},{"name":"cf47","type":9,"text":"Higher accuracy for large-size classes.","markups":[]},{"name":"19c9","type":9,"text":"Lower accuracy for small-size classes.","markups":[]},{"name":"41e8","type":4,"text":"Qualitative Results","markups":[{"type":1,"start":0,"end":19}],"layout":3,"metadata":{"id":"1*F0WEadFwdQ1JWyHaH7UoPw.png","originalWidth":1204,"originalHeight":1096}},{"name":"fb52","type":13,"text":"3.3. Memory and Inference Time","markups":[]},{"name":"576d","type":4,"text":"Memory and Inference Time","markups":[{"type":1,"start":0,"end":25}],"layout":3,"metadata":{"id":"1*q9-COyyJ2d8oK6XhuKzb-g.png","originalWidth":1233,"originalHeight":138}},{"name":"25d8","type":9,"text":"SegNet is slower than FCN and DeepLabv1 because SegNet contains the decoder architecture. And it is faster than DeconvNet because it does not have fully connected layers.","markups":[{"type":3,"start":22,"end":25,"href":"https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1","title":"","rel":"noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener noopener","anchorType":0},{"type":3,"start":30,"end":39,"href":"https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d","title":"","rel":"noopener","anchorType":0},{"type":3,"start":112,"end":121,"href":"https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e","title":"","rel":"noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener noopener","anchorType":0}]},{"name":"cfd6","type":9,"text":"And SegNet has low memory requirement during both training and testing. And the model size is much smaller than FCN and DeconvNet.","markups":[{"type":3,"start":112,"end":115,"href":"https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1","title":"","rel":"noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener noopener","anchorType":0},{"type":3,"start":120,"end":129,"href":"https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e","title":"","rel":"noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener noopener","anchorType":0}]},{"name":"de3b","type":13,"text":"References","markups":[]},{"name":"9eaa","type":1,"text":"[2015 arXiv] [SegNet]\nSegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling","markups":[{"type":3,"start":22,"end":120,"href":"https://arxiv.org/abs/1505.07293","title":"","rel":"","anchorType":0}]},{"name":"3317","type":1,"text":"[2017 TPAMI] [SegNet]\nSegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation","markups":[{"type":3,"start":22,"end":102,"href":"https://arxiv.org/abs/1511.00561","title":"","rel":"","anchorType":0}]},{"name":"93e9","type":13,"text":"My Previous Reviews","markups":[]},{"name":"4da9","type":1,"text":"Image Classification\n[LeNet] [AlexNet] [ZFNet] [VGGNet] [SPPNet] [PReLU-Net] [STN] [DeepImage] [GoogLeNet / Inception-v1] [BN-Inception / Inception-v2] [Inception-v3] [Inception-v4] [Xception] [MobileNetV1] [ResNet] [Pre-Activation ResNet] [RiR] [RoR] [Stochastic Depth] [WRN] [FractalNet] [Trimps-Soushen] [PolyNet] [ResNeXt] [DenseNet] [PyramidNet]","markups":[{"type":3,"start":22,"end":27,"href":"https://medium.com/@sh.tsang/paper-brief-review-of-lenet-1-lenet-4-lenet-5-boosted-lenet-4-image-classification-1f5f809dbf17","title":"","rel":"","anchorType":0},{"type":3,"start":30,"end":37,"href":"https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160","title":"","rel":"","anchorType":0},{"type":3,"start":40,"end":45,"href":"https://medium.com/coinmonks/paper-review-of-zfnet-the-winner-of-ilsvlc-2013-image-classification-d1a5a0c45103","title":"","rel":"","anchorType":0},{"type":3,"start":48,"end":54,"href":"https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11","title":"","rel":"","anchorType":0},{"type":3,"start":57,"end":63,"href":"https://medium.com/coinmonks/review-sppnet-1st-runner-up-object-detection-2nd-runner-up-image-classification-in-ilsvrc-906da3753679","title":"","rel":"","anchorType":0},{"type":3,"start":66,"end":75,"href":"https://medium.com/coinmonks/review-prelu-net-the-first-to-surpass-human-level-performance-in-ilsvrc-2015-image-f619dddd5617","title":"","rel":"","anchorType":0},{"type":3,"start":78,"end":81,"href":"https://towardsdatascience.com/review-stn-spatial-transformer-network-image-classification-d3cbd98a70aa","title":"","rel":"nofollow noopener noopener","anchorType":0},{"type":3,"start":84,"end":93,"href":"https://medium.com/@sh.tsang/review-deep-image-a-big-data-solution-for-image-recognition-99e5f7b1c802","title":"","rel":"","anchorType":0},{"type":3,"start":96,"end":120,"href":"https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7","title":"","rel":"","anchorType":0},{"type":3,"start":123,"end":150,"href":"https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651","title":"","rel":"","anchorType":0},{"type":3,"start":153,"end":165,"href":"https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c","title":"","rel":"","anchorType":0},{"type":3,"start":168,"end":180,"href":"https://towardsdatascience.com/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc","title":"","rel":"noopener nofollow noopener noopener nofollow noopener noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":183,"end":191,"href":"https://towardsdatascience.com/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568","title":"","rel":"noopener nofollow noopener noopener nofollow noopener noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":194,"end":205,"href":"https://towardsdatascience.com/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69","title":"","rel":"noopener nofollow noopener noopener nofollow noopener noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":208,"end":214,"href":"https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8","title":"","rel":"noopener nofollow noopener noopener nofollow noopener noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":217,"end":238,"href":"https://towardsdatascience.com/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e","title":"","rel":"noopener nofollow noopener noopener nofollow noopener noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":241,"end":244,"href":"https://medium.com/@sh.tsang/review-rir-resnet-in-resnet-image-classification-be4c79fde8ba","title":"","rel":"","anchorType":0},{"type":3,"start":247,"end":250,"href":"https://towardsdatascience.com/review-ror-resnet-of-resnet-multilevel-resnet-image-classification-cd3b0fcc19bb","title":"","rel":"noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":253,"end":269,"href":"https://towardsdatascience.com/review-stochastic-depth-image-classification-a4e225807f4a","title":"","rel":"noopener nofollow noopener noopener nofollow noopener noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":272,"end":275,"href":"https://towardsdatascience.com/review-wrns-wide-residual-networks-image-classification-d3feb3fb2004","title":"","rel":"noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":278,"end":288,"href":"https://medium.com/datadriveninvestor/review-fractalnet-image-classification-c5bdd855a090","title":"","rel":"","anchorType":0},{"type":3,"start":291,"end":305,"href":"https://towardsdatascience.com/review-trimps-soushen-winner-in-ilsvrc-2016-image-classification-dfbc423111dd","title":"","rel":"noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":308,"end":315,"href":"https://towardsdatascience.com/review-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea","title":"","rel":"noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":318,"end":325,"href":"https://towardsdatascience.com/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac","title":"","rel":"noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":328,"end":336,"href":"https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803","title":"","rel":"nofollow noopener noopener noopener nofollow noopener noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":339,"end":349,"href":"https://medium.com/@sh.tsang/review-pyramidnet-deep-pyramidal-residual-networks-image-classification-85a87b60ae78","title":"","rel":"","anchorType":0},{"type":1,"start":0,"end":21}]},{"name":"8b77","type":1,"text":"Object Detection\n[OverFeat] [R-CNN] [Fast R-CNN] [Faster R-CNN] [DeepID-Net] [R-FCN] [ION] [MultiPathNet] [NoC] [G-RMI] [TDM] [SSD] [DSSD] [YOLOv1] [YOLOv2 / YOLO9000] [YOLOv3] [FPN] [RetinaNet] [DCN]","markups":[{"type":3,"start":18,"end":26,"href":"https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754","title":"","rel":"","anchorType":0},{"type":3,"start":29,"end":34,"href":"https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1","title":"","rel":"","anchorType":0},{"type":3,"start":37,"end":47,"href":"https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba","title":"","rel":"","anchorType":0},{"type":3,"start":50,"end":62,"href":"https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202","title":"","rel":"noopener nofollow nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":65,"end":75,"href":"https://towardsdatascience.com/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6","title":"","rel":"noopener nofollow nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":78,"end":83,"href":"https://towardsdatascience.com/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c","title":"","rel":"nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":86,"end":89,"href":"https://towardsdatascience.com/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766","title":"","rel":"nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":92,"end":104,"href":"https://towardsdatascience.com/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413","title":"","rel":"nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":107,"end":110,"href":"https://medium.com/datadriveninvestor/review-noc-winner-in-2015-coco-ilsvrc-detection-object-detection-d5cc84e372a","title":"","rel":"","anchorType":0},{"type":3,"start":113,"end":118,"href":"https://towardsdatascience.com/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4","title":"","rel":"nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":121,"end":124,"href":"https://medium.com/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151","title":"","rel":"","anchorType":0},{"type":3,"start":127,"end":130,"href":"https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11","title":"","rel":"noopener nofollow nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":133,"end":137,"href":"https://towardsdatascience.com/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5","title":"","rel":"noopener nofollow nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":140,"end":146,"href":"https://towardsdatascience.com/yolov1-you-only-look-once-object-detection-e1f3ffec8a89","title":"","rel":"noopener nofollow nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":149,"end":166,"href":"https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65","title":"","rel":"noopener nofollow nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":169,"end":175,"href":"https://towardsdatascience.com/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6","title":"","rel":"","anchorType":0},{"type":3,"start":178,"end":181,"href":"https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610","title":"","rel":"nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":184,"end":193,"href":"https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4","title":"","rel":"nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":196,"end":199,"href":"https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44","title":"","rel":"nofollow noopener noopener","anchorType":0},{"type":1,"start":0,"end":17}]},{"name":"6582","type":1,"text":"Semantic Segmentation\n[FCN] [DeconvNet] [DeepLabv1 & DeepLabv2] [ParseNet] [DilatedNet] [PSPNet] [DeepLabv3]","markups":[{"type":3,"start":23,"end":26,"href":"https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1","title":"","rel":"noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":29,"end":38,"href":"https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e","title":"","rel":"noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":41,"end":62,"href":"https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d","title":"","rel":"noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":65,"end":73,"href":"https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990","title":"","rel":"","anchorType":0},{"type":3,"start":76,"end":86,"href":"https://towardsdatascience.com/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5","title":"","rel":"noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":89,"end":95,"href":"https://towardsdatascience.com/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d","title":"","rel":"noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":98,"end":107,"href":"https://towardsdatascience.com/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74","title":"","rel":"nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":1,"start":0,"end":22}]},{"name":"fc65","type":1,"text":"Biomedical Image Segmentation\n[CUMedVision1] [CUMedVision2 / DCAN] [U-Net] [CFS-FCN] [U-Net+ResNet]","markups":[{"type":3,"start":31,"end":43,"href":"https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6","title":"","rel":"","anchorType":0},{"type":3,"start":46,"end":65,"href":"https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560","title":"","rel":"","anchorType":0},{"type":3,"start":68,"end":73,"href":"https://towardsdatascience.com/review-u-net-biomedical-image-segmentation-d02bf06ca760","title":"","rel":"noopener nofollow nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":76,"end":83,"href":"https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6","title":"","rel":"","anchorType":0},{"type":3,"start":86,"end":98,"href":"https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43","title":"","rel":"","anchorType":0},{"type":1,"start":0,"end":30}]},{"name":"3134","type":1,"text":"Instance Segmentation\n[DeepMask] [SharpMask] [MultiPathNet] [MNC] [InstanceFCN] [FCIS]","markups":[{"type":3,"start":23,"end":31,"href":"https://towardsdatascience.com/review-deepmask-instance-segmentation-30327a072339","title":"","rel":"nofollow noopener nofollow noopener noopener nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":34,"end":43,"href":"https://towardsdatascience.com/review-sharpmask-instance-segmentation-6509f7401a61","title":"","rel":"nofollow noopener nofollow noopener noopener nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":46,"end":58,"href":"https://towardsdatascience.com/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413","title":"","rel":"nofollow noopener noopener noopener nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":61,"end":64,"href":"https://towardsdatascience.com/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34","title":"","rel":"nofollow noopener nofollow noopener noopener nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":67,"end":78,"href":"https://towardsdatascience.com/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92","title":"","rel":"noopener nofollow nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":81,"end":85,"href":"https://towardsdatascience.com/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2","title":"","rel":"nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":1,"start":0,"end":22}]},{"name":"58de","type":1,"text":"Super Resolution\n[SRCNN] [FSRCNN] [VDSR] [ESPCN] [RED-Net] [DRCN] [DRRN] [LapSRN & MS-LapSRN]","markups":[{"type":3,"start":18,"end":23,"href":"https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c","title":"","rel":"","anchorType":0},{"type":3,"start":26,"end":32,"href":"https://towardsdatascience.com/review-fsrcnn-super-resolution-80ca2ee14da4","title":"","rel":"nofollow noopener nofollow noopener noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":35,"end":39,"href":"https://towardsdatascience.com/review-vdsr-super-resolution-f8050d49362f","title":"","rel":"nofollow noopener nofollow noopener noopener nofollow noopener nofollow noopener nofollow noopener nofollow noopener noopener","anchorType":0},{"type":3,"start":42,"end":47,"href":"https://medium.com/datadriveninvestor/review-espcn-real-time-sr-super-resolution-8dceca249350","title":"","rel":"","anchorType":0},{"type":3,"start":50,"end":57,"href":"https://medium.com/datadriveninvestor/review-red-net-residual-encoder-decoder-network-denoising-super-resolution-cb6364ae161e","title":"","rel":"","anchorType":0},{"type":3,"start":60,"end":64,"href":"https://medium.com/datadriveninvestor/review-drcn-deeply-recursive-convolutional-network-super-resolution-f0a380f79b20","title":"","rel":"","anchorType":0},{"type":3,"start":67,"end":71,"href":"https://towardsdatascience.com/review-drrn-deep-recursive-residual-network-super-resolution-dca4a35ce994","title":"","rel":"noopener","anchorType":0},{"type":3,"start":74,"end":92,"href":"https://towardsdatascience.com/review-lapsrn-ms-lapsrn-laplacian-pyramid-super-resolution-network-super-resolution-c5fe2b65f5e8","title":"","rel":"noopener","anchorType":0},{"type":1,"start":0,"end":17}]}],"sections":[{"name":"17dc","startIndex":0},{"name":"b0ca","startIndex":9},{"name":"bfb5","startIndex":13},{"name":"eb87","startIndex":25},{"name":"760b","startIndex":34},{"name":"433c","startIndex":55}]},"postDisplay":{"coverless":true}},"virtuals":{"statusForCollection":"APPROVED","allowNotes":true,"previewImage":{"imageId":"0*k8ejti9_6CHwxzFQ.gif","filter":"","backgroundSize":"","originalWidth":600,"originalHeight":338,"strategy":"resample","height":0,"width":0},"wordCount":662,"imageCount":11,"readingTime":3.7981132075471695,"subtitle":"Encoder Decoder Architecture, Using Max Pooling Indices to Upsample, Outperforms FCN, DeepLabv1, DeconvNet","publishedInCount":1,"usersBySocialRecommends":[],"noIndex":false,"recommends":62,"socialRecommends":[],"isBookmarked":false,"tags":[{"slug":"machine-learning","name":"Machine Learning","postCount":76604,"metadata":{"postCount":76604,"coverImage":{"id":"1*Objn6iYe6g4-DLDV67JKWA.jpeg","originalWidth":1904,"originalHeight":1068,"isFeatured":true}},"type":"Tag"},{"slug":"deep-learning","name":"Deep Learning","postCount":18840,"metadata":{"postCount":18840,"coverImage":{"id":"1*uyQ61XHkTY2qHhovclaLIQ.png","originalWidth":1073,"originalHeight":741,"isFeatured":true}},"type":"Tag"},{"slug":"artificial-intelligence","name":"Artificial Intelligence","postCount":87889,"metadata":{"postCount":87889,"coverImage":{"id":"1*gAn_BSffVBcwCIR6bDgK1g.jpeg"}},"type":"Tag"},{"slug":"data-science","name":"Data Science","postCount":50913,"metadata":{"postCount":50913,"coverImage":{"id":"1*Objn6iYe6g4-DLDV67JKWA.jpeg","originalWidth":1904,"originalHeight":1068,"isFeatured":true}},"type":"Tag"},{"slug":"semantic-segmentation","name":"Semantic Segmentation","postCount":47,"metadata":{"postCount":47,"coverImage":{"id":"1*nXlx7s4wQhVgVId8qkkMMA.png","originalWidth":1058,"originalHeight":478,"isFeatured":true}},"type":"Tag"}],"socialRecommendsCount":0,"responsesCreatedCount":1,"links":{"entries":[{"url":"http://U-Net","alts":[],"httpStatus":0},{"url":"https://medium.com/@sh.tsang/review-rir-resnet-in-resnet-image-classification-be4c79fde8ba","alts":[{"type":2,"url":"medium://p/be4c79fde8ba"},{"type":3,"url":"medium://p/be4c79fde8ba"}],"httpStatus":200},{"url":"https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c","alts":[{"type":3,"url":"medium://p/17915421f77c"},{"type":2,"url":"medium://p/17915421f77c"}],"httpStatus":200},{"url":"https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651","alts":[{"type":2,"url":"medium://p/18e2d0f56651"},{"type":3,"url":"medium://p/18e2d0f56651"}],"httpStatus":200},{"url":"https://medium.com/@sh.tsang/paper-brief-review-of-lenet-1-lenet-4-lenet-5-boosted-lenet-4-image-classification-1f5f809dbf17","alts":[{"type":2,"url":"medium://p/1f5f809dbf17"},{"type":3,"url":"medium://p/1f5f809dbf17"}],"httpStatus":200},{"url":"https://medium.com/coinmonks/review-sppnet-1st-runner-up-object-detection-2nd-runner-up-image-classification-in-ilsvrc-906da3753679","alts":[{"type":2,"url":"medium://p/906da3753679"},{"type":3,"url":"medium://p/906da3753679"}],"httpStatus":200},{"url":"https://medium.com/@sh.tsang/review-deep-image-a-big-data-solution-for-image-recognition-99e5f7b1c802","alts":[{"type":2,"url":"medium://p/99e5f7b1c802"},{"type":3,"url":"medium://p/99e5f7b1c802"}],"httpStatus":200},{"url":"https://medium.com/coinmonks/paper-review-of-zfnet-the-winner-of-ilsvlc-2013-image-classification-d1a5a0c45103","alts":[{"type":2,"url":"medium://p/d1a5a0c45103"},{"type":3,"url":"medium://p/d1a5a0c45103"}],"httpStatus":200},{"url":"https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990","alts":[{"type":2,"url":"medium://p/aa6b6a380990"},{"type":3,"url":"medium://p/aa6b6a380990"}],"httpStatus":200},{"url":"https://medium.com/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151","alts":[{"type":2,"url":"medium://p/3f0efe9e0151"},{"type":3,"url":"medium://p/3f0efe9e0151"}],"httpStatus":200},{"url":"https://medium.com/datadriveninvestor/review-red-net-residual-encoder-decoder-network-denoising-super-resolution-cb6364ae161e","alts":[{"type":2,"url":"medium://p/cb6364ae161e"},{"type":3,"url":"medium://p/cb6364ae161e"}],"httpStatus":200},{"url":"https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7","alts":[{"type":2,"url":"medium://p/c2b3565a64e7"},{"type":3,"url":"medium://p/c2b3565a64e7"}],"httpStatus":200},{"url":"https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c","alts":[{"type":2,"url":"medium://p/3cb3a4f67a7c"},{"type":3,"url":"medium://p/3cb3a4f67a7c"}],"httpStatus":200},{"url":"https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560","alts":[{"type":2,"url":"medium://p/878b5a443560"},{"type":3,"url":"medium://p/878b5a443560"}],"httpStatus":200},{"url":"https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1","alts":[{"type":2,"url":"medium://p/b476aba290d1"},{"type":3,"url":"medium://p/b476aba290d1"}],"httpStatus":200},{"url":"https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754","alts":[{"type":2,"url":"medium://p/a6f8b9044754"},{"type":3,"url":"medium://p/a6f8b9044754"}],"httpStatus":200},{"url":"https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6","alts":[{"type":2,"url":"medium://p/5434280d6e6"},{"type":3,"url":"medium://p/5434280d6e6"}],"httpStatus":200},{"url":"https://medium.com/datadriveninvestor/review-fractalnet-image-classification-c5bdd855a090","alts":[{"type":2,"url":"medium://p/c5bdd855a090"},{"type":3,"url":"medium://p/c5bdd855a090"}],"httpStatus":200},{"url":"https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43","alts":[{"type":2,"url":"medium://p/ccbf8061ff43"},{"type":3,"url":"medium://p/ccbf8061ff43"}],"httpStatus":200},{"url":"https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11","alts":[{"type":2,"url":"medium://p/d02355543a11"},{"type":3,"url":"medium://p/d02355543a11"}],"httpStatus":200},{"url":"https://arxiv.org/abs/1505.07293","alts":[],"httpStatus":200},{"url":"https://medium.com/coinmonks/review-prelu-net-the-first-to-surpass-human-level-performance-in-ilsvrc-2015-image-f619dddd5617","alts":[{"type":2,"url":"medium://p/f619dddd5617"},{"type":3,"url":"medium://p/f619dddd5617"}],"httpStatus":200},{"url":"https://medium.com/datadriveninvestor/review-drcn-deeply-recursive-convolutional-network-super-resolution-f0a380f79b20","alts":[{"type":2,"url":"medium://p/f0a380f79b20"},{"type":3,"url":"medium://p/f0a380f79b20"}],"httpStatus":200},{"url":"https://arxiv.org/abs/1511.00561","alts":[],"httpStatus":200},{"url":"https://www.youtube.com/watch?v=CxanE_W46ts","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=CxanE_W46ts&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=CxanE_W46ts&feature=applinks"}],"httpStatus":200},{"url":"https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba","alts":[{"type":2,"url":"medium://p/a82e172e87ba"},{"type":3,"url":"medium://p/a82e172e87ba"}],"httpStatus":200},{"url":"https://medium.com/datadriveninvestor/review-espcn-real-time-sr-super-resolution-8dceca249350","alts":[{"type":2,"url":"medium://p/8dceca249350"},{"type":3,"url":"medium://p/8dceca249350"}],"httpStatus":200},{"url":"https://medium.com/datadriveninvestor/review-noc-winner-in-2015-coco-ilsvrc-detection-object-detection-d5cc84e372a","alts":[{"type":2,"url":"medium://p/d5cc84e372a"},{"type":3,"url":"medium://p/d5cc84e372a"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1","alts":[{"type":2,"url":"medium://p/eb8c9b50d2d1"},{"type":3,"url":"medium://p/eb8c9b50d2d1"}],"httpStatus":200},{"url":"https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160","alts":[{"type":2,"url":"medium://p/b93598314160"},{"type":3,"url":"medium://p/b93598314160"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c","alts":[{"type":2,"url":"medium://p/91cd2389345c"},{"type":3,"url":"medium://p/91cd2389345c"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-fsrcnn-super-resolution-80ca2ee14da4","alts":[{"type":2,"url":"medium://p/80ca2ee14da4"},{"type":3,"url":"medium://p/80ca2ee14da4"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202","alts":[{"type":2,"url":"medium://p/f5685cb30202"},{"type":3,"url":"medium://p/f5685cb30202"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69","alts":[{"type":2,"url":"medium://p/a382df364b69"},{"type":3,"url":"medium://p/a382df364b69"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e","alts":[{"type":2,"url":"medium://p/55cf8a6e380e"},{"type":3,"url":"medium://p/55cf8a6e380e"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65","alts":[{"type":2,"url":"medium://p/7883d2b02a65"},{"type":3,"url":"medium://p/7883d2b02a65"}],"httpStatus":200},{"url":"https://towardsdatascience.com/yolov1-you-only-look-once-object-detection-e1f3ffec8a89","alts":[{"type":2,"url":"medium://p/e1f3ffec8a89"},{"type":3,"url":"medium://p/e1f3ffec8a89"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac","alts":[{"type":2,"url":"medium://p/15d7f17b42ac"},{"type":3,"url":"medium://p/15d7f17b42ac"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-trimps-soushen-winner-in-ilsvrc-2016-image-classification-dfbc423111dd","alts":[{"type":2,"url":"medium://p/dfbc423111dd"},{"type":3,"url":"medium://p/dfbc423111dd"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803","alts":[{"type":3,"url":"medium://p/b6631a8ef803"},{"type":2,"url":"medium://p/b6631a8ef803"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5","alts":[{"type":2,"url":"medium://p/d4821a2bbeb5"},{"type":3,"url":"medium://p/d4821a2bbeb5"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413","alts":[{"type":2,"url":"medium://p/ea9741e7c413"},{"type":3,"url":"medium://p/ea9741e7c413"}],"httpStatus":200},{"url":"http://mi.eng.cam.ac.uk/projects/segnet/demo.php","alts":[],"httpStatus":200},{"url":"https://towardsdatascience.com/review-stn-spatial-transformer-network-image-classification-d3cbd98a70aa","alts":[{"type":2,"url":"medium://p/d3cbd98a70aa"},{"type":3,"url":"medium://p/d3cbd98a70aa"}],"httpStatus":200},{"url":"https://towardsdatascience.com/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e","alts":[{"type":2,"url":"medium://p/bb50a42af03e"},{"type":3,"url":"medium://p/bb50a42af03e"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-drrn-deep-recursive-residual-network-super-resolution-dca4a35ce994","alts":[{"type":2,"url":"medium://p/dca4a35ce994"},{"type":3,"url":"medium://p/dca4a35ce994"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610","alts":[{"type":2,"url":"medium://p/262fc7482610"},{"type":3,"url":"medium://p/262fc7482610"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-sharpmask-instance-segmentation-6509f7401a61","alts":[{"type":2,"url":"medium://p/6509f7401a61"},{"type":3,"url":"medium://p/6509f7401a61"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-wrns-wide-residual-networks-image-classification-d3feb3fb2004","alts":[{"type":2,"url":"medium://p/d3feb3fb2004"},{"type":3,"url":"medium://p/d3feb3fb2004"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34","alts":[{"type":2,"url":"medium://p/42a9334e6a34"},{"type":3,"url":"medium://p/42a9334e6a34"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-u-net-biomedical-image-segmentation-d02bf06ca760","alts":[{"type":2,"url":"medium://p/d02bf06ca760"},{"type":3,"url":"medium://p/d02bf06ca760"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568","alts":[{"type":2,"url":"medium://p/dc967dd42568"},{"type":3,"url":"medium://p/dc967dd42568"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-deepmask-instance-segmentation-30327a072339","alts":[{"type":2,"url":"medium://p/30327a072339"},{"type":3,"url":"medium://p/30327a072339"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-ror-resnet-of-resnet-multilevel-resnet-image-classification-cd3b0fcc19bb","alts":[{"type":2,"url":"medium://p/cd3b0fcc19bb"},{"type":3,"url":"medium://p/cd3b0fcc19bb"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-vdsr-super-resolution-f8050d49362f","alts":[{"type":2,"url":"medium://p/f8050d49362f"},{"type":3,"url":"medium://p/f8050d49362f"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea","alts":[{"type":2,"url":"medium://p/8a1a941ce9ea"},{"type":3,"url":"medium://p/8a1a941ce9ea"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d","alts":[{"type":2,"url":"medium://p/e089e5df177d"},{"type":3,"url":"medium://p/e089e5df177d"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2","alts":[{"type":3,"url":"medium://p/ee2d61f465e2"},{"type":2,"url":"medium://p/ee2d61f465e2"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6","alts":[{"type":3,"url":"medium://p/eab75d7a1ba6"},{"type":2,"url":"medium://p/eab75d7a1ba6"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44","alts":[{"type":2,"url":"medium://p/14e488efce44"},{"type":3,"url":"medium://p/14e488efce44"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6","alts":[{"type":2,"url":"medium://p/f72486f1a0f6"},{"type":3,"url":"medium://p/f72486f1a0f6"}],"httpStatus":200},{"url":"https://medium.com/@sh.tsang/review-pyramidnet-deep-pyramidal-residual-networks-image-classification-85a87b60ae78","alts":[{"type":2,"url":"medium://p/85a87b60ae78"},{"type":3,"url":"medium://p/85a87b60ae78"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766","alts":[{"type":2,"url":"medium://p/da19993f4766"},{"type":3,"url":"medium://p/da19993f4766"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d","alts":[{"type":2,"url":"medium://p/b51c5fbde92d"},{"type":3,"url":"medium://p/b51c5fbde92d"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc","alts":[{"type":2,"url":"medium://p/5e8c339d18bc"},{"type":3,"url":"medium://p/5e8c339d18bc"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74","alts":[{"type":2,"url":"medium://p/6d818bfd1d74"},{"type":3,"url":"medium://p/6d818bfd1d74"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11","alts":[{"type":2,"url":"medium://p/851a94607d11"},{"type":3,"url":"medium://p/851a94607d11"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4","alts":[{"type":2,"url":"medium://p/af3f2eaf87e4"},{"type":3,"url":"medium://p/af3f2eaf87e4"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92","alts":[{"type":2,"url":"medium://p/dbfe67d4ee92"},{"type":3,"url":"medium://p/dbfe67d4ee92"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-lapsrn-ms-lapsrn-laplacian-pyramid-super-resolution-network-super-resolution-c5fe2b65f5e8","alts":[{"type":2,"url":"medium://p/c5fe2b65f5e8"},{"type":3,"url":"medium://p/c5fe2b65f5e8"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-stochastic-depth-image-classification-a4e225807f4a","alts":[{"type":2,"url":"medium://p/a4e225807f4a"},{"type":3,"url":"medium://p/a4e225807f4a"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4","alts":[{"type":3,"url":"medium://p/38fba6afabe4"},{"type":2,"url":"medium://p/38fba6afabe4"}],"httpStatus":200},{"url":"https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6","alts":[{"type":2,"url":"medium://p/ae4c9c75bea6"},{"type":3,"url":"medium://p/ae4c9c75bea6"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8","alts":[{"type":2,"url":"medium://p/e39402bfa5d8"},{"type":3,"url":"medium://p/e39402bfa5d8"}],"httpStatus":200},{"url":"https://towardsdatascience.com/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5","alts":[{"type":2,"url":"medium://p/9d5a5bd768f5"},{"type":3,"url":"medium://p/9d5a5bd768f5"}],"httpStatus":200}],"version":"0.3","generatedAt":1554779926687},"isLockedPreviewOnly":false,"metaDescription":"","totalClapCount":191,"sectionCount":6,"readingList":0,"topics":[{"topicId":"1eca0103fff3","slug":"machine-learning","createdAt":1534449726145,"deletedAt":0,"image":{"id":"1*gFJS3amhZEg_z39D5EErVg@2x.png","originalWidth":2800,"originalHeight":1750},"name":"Machine Learning","description":"Teaching the learners.","relatedTopics":[],"visibility":1,"relatedTags":[],"relatedTopicIds":[],"type":"Topic"},{"topicId":"ae5d4995e225","slug":"data-science","createdAt":1493923906289,"deletedAt":0,"image":{"id":"1*NHWOEki_ncCX-xzbKtkEWw@2x.jpeg","originalWidth":5760,"originalHeight":3840},"name":"Data Science","description":"Query this.","relatedTopics":[],"visibility":1,"relatedTags":[],"relatedTopicIds":[],"type":"Topic"}]},"coverless":true,"slug":"review-segnet-semantic-segmentation","translationSourcePostId":"","translationSourceCreatorId":"","isApprovedTranslation":false,"inResponseToPostId":"","inResponseToRemovedAt":0,"isTitleSynthesized":false,"allowResponses":true,"importedUrl":"","importedPublishedAt":0,"visibility":0,"uniqueSlug":"review-segnet-semantic-segmentation-e66f2e30fb96","previewContent":{"bodyModel":{"paragraphs":[{"name":"previewImage","type":4,"text":"","layout":10,"metadata":{"id":"0*k8ejti9_6CHwxzFQ.gif","originalWidth":600,"originalHeight":338,"isFeatured":true}},{"name":"ac92","type":3,"text":"Review: SegNet (Semantic Segmentation)","markups":[],"alignment":1},{"name":"7054","type":13,"text":"Encoder Decoder Architecture, Using Max Pooling Indices to…","markups":[],"alignment":1}],"sections":[{"startIndex":0}]},"isFullContent":false,"subtitle":"Encoder Decoder Architecture, Using Max Pooling Indices to Upsample, Outperforms FCN, DeepLabv1, DeconvNet"},"license":0,"inResponseToMediaResourceId":"","canonicalUrl":"https://towardsdatascience.com/review-segnet-semantic-segmentation-e66f2e30fb96","approvedHomeCollectionId":"7f60cf5620c9","approvedHomeCollection":{"id":"7f60cf5620c9","name":"Towards Data Science","slug":"towards-data-science","tags":["DATA SCIENCE","MACHINE LEARNING","ARTIFICIAL INTELLIGENCE","ANALYTICS","PROGRAMMING"],"creatorId":"895063a310f4","description":"Sharing concepts, ideas, and codes.","shortDescription":"Sharing concepts, ideas, and codes.","image":{"imageId":"1*F0LADxTtsKOgmPa-_7iUEQ.jpeg","filter":"","backgroundSize":"","originalWidth":1275,"originalHeight":1275,"strategy":"resample","height":0,"width":0},"metadata":{"followerCount":240824,"activeAt":1562947087565},"virtuals":{"permissions":{"canPublish":false,"canPublishAll":false,"canRepublish":false,"canRemove":false,"canManageAll":false,"canSubmit":false,"canEditPosts":false,"canAddWriters":false,"canViewStats":false,"canSendNewsletter":false,"canViewLockedPosts":false,"canViewCloaked":false,"canEditOwnPosts":false,"canBeAssignedAuthor":false,"canEnrollInHightower":false,"canLockPostsForMediumMembers":false,"canLockOwnPostsForMediumMembers":false},"isSubscribed":false,"isNewsletterSubscribed":false,"isEnrolledInHightower":false,"isEligibleForHightower":false,"mediumNewsletterId":"","isSubscribedToMediumNewsletter":false},"logo":{"imageId":"1*5EUO1kUYBthpOCPzRj_l2g.png","filter":"","backgroundSize":"","originalWidth":1010,"originalHeight":376,"strategy":"resample","height":0,"width":0},"twitterUsername":"TDataScience","facebookPageName":"towardsdatascience","collectionMastheadId":"8b6aceffde6","domain":"towardsdatascience.com","sections":[{"type":2,"collectionHeaderMetadata":{"title":"Towards Data Science","description":"Sharing concepts, ideas, and codes","backgroundImage":{},"logoImage":{},"alignment":2,"layout":5}},{"type":1,"postListMetadata":{"source":3,"layout":4,"number":2,"postIds":["39967f07a67f","1b295ea5bbe0"]}},{"type":1,"postListMetadata":{"source":3,"layout":4,"number":3,"postIds":["85e123a448bf","937d1fb582ee","792160ab5495"],"sectionHeader":"Featured "}},{"type":1,"postListMetadata":{"source":1,"layout":4,"number":6,"postIds":[],"sectionHeader":"Latest"}},{"type":1,"postListMetadata":{"source":3,"layout":4,"number":2,"postIds":["272f28835af7","67a3e08ce50d"],"sectionHeader":"Our Letters"}},{"type":3,"promoMetadata":{"sectionHeader":"","promoId":"3245ee5b4331"}},{"type":1,"postListMetadata":{"source":2,"layout":4,"number":6,"postIds":[],"sectionHeader":"Trending"}},{"type":1,"postListMetadata":{"source":3,"layout":4,"number":2,"postIds":["3bf37f75a345","3920888f831c"],"sectionHeader":"Our Readers’ Guide"}},{"type":1,"postListMetadata":{"source":4,"layout":4,"number":9,"postIds":[],"tagSlug":"Towards Data Science","sectionHeader":"Editors' Picks"}},{"type":1,"postListMetadata":{"source":3,"layout":4,"number":2,"postIds":["96667b06af5","d691af11cc2f"],"sectionHeader":"Contribute"}},{"type":1,"postListMetadata":{"source":3,"layout":4,"number":2,"postIds":["8bae371d4bb1","766cdd74d13e"]}},{"type":1,"postListMetadata":{"source":4,"layout":5,"number":3,"postIds":[],"tagSlug":"Towards Data Science","sectionHeader":"Last Chance To Read"}}],"tintColor":"#FF355876","lightText":true,"favicon":{"imageId":"1*F0LADxTtsKOgmPa-_7iUEQ.jpeg","filter":"","backgroundSize":"","originalWidth":1275,"originalHeight":1275,"strategy":"resample","height":0,"width":0},"colorPalette":{"defaultBackgroundSpectrum":{"colorPoints":[{"color":"#FF668AAA","point":0},{"color":"#FF61809D","point":0.1},{"color":"#FF5A7690","point":0.2},{"color":"#FF546C83","point":0.3},{"color":"#FF4D6275","point":0.4},{"color":"#FF455768","point":0.5},{"color":"#FF3D4C5A","point":0.6},{"color":"#FF34414C","point":0.7},{"color":"#FF2B353E","point":0.8},{"color":"#FF21282F","point":0.9},{"color":"#FF161B1F","point":1}],"backgroundColor":"#FFFFFFFF"},"tintBackgroundSpectrum":{"colorPoints":[{"color":"#FF355876","point":0},{"color":"#FF4D6C88","point":0.1},{"color":"#FF637F99","point":0.2},{"color":"#FF7791A8","point":0.3},{"color":"#FF8CA2B7","point":0.4},{"color":"#FF9FB3C6","point":0.5},{"color":"#FFB2C3D4","point":0.6},{"color":"#FFC5D2E1","point":0.7},{"color":"#FFD7E2EE","point":0.8},{"color":"#FFE9F1FA","point":0.9},{"color":"#FFFBFFFF","point":1}],"backgroundColor":"#FF355876"},"highlightSpectrum":{"colorPoints":[{"color":"#FFEDF4FC","point":0},{"color":"#FFE9F2FD","point":0.1},{"color":"#FFE6F1FD","point":0.2},{"color":"#FFE2EFFD","point":0.3},{"color":"#FFDFEEFD","point":0.4},{"color":"#FFDBECFE","point":0.5},{"color":"#FFD7EBFE","point":0.6},{"color":"#FFD4E9FE","point":0.7},{"color":"#FFD0E7FF","point":0.8},{"color":"#FFCCE6FF","point":0.9},{"color":"#FFC8E4FF","point":1}],"backgroundColor":"#FFFFFFFF"}},"navItems":[{"type":4,"title":"Data Science","url":"https://towardsdatascience.com/data-science/home","topicId":"cf416843aadc","source":"topicId"},{"type":4,"title":"Machine Learning","url":"https://towardsdatascience.com/machine-learning/home","topicId":"a5c9b2f1cb6b","source":"topicId"},{"type":4,"title":"Programming","url":"https://towardsdatascience.com/programming/home","topicId":"41533a1dc73c","source":"topicId"},{"type":4,"title":"Visualization","url":"https://towardsdatascience.com/data-visualization/home","topicId":"825e6cb8b9ce","source":"topicId"},{"type":4,"title":"AI","url":"https://towardsdatascience.com/artificial-intelligence/home","topicId":"7f029b17bf96","source":"topicId"},{"type":4,"title":"Journalism","url":"https://towardsdatascience.com/data-journalism/home","topicId":"27a6ac3980c6","source":"topicId"},{"type":2,"title":"Events","postId":"8bae371d4bb1","url":"https://towardsdatascience.com/toronto-machine-learning-summit-8bae371d4bb1","source":"postId"},{"type":3,"title":"Submit","url":"https://towardsdatascience.com/contribute/home"}],"colorBehavior":2,"instantArticlesState":0,"acceleratedMobilePagesState":0,"googleAnalyticsId":"UA-19707169-24","ampLogo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"header":{"title":"Towards Data Science","description":"Sharing concepts, ideas, and codes","backgroundImage":{},"logoImage":{},"alignment":2,"layout":5},"paidForDomainAt":1509037374118,"type":"Collection"},"newsletterId":"","webCanonicalUrl":"https://towardsdatascience.com/review-segnet-semantic-segmentation-e66f2e30fb96","mediumUrl":"https://towardsdatascience.com/review-segnet-semantic-segmentation-e66f2e30fb96","migrationId":"","notifyFollowers":true,"notifyTwitter":false,"notifyFacebook":false,"responseHiddenOnParentPostAt":0,"isSeries":false,"isSubscriptionLocked":false,"seriesLastAppendedAt":0,"audioVersionDurationSec":0,"sequenceId":"","isNsfw":false,"isEligibleForRevenue":false,"isBlockedFromHightower":false,"deletedAt":0,"lockedPostSource":0,"hightowerMinimumGuaranteeStartsAt":0,"hightowerMinimumGuaranteeEndsAt":0,"featureLockRequestAcceptedAt":0,"mongerRequestType":1,"layerCake":3,"socialTitle":"","socialDek":"","editorialPreviewTitle":"","editorialPreviewDek":"","curationEligibleAt":0,"primaryTopic":{"topicId":"ae5d4995e225","slug":"data-science","createdAt":1493923906289,"deletedAt":0,"image":{"id":"1*NHWOEki_ncCX-xzbKtkEWw@2x.jpeg","originalWidth":5760,"originalHeight":3840},"name":"Data Science","description":"Query this.","relatedTopics":[],"visibility":1,"relatedTags":[],"relatedTopicIds":[],"type":"Topic"},"primaryTopicId":"ae5d4995e225","isProxyPost":false,"proxyPostFaviconUrl":"","proxyPostProviderName":"","proxyPostType":0,"type":"Post"},"mentionedUsers":[{"userId":"aff72a0c1243","name":"Sik-Ho Tsang","username":"sh.tsang","createdAt":1524566858911,"imageId":"1*OxjNUHcLFU8-pp-j8su6pg.jpeg","backgroundImageId":"","bio":"PhD, Researcher. I share what I've learnt and done. :) My LinkedIn: https://www.linkedin.com/in/sh-tsang/","twitterScreenName":"","socialStats":{"userId":"aff72a0c1243","usersFollowedCount":31,"usersFollowedByCount":2051,"type":"SocialStats"},"social":{"userId":"5f069cdb2c1b","targetUserId":"aff72a0c1243","type":"Social"},"facebookAccountId":"","allowNotes":1,"mediumMemberAt":0,"isNsfw":false,"isWriterProgramEnrolled":true,"isQuarantined":false,"type":"User"}],"collaborators":[],"hideMeter":false,"collectionUserRelations":[],"mode":null,"references":{"User":{"aff72a0c1243":{"userId":"aff72a0c1243","name":"Sik-Ho Tsang","username":"sh.tsang","createdAt":1524566858911,"imageId":"1*OxjNUHcLFU8-pp-j8su6pg.jpeg","backgroundImageId":"","bio":"PhD, Researcher. I share what I've learnt and done. :) My LinkedIn: https://www.linkedin.com/in/sh-tsang/","twitterScreenName":"","socialStats":{"userId":"aff72a0c1243","usersFollowedCount":31,"usersFollowedByCount":2051,"type":"SocialStats"},"social":{"userId":"5f069cdb2c1b","targetUserId":"aff72a0c1243","type":"Social"},"facebookAccountId":"","allowNotes":1,"mediumMemberAt":0,"isNsfw":false,"isWriterProgramEnrolled":true,"isQuarantined":false,"type":"User"}},"Collection":{"7f60cf5620c9":{"id":"7f60cf5620c9","name":"Towards Data Science","slug":"towards-data-science","tags":["DATA SCIENCE","MACHINE LEARNING","ARTIFICIAL INTELLIGENCE","ANALYTICS","PROGRAMMING"],"creatorId":"895063a310f4","description":"Sharing concepts, ideas, and codes.","shortDescription":"Sharing concepts, ideas, and codes.","image":{"imageId":"1*F0LADxTtsKOgmPa-_7iUEQ.jpeg","filter":"","backgroundSize":"","originalWidth":1275,"originalHeight":1275,"strategy":"resample","height":0,"width":0},"metadata":{"followerCount":240824,"activeAt":1562947087565},"virtuals":{"permissions":{"canPublish":false,"canPublishAll":false,"canRepublish":false,"canRemove":false,"canManageAll":false,"canSubmit":false,"canEditPosts":false,"canAddWriters":false,"canViewStats":false,"canSendNewsletter":false,"canViewLockedPosts":false,"canViewCloaked":false,"canEditOwnPosts":false,"canBeAssignedAuthor":false,"canEnrollInHightower":false,"canLockPostsForMediumMembers":false,"canLockOwnPostsForMediumMembers":false},"isSubscribed":false,"isNewsletterSubscribed":false,"isEnrolledInHightower":false,"isEligibleForHightower":false,"mediumNewsletterId":"","isSubscribedToMediumNewsletter":false},"logo":{"imageId":"1*5EUO1kUYBthpOCPzRj_l2g.png","filter":"","backgroundSize":"","originalWidth":1010,"originalHeight":376,"strategy":"resample","height":0,"width":0},"twitterUsername":"TDataScience","facebookPageName":"towardsdatascience","collectionMastheadId":"8b6aceffde6","domain":"towardsdatascience.com","sections":[{"type":2,"collectionHeaderMetadata":{"title":"Towards Data Science","description":"Sharing concepts, ideas, and codes","backgroundImage":{},"logoImage":{},"alignment":2,"layout":5}},{"type":1,"postListMetadata":{"source":3,"layout":4,"number":2,"postIds":["39967f07a67f","1b295ea5bbe0"]}},{"type":1,"postListMetadata":{"source":3,"layout":4,"number":3,"postIds":["85e123a448bf","937d1fb582ee","792160ab5495"],"sectionHeader":"Featured "}},{"type":1,"postListMetadata":{"source":1,"layout":4,"number":6,"postIds":[],"sectionHeader":"Latest"}},{"type":1,"postListMetadata":{"source":3,"layout":4,"number":2,"postIds":["272f28835af7","67a3e08ce50d"],"sectionHeader":"Our Letters"}},{"type":3,"promoMetadata":{"sectionHeader":"","promoId":"3245ee5b4331"}},{"type":1,"postListMetadata":{"source":2,"layout":4,"number":6,"postIds":[],"sectionHeader":"Trending"}},{"type":1,"postListMetadata":{"source":3,"layout":4,"number":2,"postIds":["3bf37f75a345","3920888f831c"],"sectionHeader":"Our Readers’ Guide"}},{"type":1,"postListMetadata":{"source":4,"layout":4,"number":9,"postIds":[],"tagSlug":"Towards Data Science","sectionHeader":"Editors' Picks"}},{"type":1,"postListMetadata":{"source":3,"layout":4,"number":2,"postIds":["96667b06af5","d691af11cc2f"],"sectionHeader":"Contribute"}},{"type":1,"postListMetadata":{"source":3,"layout":4,"number":2,"postIds":["8bae371d4bb1","766cdd74d13e"]}},{"type":1,"postListMetadata":{"source":4,"layout":5,"number":3,"postIds":[],"tagSlug":"Towards Data Science","sectionHeader":"Last Chance To Read"}}],"tintColor":"#FF355876","lightText":true,"favicon":{"imageId":"1*F0LADxTtsKOgmPa-_7iUEQ.jpeg","filter":"","backgroundSize":"","originalWidth":1275,"originalHeight":1275,"strategy":"resample","height":0,"width":0},"colorPalette":{"defaultBackgroundSpectrum":{"colorPoints":[{"color":"#FF668AAA","point":0},{"color":"#FF61809D","point":0.1},{"color":"#FF5A7690","point":0.2},{"color":"#FF546C83","point":0.3},{"color":"#FF4D6275","point":0.4},{"color":"#FF455768","point":0.5},{"color":"#FF3D4C5A","point":0.6},{"color":"#FF34414C","point":0.7},{"color":"#FF2B353E","point":0.8},{"color":"#FF21282F","point":0.9},{"color":"#FF161B1F","point":1}],"backgroundColor":"#FFFFFFFF"},"tintBackgroundSpectrum":{"colorPoints":[{"color":"#FF355876","point":0},{"color":"#FF4D6C88","point":0.1},{"color":"#FF637F99","point":0.2},{"color":"#FF7791A8","point":0.3},{"color":"#FF8CA2B7","point":0.4},{"color":"#FF9FB3C6","point":0.5},{"color":"#FFB2C3D4","point":0.6},{"color":"#FFC5D2E1","point":0.7},{"color":"#FFD7E2EE","point":0.8},{"color":"#FFE9F1FA","point":0.9},{"color":"#FFFBFFFF","point":1}],"backgroundColor":"#FF355876"},"highlightSpectrum":{"colorPoints":[{"color":"#FFEDF4FC","point":0},{"color":"#FFE9F2FD","point":0.1},{"color":"#FFE6F1FD","point":0.2},{"color":"#FFE2EFFD","point":0.3},{"color":"#FFDFEEFD","point":0.4},{"color":"#FFDBECFE","point":0.5},{"color":"#FFD7EBFE","point":0.6},{"color":"#FFD4E9FE","point":0.7},{"color":"#FFD0E7FF","point":0.8},{"color":"#FFCCE6FF","point":0.9},{"color":"#FFC8E4FF","point":1}],"backgroundColor":"#FFFFFFFF"}},"navItems":[{"type":4,"title":"Data Science","url":"https://towardsdatascience.com/data-science/home","topicId":"cf416843aadc","source":"topicId"},{"type":4,"title":"Machine Learning","url":"https://towardsdatascience.com/machine-learning/home","topicId":"a5c9b2f1cb6b","source":"topicId"},{"type":4,"title":"Programming","url":"https://towardsdatascience.com/programming/home","topicId":"41533a1dc73c","source":"topicId"},{"type":4,"title":"Visualization","url":"https://towardsdatascience.com/data-visualization/home","topicId":"825e6cb8b9ce","source":"topicId"},{"type":4,"title":"AI","url":"https://towardsdatascience.com/artificial-intelligence/home","topicId":"7f029b17bf96","source":"topicId"},{"type":4,"title":"Journalism","url":"https://towardsdatascience.com/data-journalism/home","topicId":"27a6ac3980c6","source":"topicId"},{"type":2,"title":"Events","postId":"8bae371d4bb1","url":"https://towardsdatascience.com/toronto-machine-learning-summit-8bae371d4bb1","source":"postId"},{"type":3,"title":"Submit","url":"https://towardsdatascience.com/contribute/home"}],"colorBehavior":2,"instantArticlesState":0,"acceleratedMobilePagesState":0,"googleAnalyticsId":"UA-19707169-24","ampLogo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"header":{"title":"Towards Data Science","description":"Sharing concepts, ideas, and codes","backgroundImage":{},"logoImage":{},"alignment":2,"layout":5},"paidForDomainAt":1509037374118,"type":"Collection"}},"Social":{"aff72a0c1243":{"userId":"5f069cdb2c1b","targetUserId":"aff72a0c1243","type":"Social"}},"SocialStats":{"aff72a0c1243":{"userId":"aff72a0c1243","usersFollowedCount":31,"usersFollowedByCount":2051,"type":"SocialStats"}}}})
// ]]></script><script>window.PARSELY = window.PARSELY || { autotrack: false }</script><script id="parsely-cfg" src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/p.js.download"></script><script type="text/javascript">(function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src="https://cdn.branch.io/branch-latest.min.js";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,"script","branch",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},"addListener applyCode autoAppIndex banner closeBanner closeJourney creditHistory credits data deepview deepviewCta first getCode init link logout redeem referrals removeListener sendSMS setBranchViewData setIdentity track validateCode trackCommerceEvent logEvent".split(" "), 0); branch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true, 'tracking_disabled':  false }, function(err, data) {});</script><div class="surface-scrollOverlay"></div><script charset="UTF-8" src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/main-common-async.bundle.FlwFcvMmAdE8L4RC6Z-SKQ.js.download"></script><script charset="UTF-8" src="./Review_ SegNet (Semantic Segmentation) – Towards Data Science_files/main-notes.bundle.jNkicUtbDgE4YuHg2DfzMw.js.download"></script></body></html>